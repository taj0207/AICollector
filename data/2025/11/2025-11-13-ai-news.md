# AI News for 2025-11-13 (Asia/Taipei)

Collected 5 article(s).

- [Google DeepMind is using Gemini to train agents inside Goat Simulator 3](https://www.technologyreview.com/2025/11/13/1127921/google-deepmind-is-using-gemini-to-train-agents-inside-goat-simulator-3/) — 23:00 · MIT Technology Review (AI)
  > Google DeepMind has built a new video-game-playing agent called SIMA 2 that can navigate and solve problems in a wide range of 3D virtual worlds. The company claims it’s a big step toward more general-purpose agents and better real-world robots.    Google DeepMind first demoed SIMA (which stands for “scalable instructable multiworld agent”) last year. But…
- [The Download: AI to measure pain, and how to deal with conspiracy theorists](https://www.technologyreview.com/2025/11/13/1127911/the-download-ai-to-measure-pain-and-how-to-deal-with-conspiracy-theorists/) — 21:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. AI is changing how we quantify pain Researchers around the world are racing to turn pain—medicine’s most subjective vital sign—into something a camera or sensor can score as reliably as blood pressure. The…
- [Google is still aiming for its “moonshot” 2030 energy goals](https://www.technologyreview.com/2025/11/13/1127896/google-energy-goals/) — 19:00 · MIT Technology Review (AI)
  > Last week, we hosted EmTech MIT, MIT Technology Review’s annual flagship conference in Cambridge, Massachusetts. Over the course of three days of main-stage sessions, I learned about innovations in AI, biotech, and robotics.  But as you might imagine, some of this climate reporter’s favorite moments came in the climate sessions. I was listening especially closely…
- [Alembic melted GPUs chasing causal A.I. — now it's running one of the fastest supercomputers in the world](https://venturebeat.com/ai/alembic-melted-gpus-chasing-causal-a-i-now-its-running-one-of-the-fastest) — 18:00 · VentureBeat AI
  > Alembic Technologies has raised $145 million in Series B and growth funding at a valuation 13 times higher than its previous round, betting that the next competitive advantage in artificial intelligence will come not from better language models but from proprietary data and causal reasoning.The San Francisco-based startup, which builds AI systems that identify cause-and-effect relationships rather than mere correlations, is using a significant portion of the capital to deploy what it claims is one of the fastest privately owned supercomputers ever built — an Nvidia NVL72 superPOD that will power its enterprise-grade causal AI models.The investment, led by Prysm Capital and Accenture with participation from Silver Lake Waterman, Liquid 2 Ventures, NextEquity, Friends & Family Capital and WndrCo, positions Alembic among a select group of well-funded AI laboratories transforming how corporations make multimillion-dollar decisions.The funding round and the company's strategic direction reflect a broader shift taking place in enterprise AI as the performance gap between competing large language models narrows. While startups and tech giants have poured billions into building ever-larger chatbots, Alembic is pursuing a different thesis: that the real value in AI will accrue to systems that can process private corporate data to answer questions that generic models cannot."As powerful artificial intelligence models increasingly converge in capability, the key competitive advantage shifts to proprietary data," said Tomás Puig, Alembic's founder and chief executive, in an interview with VentureBeat. "Getting a real edge isn't about using the best LLM; it's leveraging the unique information rivals can't access."Puig illustrated the problem facing enterprise executives: "Imagine I run a CPG company and I install the latest ChatGPT. I ask, 'Hey, ChatGPT, give me a strategy for how to increase my revenue share in the northeast.' Then your competitor down the road asks the exact same question. How much trouble are you in when they get the exact same answer?"How a broke startup on Mac Pros discovered a breakthrough that changed everythingThe dramatic valuation increase—from roughly $50 million at the Series A to approximately $645 million now, according to people familiar with the matter — reflects a fundamental transformation in Alembic's technology and market positioning since its previous funding round.When the company raised its Series A in early 2024, it was primarily a signal processing and correlation analytics company focused on marketing measurement. "Causal did not exist as a technology for us till after the Series A," Puig told VentureBeat. The company was so resource-constrained that it couldn't even run simulations to test whether its causal models would work.The breakthrough came after the Series A when the company finally had enough capital to test its theories. "We were so broke that we couldn't even run the simulation to see if it worked," Puig recalled. When they did run the tests — initially on an "army of Mac Pros" because they didn't yet have GPU infrastructure — they discovered something unexpected: their causal model worked not just for marketing analytics but across virtually any business domain with time-series data."We started adding capabilities as customers requested them, which was just sensible—iterative," Puig explained. "We found out the model works across a huge majority of data universally. What we thought might be a model for a specific vertical ended up being a full, generalized foundational model."That discovery transformed Alembic from a marketing technology vendor into a company building what Puig describes as "the entire central nervous system of the enterprise across all verticals — not just sales, marketing, supply chain, finance, and beyond."Why cause-and-effect AI matters more than correlation for enterprise decision-makingCausal AI is a fundamentally different approach from the correlation-based analytics that dominate most business intelligence tools and even many AI systems. Where traditional analytics might show that social media engagement correlates with sales increases, causal AI can determine whether the social media activity actually caused the sales lift — or whether both were driven by some third factor, like a viral news event.The distinction matters enormously for executives making budget allocation decisions. "Most businesses are not short on data," Puig said. "They are short on answers."For Alembic's customers, which now include Delta Air Lines, Mars, Nvidia and several Fortune 500 companies across financial services, technology and consumer packaged goods, the platform can answer previously unanswerable questions about marketing effectiveness, operational efficiency and strategic investments."Alembic's ability to connect marketing exposure directly to business outcomes—with speed, precision and granularity—is what made this relationship so transformative for us," said Alicia Tillman, chief marketing officer at Delta Air Lines. "Unlike traditional measurement tools, Alembic gave us a unified view across channels and campaigns, unlocking insights we simply couldn't access before."The airline used Alembic to quantify the revenue lift from its Team USA Olympics sponsorship within days of activation, directly linking brand activities to ticket sales—a type of measurement that has eluded marketers for decades. Traditional attribution models either ignore brand-building entirely or assign it vague "awareness" metrics that don't translate to financial impact."It's very transformative," Puig said of the customer impact. "What's interesting is that executives themselves are the users of our software and our outputs. It's not a tool used by a single campaign manager."Inside the two-story liquid-cooled supercomputer that literally melted GPUsAlembic's decision to invest heavily in owned computing infrastructure rather than rely on cloud providers stems from both the technical demands of its causal models and the extreme data sensitivity requirements of its enterprise customers.The company is deploying an Nvidia NVL72 superPOD — a massive liquid-cooled system equipped with Nvidia's most advanced Blackwell graphics processing units — in partnership with data center operator Equinix in San Jose, Calif. According to Puig, Nvidia informed Alembic that it is the only non-Fortune 500 company in the world to operate such a system.The need for this level of compute stems from how Alembic's models work. Unlike large language models that are trained once on historical data and then deployed, Alembic's system uses "online and evolving" models built on spiking neural networks — brain-inspired architectures that continuously learn as new data arrives."It creates itself as you feed it data, like human evolution," Puig explained. "The model is singular, but it ends up creating a different brain for every single company."This continuous learning happens at massive scale. When a customer brings in data, Alembic's system automatically permutates through billions of possible combinations of how that data could be analyzed — testing every conceivable way to slice metrics and dimensions to find the strongest causal signals. That level of computation requires what Puig calls "F1 car" infrastructure rather than the "production Porsche" offered by cloud providers.The company writes custom CUDA code and low-level GPU kernels optimized specifically for causal inference workloads — optimizations that aren't possible on standard cloud configurations. The approach has proven so demanding that Alembic famously once melted down its GPUs by pushing them beyond their thermal limits. "We literally just drive these circuits so hard that we need the F1 car version and we have to have access to it," Puig said.The move to liquid-cooled systems addresses that problem, but it also enables Alembic to run workloads that would cost orders of magnitude more on cloud platforms. "We did the math—if we were to buy just one subsection of our compute from AWS, it would be $62 million a year," Puig said. Owning the infrastructure costs "a fraction of that."The supercomputer strategy serves another crucial purpose: data sovereignty. Many of Alembic's customers — particularly in financial services, consumer packaged goods and regulated industries — have contractual prohibitions against putting sensitive data on Amazon Web Services, Microsoft Azure or Google Cloud."CPG companies do not want any data to exist on Amazon, ever," Puig said. "They simply won't allow it. Some customers refuse to use Microsoft, others avoid different providers. And certain banks and financial institutions are legally prohibited from using cloud platforms at all."By operating its own infrastructure in neutral data centers, Alembic can serve customers who would never consider cloud-based analytics — a competitive moat that would be difficult for hyperscale cloud providers to replicate.How Jensen Huang read a news article and changed Alembic's destinyAlembic's relationship with Nvidia illustrates both the startup's technical ambitions and how the chip giant supports promising AI companies. Nvidia is Alembic's founding enterprise customer, exclusive supercomputing partner and a key technical collaborator — though notably not an investor.The relationship began in an unlikely way. After Alembic announced its Series A funding in early 2024, Nvidia co-founder and CEO Jensen Huang read the VentureBeat coverage and emailed his staff suggesting they explore the company, according to Puig. Because Alembic didn't yet have a contact form on its website, an Nvidia director reached out via LinkedIn.The partnership nearly foundered on a basic constraint: computing capacity. After Alembic delivered its first causal analysis — which took weeks to generate on an array of Mac Pros — Nvidia asked if they could produce weekly reports. "I said no, because it took weeks on this army of machines," Puig recalled.When Alembic said they could do it with GPUs but couldn't secure the necessary compute — cloud providers at the time required committee approvals and offered two- to six-week lead times with no guarantees — Nvidia intervened directly. The chip maker arranged for Equinix to provide a private cage in Northern Virginia with sufficient power capacity and helped Alembic source its first H100 GPU cluster."Without that, the company would never have existed," Puig said. "We couldn't get the compute in the configuration we needed anywhere else."The partnership has since deepened. Alembic uses Nvidia's AI Enterprise software suite, including specialized libraries like cuGraph for graph processing and TensorRT for high-speed inference. The tight integration, Puig said, allows "our research teams to leverage multi-exaflop-level compute and Nvidia's algorithmic software stack. This integration is one of our secret weapons: we spend more time on breakthrough research and mathematics and less time on repetitive low-level engineering."Nvidia's support extended beyond technology. When Alembic kept destroying GPUs under extreme workloads — pushing chips so hard that thermal stress cracked circuit boards — Nvidia fast-tracked the startup's access to next-generation liquid-cooled systems. "The funny reason we got [the NVL72]," Puig said, "is because when we melted the chips, Nvidia was literally annoyed with how often they had to service our warranty."From Olympics sponsorships to viral candy moments: How Fortune 500s measure what was unmeasurableAlembic's customer roster has expanded rapidly as enterprises seek ways to measure AI and marketing investments that traditional analytics cannot capture. The company now works with Delta Air Lines, Mars, multiple Fortune 500 technology and financial services firms, and Texas A&M University's athletics program.The use cases span far beyond Alembic's original marketing focus. Mars used the platform to measure the sales impact of changing candy shapes for themed promotions. A Fortune 500 technology company expanded its sales pipeline by 37% using Alembic's attribution models. Financial services firms are using it to connect CEO public appearances and co-marketing expenditures to actual fund flows."Alembic helped us move past impression counts to show what actually drove net-new investment," said the head of co-marketing at a Fortune 200 financial services company. "For the first time, we could see how our CEO in the public eye and our co-marketing dollars with exchanges translated into real fund flows."For Mars, the ability to measure previously unmeasurable activities has transformed decision-making. "We are using math to liberate creativity," said Gülen Bengi, lead global chief marketing officer for Mars and global chief growth officer for Mars Snacking. "Our fans and communities create billions of organic conversations and content about our brands. When a viral moment happens, we normally know it's directionally positive, but we can't attribute the sales uplift or its place in the customer journey. Alembic's Causal AI is a breakthrough, allowing us to move beyond correlation to see exactly how that organic conversation created a sequence that directly impacted sales."The platform can predict revenue, close rates and customer acquisition up to two years in advance with 95% confidence, according to Puig. "What they were doing before was they actually literally did not know about certain things," he said, describing how customers previously estimated the value of stadium naming rights or major sponsorships without ever measuring actual dollar impact. "Now you can go and be like it had this effect on this much P&L, and this is where it's flowing, and you can know within days or near real time."Why Google, Meta and Nielsen can't easily replicate what Alembic builtAlembic operates in a competitive landscape that includes traditional marketing measurement vendors like Nielsen, analytics platforms from Google and Meta, and emerging AI-powered analytics startups. But Puig argues the company has built structural advantages that would be difficult to replicate.First, the company's causal models rely on proprietary mathematics developed over years and protected by patents. "You would have to start from scratch," Puig said. "This is not like an LLM that uses a transformer that has a paper, and you could attempt to recreate. You'd actually have to go and recreate the methodology from scratch."Second, the massive computing requirements create a natural barrier. Alembic operates at "foundational model levels of compute, not like even something you would run from [AWS] Sagemaker," Puig said. "We're talking about hundreds of millions of dollars a year" in equivalent cloud costs.Third, the data sovereignty requirements of enterprise customers create opportunities for neutral third parties that hyperscale cloud providers struggle to address. As one venture capital investor noted, enterprises increasingly worry about putting strategic data into systems owned by potential competitors.Finally, Alembic's ability to work with messy, fragmented data reflects years of engineering that preceded its causal AI breakthrough. "The first four [or] five years of the company's life was building that giant signal processor that dealt with messy data," Puig said. "We would not be able to do it if we had not taken all that time."Why Alembic's contrarian bet on private data could reshape enterprise AIThe $145 million funding round validates a contrarian bet in an AI landscape dominated by the race to build ever-larger language models. While OpenAI, Anthropic and others compete on whose chatbot can write better code or answer more trivia questions, Alembic is building infrastructure for a different kind of intelligence — one that understands cause and effect in the messy, proprietary data that defines each company's unique competitive position.The company's evolution from a bootstrapped startup running simulations on Mac Pros to operating one of the world's fastest private supercomputers mirrors the broader maturation of enterprise AI. As the technology moves from experimentation to mission-critical deployment, companies need more than general-purpose models trained on public data. They need systems that can process their private information to answer questions their competitors cannot.Puig's thesis — that private data becomes the key differentiator as public models converge — resonates with how other technologies evolved. Search engines commoditized access to public information, making proprietary data more valuable. Cloud computing made infrastructure a utility, elevating the importance of what you build on top of it. If large language models similarly converge in capability, the competitive advantage flows to whoever can best extract intelligence from data others cannot access.The company is already testing its technology beyond marketing analytics. Pilots are underway in robotics, where causal models could help autonomous systems understand how actions lead to outcomes. New product lines are launching, including the GPU-accelerated database that customers are buying separately. The ambition, Puig said, is to become "the central nervous system" of the enterprise — the layer that connects cause and effect across every business function.Whether Alembic can deliver on that vision remains to be seen. The company operates in complex enterprise environments where sales cycles are long and integration challenges are significant. Competitors aren't standing still, and the technical moats that protect it today may erode as causal AI techniques become better understood.But for now, Alembic occupies a unique position. It has marquee customers achieving measurable results. It has infrastructure that would cost hundreds of millions to replicate on cloud platforms. It has proprietary mathematics refined over years of dealing with messy enterprise data. And it has $145 million to scale what Puig describes as a fundamental shift from correlation to causation.In his interview with VentureBeat, Puig drew a parallel to quantitative hedge funds that use mathematics to gain trading advantages that general-purpose AI cannot match. "ChatGPT still can't equal Renaissance Technologies," he said, referring to the secretive firm that has generated historic returns through quantitative models.The comparison captures Alembic's core insight: that in a world where everyone has access to the same general-purpose AI, sustainable advantage comes from specialized systems that understand the cause-and-effect relationships hiding in your data. It's a bet that the future of enterprise AI looks less like a universal chatbot and more like a private intelligence engine — one that, to Puig's original point, prevents your competitor from getting the same answer when they ask the same question.
- [Weibo's new open source AI model VibeThinker-1.5B outperforms DeepSeek-R1 on $7,800 post-training budget](https://venturebeat.com/ai/weibos-new-open-source-ai-model-vibethinker-1-5b-outperforms-deepseek-r1-on) — 03:31 · VentureBeat AI
  > Another day in late 2025, another impressive result from a Chinese company in open source artificial intelligence.Chinese social networking company Weibo's AI division recently released its open source VibeThinker-1.5B—a 1.5 billion parameter large language model (LLM) that is a fine-tuned variant of rival Chinese tech firm Alibaba's Qwen2.5-Math-1.5B. It's available now for free download and usage by researchers and enterprise developers—even for commercial purposes—under a permissive MIT License on Hugging Face, GitHub and ModelScope, with a technical report on open access science publishing site arxiv.org.And yet, despite its compact size, VibeThinker-1.5B achieves benchmark-topping reasoning performance on math and code tasks, rivaling or surpassing models hundreds of times its size, even outperforming Chinese rival DeepSeek's famed R1 that went viral at the start of this year—a 671-billion parameter model—on formal reasoning benchmark.It further eclipses Mistral AI's Magistral Medium and holds its own against Anthropic's Claude Opus 4 and OpenAI's gpt-oss-20B Medium, all while requiring a fraction of the infrastructure and investment.It also does so having been post-trained on a budget of merely $7800 USD for compute resources (3900 GPU hours on Nvidia H800s) — far less than the tens, or even hundreds, of thousands of dollars typically required to fine-tune models of similar or larger scale.Recall this is not the total cost of the model's development, however: LLMs are trained in stages. First comes pre-training, when the model learns basic language structure and general knowledge by predicting the next word across enormous amounts of text from the internet, books, and articles. This gives it fluency but not much sense of how to follow instructions or hold a conversationPost-training comes next, using much smaller, higher-quality datasets—typically collections of example questions, prompts, and expert-written answers—to teach the model how to respond helpfully, reason through problems, and align with human expectations. Still, Weibo's post-training cost effectiveness on VibeThinker-1.5B is noteworthy and should be commended.The open-source release upends assumptions about parameter scale, compute intensity, and the minimum viable size for high-performance LLMs.A Different Training Approach: Spectrum-to-SignalVibeThinker-1.5B owes its performance not to scale, but to the training framework behind it: the Spectrum-to-Signal Principle (SSP).Instead of optimizing a model purely for single-answer correctness (Pass@1), the SSP framework decouples supervised fine-tuning (SFT) and reinforcement learning (RL) into two distinct phases with different goals:SFT (“Spectrum Phase”): The model is trained to maximize diversity across potential correct answers, improving its Pass@K score. This builds a wide range of plausible solution paths.RL (“Signal Phase”): A second-stage reinforcement learning system (called MaxEnt-Guided Policy Optimization, or MGPO) is used to identify and amplify the most correct paths from this diverse solution pool. MGPO prioritizes problems where the model is most uncertain, using entropy-based weighting to focus learning.The authors argue this separation allows small models to explore reasoning space more effectively—achieving signal amplification without relying on massive parameter counts.VibeThinker-1.5B makes a compelling case that the industry’s reliance on parameter scaling as the only route to better reasoning performance may be outdated. By adopting a diversity-first training pipeline, WeiboAI has shown that smaller, more accessible models can match and even outperform billion-dollar systems in logic-heavy tasks.The low resource footprint is among the most significant aspects of VibeThinker-1.5B. At under $8,000, the post-training cost is 30–60x lower than models like DeepSeek R1 and MiniMax-M1, which cost between $294K and $535K to train.Performance Across DomainsDespite its small size, VibeThinker-1.5B delivers cross-domain reasoning that outpaces many larger open-source and commercial models:ModelAIME25LiveCodeBench v6GPQA-DiamondVibeThinker-1.5B74.451.146.7GPT-OSS-20B-Medium72.154.966.0Claude Opus 469.256.679.6MiniMax M1 (456B)74.662.369.2DeepSeek R1 (671B)70.065.971.5Kimi K2 (1.09T)49.553.775.1VibeThinker was benchmarked against both reasoning-centric models (Magistral, Claude, OpenAI o3-mini) and non-reasoning LLMs (GPT-4.1, Kimi K2, DeepSeek V3). Across structured reasoning benchmarks, the model consistently outperformed non-reasoning models, regardless of size:On AIME24 (math), it beat Kimi K2 (1.09T) by over 10 points (80.3 vs. 69.6).On LiveCodeBench v6, it surpassed Claude Opus 4 (51.1 vs. 47.4).On GPQA, it scored below GPT-4.1 and Claude, but still doubled its base model (from 16.4 to 46.7).This supports the authors’ claim that size is not the only path to reasoning capability—with proper training design, smaller models can reach or even exceed the performance of far larger systems in targeted tasks.Notably, it achieves parity with models hundreds of times larger on math and code, though it lags behind in general knowledge reasoning (GPQA), where larger models maintain an edge.This suggests a potential specialization trade-off: while VibeThinker excels at structured logical tasks, it has less capacity for wide-ranging encyclopedic recall, a known limitation of smaller architectures.Guidance for Enterprise AdoptionThe release includes recommended inference settings (temperature = 0.6, top_p = 0.95, max tokens = 40960).The model is small enough to be deployed on edge devices, including mobile phones and vehicle-embedded systems, while inference costs are estimated to be 20–70x cheaper than with large models.This positions VibeThinker-1.5B not just as a research achievement, but as a potential foundation for cost-efficient, locally deployable reasoning systems.Weibo’s Strategy and Market PositionWeibo, launched by Sina Corporation in 2009, remains a cornerstone of China’s social media ecosystem. Often described as China’s version of X (formerly Twitter), the platform blends microblogging, multimedia content, and trending-topic features with a regulatory environment shaped by tight government oversight. Despite counting 600 million monthly active users (more than twice that of X), investors are not optimistic about its advertising revenue growth potential in the near term, and Weibo is navigating intensifying competition from video-first platforms like Douyin, which are drawing younger users and increasing time-spent elsewhere. In response, Weibo has leaned into creator-economy monetization, live-streaming, and vertical video—adding tools for influencer engagement, e-commerce integration, and richer analytics for brands.The platform’s role as a digital public square also makes it a focus of regulatory scrutiny. Chinese authorities continue to apply pressure on issues ranging from content governance to data security. In September 2025, Weibo was among the platforms cited in official warnings, highlighting its ongoing exposure to policy risks.Weibo’s push into AI R&D—exemplified by the release of VibeThinker-1.5B—signals a shift in ambition. Beyond being a media platform, Weibo is positioning itself as a player in the next phase of Chinese AI development, using its capital reserves, user behavior data, and in-house research capacity to pursue adjacent technical domains.What It Means for Enterprise Technical Decision MakersFor engineering leaders and enterprise AI teams, VibeThinker’s release has practical implications for everything from orchestration pipelines to cost modeling. A 1.5B-parameter model that outperforms 100x larger models on math and programming tasks doesn’t just save compute—it shifts the architectural balance. It enables LLM inference on constrained infrastructure, reduces latency at the edge, and lowers the barrier to entry for applications that otherwise would have required API access to closed, frontier-scale models.That matters for enterprise ML leads trying to deploy reasoning-capable agents within existing systems, or for platform owners tasked with integrating LLMs into automated workflows. It also speaks to those running reinforcement learning from human feedback (RLHF) pipelines or managing inference optimization across hybrid cloud environments. The model’s post-training methodology—particularly its entropy-targeted reinforcement learning approach—offers a roadmap for teams looking to refine smaller checkpoints instead of relying on large-scale pretraining.VibeThinker’s benchmark transparency and data decontamination steps also address another emerging priority in enterprise AI: auditability. While its performance on general-knowledge tests still trails large frontier models, its task-specific reliability makes it an attractive candidate for controlled environments where correctness matters more than coverage.In short, VibeThinker-1.5B isn’t just a research milestone—it’s a strong candidate for practical enterprise use, deployment and learnings. It suggests that a new class of compact, reasoning-optimized models is viable for enterprise use cases that were previously the domain of far larger systems. For organizations trying to balance cost, latency, interpretability, and control, it’s a good new option to the long, growing list of Chinese open source offerings.
