# AI News for 2025-11-12 (Asia/Taipei)

Collected 24 article(s).

- [UMaine launches internships in AI, digital twins for the blue economy - The University of Maine](https://news.google.com/rss/articles/CBMirgFBVV95cUxORzBmSm5lUUhscGhaSlVLVzg1ZXFVQnVGMm5BczlXckhmVEtZblpSeGtRWV85MFFEN1VGLWpKb2xpMWVZcnZqb1k5c1FRLTM1RFBhRm1Ubml6ZUN5T3lHQlhvLWxnN25OT1FnX25mVE9FNVpZV0JHWjd2VWFyMU1rSGJXTmF1aXZlTmxRWnBraExaUENRUGlZR0tIYkVfTUtuVDYwQVcwRnd4bzFGcnc?oc=5) — 23:48 · Google News (AI)
  > UMaine launches internships in AI, digital twins for the blue economy  The University of Maine
- [Anthropic Commits $50 Billion to Build AI Data Centers in US - Bloomberg.com](https://news.google.com/rss/articles/CBMitAFBVV95cUxPWWNCLW9oVlpVYTNNVXJBTm1FcVBUVGo0c0xtQ1AtRlBTdXYtQm1Nb2FDRThocVZ5RVhMbHA4UzFPbUtVY3NCeHhrWkZyYWpHRXR2TnF2TXhlTTNiT0NNOGlpV244V2lvbC1jbGhzVWJkRFlzN0pXVkZyaHN4Q0VQaFVvNnB0TnBxSEFHaTJaWE1WWnFsTXUzRFE3TUVvaVdaLWpDc1FidXJHdVA1RVprclFiNGk?oc=5) — 23:30 · Google News (AI)
  > Anthropic Commits $50 Billion to Build AI Data Centers in US  Bloomberg.com
- [Chevron Picks Texas for First AI Data Center Power Project - Bloomberg.com](https://news.google.com/rss/articles/CBMitAFBVV95cUxPYk9OUmhBZjJCTTMybnJNZWFYenRaN19wQVFmclZSZmJObFU5bklnWG1qNkZOZDNZNHk0T3lQSVdkUEg2cVNRT0VmRGVsS2hWWGVkYnQ3bHFVSmtPNTV5eFA5Z0s2enpNeTFxQTFRdm5fU0p4d2d4NF85ckl0dDVoX0VEWDJMVnVRbWYzR2pxLTNmTW9aMHpnS0dyNWwwTDNuRC1mZWo4UnFtSGdmVGxPY1JzQ0g?oc=5) — 23:13 · Google News (AI)
  > Chevron Picks Texas for First AI Data Center Power Project  Bloomberg.com
- [Physicists discuss the future of machine learning and artificial intelligence - Physics World](https://news.google.com/rss/articles/CBMiqAFBVV95cUxPTS1Vc2VMS2xlam9IQkExWmxzWmVxQjBXSTk0UjEtYXdxdWpid19ZS2FXSFlpNUgtcnVTbkNFOFJOdXVBdUt1M0pZS3pCdzBJTlJEYUUxNHIzd1djbG9hMDRWX0ZGS1pvaE1zOGZZbGp2czhXanp0VExhY2ZwT1BDdHp6RHljYU5TYWMzT2p5TTZITEY2cnNCUXdVY3pkMDJ5cGoxQ0ZUcjM?oc=5) — 23:02 · Google News (AI)
  > Physicists discuss the future of machine learning and artificial intelligence  Physics World
- [Anthropic to spend $50 billion on U.S. AI infrastructure, starting with Texas, New York data centers - CNBC](https://news.google.com/rss/articles/CBMigwFBVV95cUxNOERib1VBN01URkJsSnlVQmhSWF8tWGE1cWJsZHQ5UEZTUXExLUpiamNJelpTZWkzYzVMVUV6OTZuZ3k5OTZ2RkYxbXBlbFljSGtZZWdWc1JYTU5tYXVyUHF2M1piUkpOcEpxUUp4VmNES2dlQ21PVThDeGhCRWxUOUItY9IBiAFBVV95cUxOYTRHUmk3di1BcUxFQUVuaWVkRTdHbm0zc0dEMDVta0FsV3ZPREJJeGJmRmlwUHZMbDZ1cEp3akRkWEhabGVaM1BBdkJsZmVaeXdkWFh3SUtIeXVPSXoxU1NMNHFZV0dlZ3h6WHUyc1c5d0FBSS1KdXJFQ3M2SDZ2eFNEb0RVNl9v?oc=5) — 23:00 · Google News (AI)
  > Anthropic to spend $50 billion on U.S. AI infrastructure, starting with Texas, New York data centers  CNBC
- [AMD stock soars 7% on strong growth projections as CEO Su calls AI spending ‘the right gamble’ - CNBC](https://news.google.com/rss/articles/CBMidEFVX3lxTE1kMlBvcVdlNVVteXd5Uy1CRW5paVA1dEJ4TVUzSndUaE1oMVBhOThBUXRmOThjQnRFN2F3eEFkY19zcldlOURTQjVDb29RLVNtbUQxT1J0MENqRDZGa1owbjdjRHpmQnNIY1IzUlczN2Vvelhq0gF6QVVfeXFMTzJaSVVUcjlwZk9ZNXBMVGFNTHcwcy0yenRDeW03SkZzZlY5bVE2RDVUSjFKRzJPN0c2TGxRMGFsRVJvTFZSYkVBNURKbDVIc1hKLXBYVDFsUF9YbVhPQmxfaS1BM1kySEVDYTVXUzE5d3ZGQktNNFlFMFE?oc=5) — 22:24 · Google News (AI)
  > AMD stock soars 7% on strong growth projections as CEO Su calls AI spending ‘the right gamble’  CNBC
- [Five Things to Know… Yale Law School’s AI lab - YaleNews](https://news.google.com/rss/articles/CBMiZ0FVX3lxTE9MaDI3UUNPVU5ENDduSWdtS2FfUGVLUW9kYzNSNnBhRjFDVU5GcEFETFU0RzBwVGpKRE5XLTFhN19xVlRBa2RNNTlHeUwxajVZbjRFNXh1bGhWSVlrWlZSbjA2MHdHWnM?oc=5) — 22:00 · Google News (AI)
  > Five Things to Know… Yale Law School’s AI lab  YaleNews
- [How Deductive AI saved DoorDash 1,000 engineering hours by automating software debugging](https://venturebeat.com/ai/how-deductive-ai-saved-doordash-1-000-engineering-hours-by-automating) — 22:00 · VentureBeat AI
  > As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: Engineers are drowning in debugging work, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it's creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.Deductive AI, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by CRV, with participation from Databricks Ventures, Thomvest Ventures, and PrimeSet, to commercialize what it calls "AI SRE agents" that can diagnose and help fix software failures at machine speed.The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause."The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it's made of a million other needles, it's constantly reshuffling itself, and is on fire — and every second you don't find it equals lost revenue," said Sameer Agarwal, Deductive's co-founder and chief technology officer, in an exclusive interview with VentureBeat.Deductive's system builds what the company calls a "knowledge graph" that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.The technology has already shown measurable impact at some of the world's most demanding production environments. DoorDash's advertising platform, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes."Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue," said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. "Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes."DoorDash estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact "in millions of dollars," according to Ansari. At location intelligence company Foursquare, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.Why AI-generated code is creating a debugging crisisThe timing of Deductive's launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain."Vibe coding," a term popularized by AI researcher Andrej Karpathy, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as "redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns" that accumulate over time."Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns," Agarwal told Venturebeat. "In many ways, we now need AI to help clean up the mess that AI itself is creating."The claim that engineers spend roughly half their time on debugging isn't hyperbole. The Association for Computing Machinery reports that developers spend 35% to 50% of their time validating and debugging software. More recently, Harness's State of Software Delivery 2025 report found that 67% of developers are spending more time debugging AI-generated code."We've seen world-class engineers spending half of their time debugging instead of building," said Rakesh Kothari, Deductive's co-founder and CEO. "And as vibe coding generates new code at a rate we've never seen, this problem is only going to get worse."How Deductive's AI agents actually investigate production failuresDeductive's technical approach differs substantially from the AI features being added to existing observability platforms like Datadog or New Relic. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls "code-aware reasoning"—the ability to understand not just that something broke, but why the code behaves the way it does."Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior," Agarwal explained. "These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills."The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.The critical difference from rule-based automation is Deductive's use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model."Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome," Agarwal said. "It learns how to think through problems, not just point them out."At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive's investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services."Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories," Ansari said. "Deductive was able to explain not just what changed, but how and why it impacted production behavior."The company keeps humans in the loop—for nowWhile Deductive's technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now."While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply," Agarwal said. "We believe maintaining a human in the loop is essential for trust, transparency and operational safety."However, he acknowledged that "over time, we do think that deeper automation will come and how humans operate in the loop will evolve."Databricks and ThoughtSpot veterans bet on reasoning over observabilityThe founding team brings deep expertise from building some of Silicon Valley's most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created BlinkDB, an influential system for approximate query processing. He was among the first engineers at Databricks, where he helped build Apache Spark. Kothari was an early engineer at ThoughtSpot, where he led teams focused on distributed query processing and large-scale system optimization.The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV's Max Gazor, the round included participation from Ion Stoica, founder of Databricks and Anyscale; Ajeet Singh, founder of Nutanix and ThoughtSpot; and Ben Sigelman, founder of Lightstep.Rather than competing with platforms like Datadog or PagerDuty, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn't store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.With fresh capital and early customer traction at companies like DoorDash, Foursquare, and Kumo AI, Deductive plans to expand its team and deepen the system's reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.DoorDash's Ansari offers a pragmatic endorsement of where the technology stands today: "Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation."In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.
- [A Policy Roadmap for Secure AI by Design - Palo Alto Networks](https://news.google.com/rss/articles/CBMiiAFBVV95cUxQeUs3dm5kUEFqRUZSQ0twdFhvY0k3Qmh1a3dpTGdJdUFZVjR5SjNkVjlNeUFaczduQlNvVWl0WVo1djdhYTJmNWdTaE9Xd2ZyUFlEVGxnME93aGgtSFJhNUltWjlrc2h4VTA1NzR2dVVpTW1mOG5STVpKNU1qX1k3SDJnLTJ4UU5x?oc=5) — 21:39 · Google News (AI)
  > A Policy Roadmap for Secure AI by Design  Palo Alto Networks
- [The Download: how to survive a conspiracy theory, and moldy cities](https://www.technologyreview.com/2025/11/12/1127881/the-download-how-to-survive-a-conspiracy-theory-and-moldy-cities/) — 21:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. What it’s like to be in the middle of a conspiracy theory (according to a conspiracy theory expert) —Mike Rothschild is a journalist and an expert on the growth and impact of conspiracy…
- [Nvidia supplier Foxconn bullish on AI demand, teases OpenAI announcement - Reuters](https://news.google.com/rss/articles/CBMiugFBVV95cUxQa3BMamhyVUdLdEhMdmNoUVBBSHgtSmhTVzI5TnFHTS1McmV6VHVMNzZ6N2phRkVJUFRiOUVFdWFoMDNjUmxNdlBhajk5VGxaNkw1N3dqVmd1OTZIQ2NqVGFTYzk2VkxSS1c2LWtoUmxaVG00M3FEUUYxcUxXRUxoT1lESGIxUGd2eVZIdFpILXBsWER1SUp6allINkNiVzZoS1BtX0pZY3NYdk1JSWxfMFZVUEdpQkV0N3c?oc=5) — 20:19 · Google News (AI)
  > Nvidia supplier Foxconn bullish on AI demand, teases OpenAI announcement  Reuters
- [AI stock boom leaves many behind, economist says: 'It really widens the wealth and income gap' - CNBC](https://news.google.com/rss/articles/CBMibkFVX3lxTE9nQWFGMnBYTmFraVRFWS16R3JmSFplUmpMdW9uMmh5S2k0WWxXQnV0cTRMbEc5REZSWmtXUTI2dGhURVpGUy1ESmVicU9mUUNXYUEzMW9jZThvZnpPUHl4bVp2MlA5YUpjNlluN0lB0gFzQVVfeXFMTUhnbUNnMnpDT0w5TnpDakdXdHg3TUF2azZJSDRZUWI2VDY5dGNramtQNFRod3pXR2dDQlZzMVUtQVhQOXZ5aGFIUzdfb1hpT0EtWU9NWEsyUm5KUEZXWmhPdC1JOU9fM2tsQnZheFdzU1czWQ?oc=5) — 20:15 · Google News (AI)
  > AI stock boom leaves many behind, economist says: 'It really widens the wealth and income gap'  CNBC
- [Companies Begin to See a Return on AI Agents - The Wall Street Journal](https://news.google.com/rss/articles/CBMijwNBVV95cUxPMWx1VDdCcFBRRFR2RFctMlkwWjd6cElVem0wd0FFZ3JlQW5qQ05RejVvSVRWNmpfZWpxMS16ZlpRaWhmc0Jnd28xdGhZV3BveXF4emgwT3o2S3JEZVFkTHU1VzI5QlQ2d1RnZE00SmY3Q1pUT1pIVkduaHdnX0hNaGpQZXJiTkgxWGtlQ0Q1bjB5RTVqY0ZFQU5qV1VlQi10TGZDZVFjdDgyU3cyQm1IT3pNSnlKWnM1emdZU2tnZzY4RUdWUjVpeUNqZXhTa0U1TVY2TThodTJnTWd6T3dBX1VVTFpvSzRKTHhwUFBMelNRdjZ5NnRyeWlvNHR0YjNVN2FzR1JTNUV0M01ieXI4SjVIWnFSeFN0VzFMcjJhTkZNaXVfQWZGUGR5bkNvSnlkTUEzN19YODdrNHhFR2tKUEl5dW1UR3dQc2ppYmRVTlpwRXNjWWdqVWxUbFhEVkU3M2s2aVA1d2xvNTRJVlBfLUUxV3NmbjhOTGstY1NDYXFQOFU1eDVkRDBfVXRqVTA?oc=5) — 20:00 · Google News (AI)
  > Companies Begin to See a Return on AI Agents  The Wall Street Journal
- [Watch: Russia's AI robot falls seconds after being unveiled - BBC](https://news.google.com/rss/articles/CBMiV0FVX3lxTE5aMmR0cnBQQzFyVmhkOGdBT2t2Zno5em14Q3REVjAyYkR6MFFUMzc2ZWtXaUc1cDJEcG8xRHZnejJwd3FSUkZoeS1FY1diQlVOWXlCVkkwSQ?oc=5) — 19:33 · Google News (AI)
  > Watch: Russia's AI robot falls seconds after being unveiled  BBC
- [Are you listening to bots? Survey shows AI music is virtually undetectable - Reuters](https://news.google.com/rss/articles/CBMiwAFBVV95cUxPTmhEWXVON0toZmFvMmVUS21ubVlBUzlWckFKbk5FRVpBdmM3S2dSN0NxYWNXYk5KeXBoaXhrQXZJOXRJWHBaM2dZSExHUUFsZGQ2bzFoRkNKZkxGMzFzTGg0TE9ZSlU1RTBzS3hPNVhfcFJSXzBqNkFyMTNuNGloY3d5a0JIUzRqTkJuZVl1a1NoM3BUX0VDcFlTVHg1NTFyckllTWU2V0t2LWNtckhPeWNxYUxOQVF2VF9QQjc0bUM?oc=5) — 19:33 · Google News (AI)
  > Are you listening to bots? Survey shows AI music is virtually undetectable  Reuters
- [All of My Employees Are AI Agents, and So Are My Executives - WIRED](https://news.google.com/rss/articles/CBMihwFBVV95cUxNU05OdFlqcWpPWjlWNjQ4ZnNyTXVTQUNkWTJwZzBPYmhjRmgwTmJja1NaSlhYTndtME4zVXpUVXluRm1JVlk4OWtxNUN4akF2LTQyRTc5eUdCZlIwNzYtbXdacVhfbHg1bnRGeFpGOUdxUTFoXzJDaEVWOU5CQ1dqX1ZnUXY0cHM?oc=5) — 19:00 · Google News (AI)
  > All of My Employees Are AI Agents, and So Are My Executives  WIRED
- [Meet chatbot Jesus: Churches tap AI to save souls — and time - Axios](https://news.google.com/rss/articles/CBMihAFBVV95cUxNZl9BbXl4Y0hBX1JfaWZxcE1hT3VSY2ZIQ00tbGEzM0tabVdsRURJMVBfaUdxYUFoQzdMUkxPUE4zUy1KWGY0RHNRY1p3aGV4VmFZdkEwN2VVNTEtVjJyQ0t6TERrZzI2c1ZXYmp2aUFJLTN6UUE3WVpvczl4dW1YSHNIVFI?oc=5) — 18:45 · Google News (AI)
  > Meet chatbot Jesus: Churches tap AI to save souls — and time  Axios
- [The AI Boom Is Looking More and More Fragile - WSJ - The Wall Street Journal](https://news.google.com/rss/articles/CBMijgNBVV95cUxQejJ4QjQ1OVJ0R2Y3Y19vOEx0eUpWR2tCOWU0dHBBS3hGVzBEdjFOSkdqLWhmZmFiTUppcm1JdnRId1BIX0VCWkloVGtXQWZaSlAzeTJPb1ZycjZ4dEUwS0lncFpYTDNpVGctMTdrUHZFVWwtWmNrVDFjcnFSUmQwMmZCVmVNUmxqOG9ZXy03MC1LR3Z3OHFlSG80eXQ4UG53OERIaG1jZkM2MEhjWXFWclFOazRYdUJxUEEtVFVDOWZmcDlha3lQUW5keXRqWFQ1ampBM2J2TnJoOXd5eWtUSFQ3TXJ4WEtXUHBtb3ZPX2VtUXI3Ri00VkxWVnk3R1hvV0dKNW1FbnhRamQ0VERKcjkyMTZPRVEwcFNfRmNKa2xQUFdTTGVvMm1WODJSYlBxVy1SXzktOWJlMGpHRUpReDZ6c0hUTThUb21SZ0NRTWprZkxSR0psU0NFNVhRQ1RaRkxBb3d3VE9XT0FYbHlWeHVoelhMMjJiMFFvZDd0RV9lSF91QlE1MjJfYzctQQ?oc=5) — 18:30 · Google News (AI)
  > The AI Boom Is Looking More and More Fragile - WSJ  The Wall Street Journal
- [Improving VMware migration workflows with agentic AI](https://www.technologyreview.com/2025/11/12/1124919/improving-vmware-migration-workflows-with-agentic-ai/) — 18:11 · MIT Technology Review (AI)
  > For years, many chief information officers (CIOs) looked at VMware-to-cloud migrations with a wary pragmatism. Manually mapping dependencies and rewriting legacy apps mid-flight was not an enticing, low-lift proposition for enterprise IT teams. But the calculus for such decisions has changed dramatically in a short period of time. Following recent VMware licensing changes, organizations are…
- [Who Pays When A.I. Is Wrong? - The New York Times](https://news.google.com/rss/articles/CBMiigFBVV95cUxOWHpkSGl1czloVzFBTS1WYndKbmtiRjRQaGpienZhb0pLVEdIRG44cHQwTmlFZ09QWWdocTR0Z2toR2w3b0FlaUZFY1JxamE5X29ZSFJ0OURTelhHaEMzRFBYZUpEc2FYeDc4a1RQQVlFbE1hQXBOMFliZVY5b0Y2cnpwVHVMcjE3bVE?oc=5) — 18:01 · Google News (AI)
  > Who Pays When A.I. Is Wrong?  The New York Times
- [OpenAI reboots ChatGPT experience with GPT-5.1 after mixed reviews of GPT-5](https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5) — 13:00 · VentureBeat AI
  > ChatGPT is about to become faster and more conversational as OpenAI upgrades its flagship model GPT-5 to GPT-5.1.OpenAI announced two updates to the GPT-5 series: GPT-5.1 Instant and GPT-5.1 Thinking. Both models are now accessible on ChatGPT. GPT-5.1 Instant, essentially the default and most-used model, is now “warmer, more intelligent, and better at following your instructions,” according to OpenAI. Meanwhile, GPT-5.1 Thinking is an advanced reasoning model that responds faster for simple tasks and more persistently on complex ones.“We heard clearly from users that great AI should not only be smart, but also enjoyable to talk to,” OpenAI said in a blog post. “GPT-5.1 improves meaningfully on both intelligence and communication style.” The company added that both models offer a way for users to “shape ChatGPT’s tone,” allowing people to control how the chat platform responds depending on the conversation they are having. Both models were rolled out to ChatGPT Pro, Plus, Go and Business users, as well as the free tier. Those on the Enterprise and Edu plans will get a seven-day early-access toggle for the models before GPT-5.1 becomes the default model. OpenAI said the models can also be accessible through the API, both with adapted reasoning. OpenAI has noted that it will soon update GPT-5 Pro to version 5.1. Instant and Thinking models The 5.1 tag reflects improvements to the base model, and OpenAI considers these part of the GPT-5 family, trained on the same stack and data as its reasoning models. The biggest difference between 5.1 and 5 is its more natural and conversational tone, OpenAI CEO for Applications Fidji Simo said in a Substack post. “Based on early testing, it often surprises people with its playfulness while remaining clear and useful,” OpenAI said in its post. Instant can use adaptive reasoning to help it decide when it needs to think about its answers, especially when it comes to more complicated questions. OpenAI noted that it has improved the model's instruction following, so that while it continues to respond quickly, it also directly addresses the user’s query. Recent model releases, such as Baidu’s ERNIE-4.5-VL-28B-A3B-Thinking, have been outperforming GPT-5 in benchmarks like instruction-following. GPT-5.1 Thinking can figure out on its own how much reasoning power it should devote to a prompt. It adapts to the type and complexity of a query, so it will take longer to answer a fuller, complex question than a simple summary request. OpenAI said evaluations showed that GPT-5.1 Thinking spends less time and therefore uses fewer tokens on simple tasks compared to GPT-5, outperforming the base model in speed of response. One thing enterprises should note is that GPT-5.1 Thinking answers “with less jargon and fewer undefined terms.” OpenAI said removing jargony responses makes Thinking more approachable when it comes to explaining technical concepts.More personalizationAnother big update to ChatGPT is increased personalization. This allows users to toggle between a friendly and authoritative chat platform experience in their conversations. ChatGPT already allows users to choose preset options for model tone, but the new update expands these options “to better reflect the most common ways people use ChatGPT.”Options include "default," "friendly" (formerly "listener"), "efficient" (previously "robot"), "professional," "candid" and "quirky." Two other personalities, "cynical" and "nerdy," remain unchanged. “We think many people will find that GPT-5.1 does a better job of bringing IQ and EQ together, but one default clearly can’t meet everyone’s needs," Simo said. "That’s why we’re also making it easier to customize ChatGPT with a range of presets to choose from. The model has the same capabilities whether you select default or one of these options, but the style of its responses will differ — more formal or familiar, more playful or direct, more or less jargon or slang. Of course, eight personalities still don't cover the full range of human diversity, but we know from our research that many people prefer simple, guided control over too many settings or open-ended options."People can also adjust how much ChatGPT uses emojis. OpenAI offers granular controls for responses and is experimenting with the ability to make the models more concise, warm or scannable.Saving a rolloutOpenAI’s GPT-5 rollout was…less than perfect. While company executives, including CEO Sam Altman, touted the new model’s capabilities, a decision to initially sunset older and beloved models on ChatGPT was met with dissatisfaction. Worse yet, many early adopters found that GPT-5 didn’t perform better than older options in domains such as math, science and writing. This led Altman to walk back some of his statements around model removal, blaming performance issues on GPT-5’s router. The router, which automatically directs queries to the most suited models, is not going away, as GPT-5.1 Auto will route prompts to the model type that can best answer queries.  OpenAI is careful to note that GPT-5 models Instant, Thinking and Pro are still available in ChatGPT’s model dropdown, although paid subscribers only have three months to compare these older versions with the 5.1 update. The sunset period for GPT-5, however, will not impact models like GPT-4o.“Going forward, when we introduce new ChatGPT models, our approach is to give people ample space to evaluate what’s changed and share feedback, allowing us to continue innovating our frontier models while transitioning smoothly,” the company said. “Sunset periods will be communicated clearly and with plenty of advance notice.”
- [Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini](https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5) — 08:00 · VentureBeat AI
  > Baidu Inc., China's largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from Google and OpenAI on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.The model, dubbed ERNIE-4.5-VL-28B-A3B-Thinking, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.What sets Baidu's release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory."Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities," Baidu wrote in the model's technical documentation on Hugging Face, the AI model repository where the system was released.The company said the model underwent "an extensive mid-training phase" that incorporated "a vast and highly diverse corpus of premium visual-language reasoning data," dramatically boosting its ability to align visual and textual information semantically.How the model mimics human visual problem-solving through dynamic image analysisPerhaps the model's most distinctive feature is what Baidu calls "Thinking with Images" — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks."The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information," according to the model card. When paired with tools like image search, Baidu claims this feature "dramatically elevates the model's ability to process fine-grained details and handle long-tail visual knowledge."This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.The model also supports what Baidu describes as enhanced "visual grounding" capabilities with "more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios," suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.Baidu's performance claims draw scrutiny as independent testing remains pendingBaidu's assertion that the model outperforms Google's Gemini 2.5 Pro and OpenAI's GPT-5-High on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.The company released the model under the permissive Apache 2.0 license, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption."Apache 2.0 is smart," wrote one X user responding to Baidu's announcement, highlighting the competitive advantage of open licensing in the enterprise market.According to Baidu's documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as "multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks," aided by what the company characterizes as "large-scale reinforcement learning." For STEM problem solving, Baidu claims that "leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos." The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.For video understanding, Baidu claims the model possesses "outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video." Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.Inside the mixture-of-experts architecture that powers efficient multimodal processingUnder the hood, ERNIE-4.5-VL-28B-A3B-Thinking employs a Mixture-of-Experts (MoE) architecture — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.This approach offers substantial practical advantages for enterprise deployments. According to Baidu's documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model's capabilities. The company used "cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency."Baidu also notes that in response to "strong community demand," the company "significantly strengthened the model's grounding performance with improved instruction-following capabilities."The new model fits into Baidu's ambitious multimodal AI ecosystemThe new release is one component of Baidu's broader ERNIE 4.5 model family, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship ERNIE-4.5-VL-424B-A47B with 424 billion total parameters down to a compact 0.3 billion parameter dense model.According to Baidu's technical report on the ERNIE 4.5 family, the models incorporate "a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality."This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design "has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks."The company reported achieving 47% Model FLOPs Utilization (MFU) — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the PaddlePaddle deep learning framework developed in-house.Comprehensive developer tools aim to simplify enterprise deployment and integrationFor organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through ERNIEKit, what the company describes as an "industrial-grade training and compression development toolkit."The model offers full compatibility with popular open-source frameworks including Hugging Face Transformers, vLLM (a high-performance inference engine), and Baidu's own FastDeploy toolkit. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model's "reasoning-parser" and "tool-call-parser" capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.The company also offers FastDeploy, a proprietary inference toolkit that Baidu claims delivers "production-ready, easy-to-use multi-hardware deployment solutions" with support for various quantization schemes that can reduce memory requirements and increase inference speed.Why this release matters for the enterprise AI market at a critical inflection pointThe release comes at a pivotal moment in the enterprise AI market. As organizations move beyond experimental chatbot deployments toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.Several enterprise use cases appear particularly well-suited to the model's capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model's grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.The model's efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars."With all these new models, where's the best place to actually build and scale? Access to compute is everything," wrote one X user in response to Baidu's announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy ERNIE-4.5-VL-28B-A3B-Thinking in production applications without ongoing licensing fees or usage restrictions.Competition intensifies as Chinese tech giant takes aim at Google and OpenAIBaidu's release intensifies competition in the vision-language model space, where Google, OpenAI, Anthropic, and Chinese companies including Alibaba and ByteDance have all released capable systems in recent months.The company's performance claims — if validated by independent testing — would represent a significant achievement. Google's Gemini 2.5 Pro and OpenAI's GPT-5-High are substantially larger models backed by the deep resources of two of the world's most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated."Impressive that ERNIE is outperforming Gemini 2.5 Pro," wrote one social media commenter, expressing surprise at the claimed results.However, some observers counseled caution about benchmark comparisons. "It's fascinating to see how multimodal models are evolving, especially with features like 'Thinking with Images,'" wrote one X user. "That said, I'm curious if ERNIE-4.5's edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart" understanding rather than general-purpose vision tasks.Industry analysts note that benchmark performance often fails to capture real-world behavior across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.Technical limitations and infrastructure requirements that enterprises must considerDespite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.The model's context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu's documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.Questions also remain about the model's behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu's documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.What technical decision-makers need to evaluate beyond the benchmark numbersFor technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.The model's MoE architecture, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.The "Thinking with Images" feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu's documentation suggests this capability works best "when paired with tools like image zooming and image search," implying that organizations may need to build additional infrastructure to fully leverage this functionality.The model's video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.Organizations considering deployment should also evaluate Baidu's ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the Apache 2.0 license ensures the model remains available, future improvements and support depend on Baidu's strategic priorities.Developer community responds with enthusiasm tempered by practical requestsEarly response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices."Release MNN and GGUF so I can run it on my phone," wrote one developer, highlighting demand for mobile deployment options.Other developers praised Baidu's technical choices while requesting additional resources. "Fantastic model! Did you use discoveries from PaddleOCR?" asked one user, referencing Baidu's open-source optical character recognition toolkit.The model's lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. "ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history," joked one observer. "But hey, if you're outperforming Gemini-2.5-Pro with only 3B active params, you've earned the right to a dramatic name!"Baidu plans to showcase the ERNIE lineup during its Baidu World 2025 conference on November 13, where the company is expected to provide additional details about the model's development, performance validation, and future roadmap.The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like ERNIE-4.5-VL-28B-A3B-Thinking is reshaping the economics of AI deployment and accelerating adoption across industries.Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: "Open source plus commercial use equals chef's kiss. Baidu not playing around."
- [Meta’s SPICE framework lets AI systems teach themselves to reason](https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason) — 06:21 · VentureBeat AI
  > Researchers at Meta FAIR and the National University of Singapore have developed a new reinforcement learning framework for self-improving AI systems. Called Self-Play In Corpus Environments (SPICE), the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.The challenge of self-improving AIThe goal of self-improving AI is to create systems that can enhance their capabilities by interacting with their environment. A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. Factual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”How SPICE worksSPICE is a self-play framework where a single model acts in two distinct roles. A "Challenger" constructs a curriculum of challenging problems from a large corpus of documents. A "Reasoner" then attempts to solve these problems without access to the source documents. This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.The adversarial dynamic between the two roles creates an automatic curriculum. The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner's capability (not too easy and also not impossible). The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.SPICE in actionThe researchers evaluated SPICE on several base models, including Qwen3-4B-Base and OctoThinker-3B-Hybrid-Base. They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed "Strong Challenger" (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. In one experiment, the Reasoner's pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.
- [Only 9% of developers think AI code can be used without human oversight, BairesDev survey reveals](https://venturebeat.com/ai/only-9-of-developers-think-ai-code-can-be-used-without-human-oversight) — 03:43 · VentureBeat AI
  > Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to BairesDev’s latest Dev Barometer report published today. VentureBeat was given an exclusive early look and the findings below come directly from that report. The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.From Coders to StrategistsAmong those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with VentureBeat conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”Erolin said the company is watching developers evolve from individual contributors into system thinkers.“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.Realism About AI’s LimitsDespite widespread enthusiasm, developers remain cautious about AI’s reliability.Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security. Only 9% trust it enough to use without human oversight.Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”That tempered optimism aligns with BairesDev’s previous Dev Barometer findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.A Year of UpskillingIn 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.The Dev Barometer findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.Leaner Teams, New PrioritiesDevelopers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”AI as an Industry StandardThe Q4 Dev Barometer frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.Background: Who BairesDev IsFounded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.The TakeawayThe Dev Barometer’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.
