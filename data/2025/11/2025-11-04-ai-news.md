# AI News for 2025-11-04 (Asia/Taipei)

Collected 5 article(s).

- [Why the for-profit race into solar geoengineering is bad for science and public trust](https://www.technologyreview.com/2025/11/04/1127532/why-the-for-profit-race-into-solar-geoengineering-is-bad-for-science-and-public-trust/) — 22:47 · MIT Technology Review (AI)
  > Last week, an American-Israeli company that claims it’s developed proprietary technology to cool the planet announced it had raised $60 million, by far the largest known venture capital round to date for a solar geoengineering startup. The company, Stardust, says the funding will enable it to develop a system that could be deployed by the…
- [The Download: the AGI myth, and US/China AI competition](https://www.technologyreview.com/2025/11/04/1127547/the-download-the-agi-myth-and-us-china-ai-competition/) — 21:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. How AGI became the most consequential conspiracy theory of our time —Will Douglas Heaven, senior AI editor  Are you feeling it? I hear it’s close: two years, five years—maybe next year! And I…
- [98% of market researchers use AI daily, but 4 in 10 say it makes errors — revealing a major trust problem](https://venturebeat.com/ai/98-of-market-researchers-use-ai-daily-but-4-in-10-say-it-makes-errors) — 16:00 · VentureBeat AI
  > Market researchers have embraced artificial intelligence at a staggering pace, with 98% of professionals now incorporating AI tools into their work and 72% using them daily or more frequently, according to a new industry survey that reveals both the technology's transformative promise and its persistent reliability problems.The findings, based on responses from 219 U.S. market research and insights professionals surveyed in August 2025 by QuestDIY, a research platform owned by The Harris Poll, paint a picture of an industry caught between competing pressures: the demand to deliver faster business insights and the burden of validating everything AI produces to ensure accuracy.While more than half of researchers — 56% — report saving at least five hours per week using AI tools, nearly four in ten say they've experienced "increased reliance on technology that sometimes produces errors." An additional 37% report that AI has "introduced new risks around data quality or accuracy," and 31% say the technology has "led to more work re-checking or validating AI outputs."The disconnect between productivity gains and trustworthiness has created what amounts to a grand bargain in the research industry: professionals accept time savings and enhanced capabilities in exchange for constant vigilance over AI's mistakes, a dynamic that may fundamentally reshape how insights work gets done.How market researchers went from AI skeptics to daily users in less than a yearThe numbers suggest AI has moved from experiment to infrastructure in record time. Among those using AI daily, 39% deploy it once per day, while 33% use it "several times per day or more," according to the survey conducted between August 15-19, 2025. Adoption is accelerating: 80% of researchers say they're using AI more than they were six months ago, and 71% expect to increase usage over the next six months. Only 8% anticipate their usage will decline.“While AI provides excellent assistance and opportunities, human judgment will remain vital,” Erica Parker, Managing Director Research Products at The Harris Poll, told VentureBeat. “The future is a teamwork dynamic where AI will accelerate tasks and quickly unearth findings, while researchers will ensure quality and provide high level consultative insights.”The top use cases reflect AI's strength in handling data at scale: 58% of researchers use it for analyzing multiple data sources, 54% for analyzing structured data, 50% for automating insight reports, 49% for analyzing open-ended survey responses, and 48% for summarizing findings. These tasks—traditionally labor-intensive and time-consuming — now happen in minutes rather than hours.Beyond time savings, researchers report tangible quality improvements. Some 44% say AI improves accuracy, 43% report it helps surface insights they might otherwise have missed, 43% cite increased speed of insights delivery, and 39% say it sparks creativity. The overwhelming majority — 89% — say AI has made their work lives better, with 25% describing the improvement as "significant."The productivity paradox: saving time while creating new validation workYet the same survey reveals deep unease about the technology's reliability. The list of concerns is extensive: 39% of researchers report increased reliance on error-prone technology, 37% cite new risks around data quality or accuracy, 31% describe additional validation work, 29% report uncertainty about job security, and 28% say AI has raised concerns about data privacy and ethics.The report notes that "accuracy is the biggest frustration with AI experienced by researchers when asked on an open-ended basis." One researcher captured the tension succinctly: "The faster we move with AI, the more we need to check if we're moving in the right direction."This paradox — saving time while simultaneously creating new work — reflects a fundamental characteristic of current AI systems, which can produce outputs that appear authoritative but contain what researchers call "hallucinations," or fabricated information presented as fact. The challenge is particularly acute in a profession where credibility depends on methodological rigor and where incorrect data can lead clients to make costly business decisions."Researchers view AI as a junior analyst, capable of speed and breadth, but needing oversight and judgment," said Gary Topiol, Managing Director at QuestDIY, in the report.That metaphor — AI as junior analyst — captures the industry's current operating model. Researchers treat AI outputs as drafts requiring senior review rather than finished products, a workflow that provides guardrails but also underscores the technology's limitations.Why data privacy fears are the biggest obstacle to AI adoption in researchWhen asked what would limit AI use at work, researchers identified data privacy and security concerns as the greatest barrier, cited by 33% of respondents. This concern isn't abstract: researchers handle sensitive customer data, proprietary business information, and personally identifiable information subject to regulations like GDPR and CCPA. Sharing that data with AI systems — particularly cloud-based large language models — raises legitimate questions about who controls the information and whether it might be used to train models accessible to competitors.Other significant barriers include time to experiment and learn new tools (32%), training (32%), integration challenges (28%), internal policy restrictions (25%), and cost (24%). An additional 31% cited lack of transparency in AI use as a concern, which could complicate explaining results to clients and stakeholders.The transparency issue is particularly thorny. When an AI system produces an analysis or insight, researchers often cannot trace how the system arrived at its conclusion — a problem that conflicts with the scientific method's emphasis on replicability and clear methodology. Some clients have responded by including no-AI clauses in their contracts, forcing researchers to either avoid the technology entirely or use it in ways that don't technically violate contractual terms but may blur ethical lines."Onboarding beats feature bloat," Parker said in the report. "The biggest brakes are time to learn and train. Packaged workflows, templates, and guided setup all unlock usage faster than piling on capabilities."Inside the new workflow: treating AI like a junior analyst who needs constant supervisionDespite these challenges, researchers aren't abandoning AI — they're developing frameworks to use it responsibly. The consensus model, according to the survey, is "human-led research supported by AI," where AI handles repetitive tasks like coding, data cleaning, and report generation while humans focus on interpretation, strategy, and business impact.About one-third of researchers (29%) describe their current workflow as "human-led with significant AI support," while 31% characterize it as "mostly human with some AI help." Looking ahead to 2030, 61% envision AI as a "decision-support partner" with expanded capabilities including generative features for drafting surveys and reports (56%), AI-driven synthetic data generation (53%), automation of core processes like project setup and coding (48%), predictive analytics (44%), and deeper cognitive insights (43%).The report describes an emerging division of labor where researchers become "Insight Advocates" — professionals who validate AI outputs, connect findings to stakeholder challenges, and translate machine-generated analysis into strategic narratives that drive business decisions. In this model, technical execution becomes less central to the researcher's value proposition than judgment, context, and storytelling."AI can surface missed insights — but it still needs a human to judge what really matters," Topiol said in the report.What other knowledge workers can learn from the research industry's AI experimentThe market research industry's AI adoption may presage similar patterns in other knowledge work professions where the technology promises to accelerate analysis and synthesis. The experience of researchers — early AI adopters who have integrated the technology into daily workflows — offers lessons about both opportunities and pitfalls.First, speed genuinely matters. One boutique agency research lead quoted in the report described watching survey results accumulate in real-time after fielding: "After submitting it for fielding, I literally watched the survey count climb and finish the same afternoon. It was a remarkable turnaround." That velocity enables researchers to respond to business questions within hours rather than weeks, making insights actionable while decisions are still being made rather than after the fact.Second, the productivity gains are real but uneven. Saving five hours per week represents meaningful efficiency for individual contributors, but those savings can disappear if spent validating AI outputs or correcting errors. The net benefit depends on the specific task, the quality of the AI tool, and the user's skill in prompting and reviewing the technology's work.Third, the skills required for research are changing. The report identifies future competencies including cultural fluency, strategic storytelling, ethical stewardship, and what it calls "inquisitive insight advocacy" — the ability to ask the right questions, validate AI outputs, and frame insights for maximum business impact. Technical execution, while still important, becomes less differentiating as AI handles more of the mechanical work.The strange phenomenon of using technology intensively while questioning its reliabilityThe survey's most striking finding may be the persistence of trust issues despite widespread adoption. In most technology adoption curves, trust builds as users gain experience and tools mature. But with AI, researchers appear to be using tools intensively while simultaneously questioning their reliability — a dynamic driven by the technology's pattern of performing well most of the time but failing unpredictably.This creates a verification burden that has no obvious endpoint. Unlike traditional software bugs that can be identified and fixed, AI systems' probabilistic nature means they may produce different outputs for the same inputs, making it difficult to develop reliable quality assurance processes.The data privacy concerns — cited by 33% as the biggest barrier to adoption — reflect a different dimension of trust. Researchers worry not just about whether AI produces accurate outputs but also about what happens to the sensitive data they feed into these systems. QuestDIY's approach, according to the report, is to build AI directly into a research platform with ISO/IEC 27001 certification rather than requiring researchers to use general-purpose tools like ChatGPT that may store and learn from user inputs."The center of gravity is analysis at scale — fusing multiple sources, handling both structured and unstructured data, and automating reporting," Topiol said in the report, describing where AI delivers the most value.The future of research work: elevation or endless verification?The report positions 2026 as an inflection point when AI moves from being a tool researchers use to something more like a team member — what the authors call a "co-analyst" that participates in the research process rather than merely accelerating specific tasks.This vision assumes continued improvement in AI capabilities, particularly in areas where researchers currently see the technology as underdeveloped. While 41% currently use AI for survey design, 37% for programming, and 30% for proposal creation, most researchers consider these appropriate use cases, suggesting significant room for growth once the tools become more reliable or the workflows more structured.The human-led model appears likely to persist. "The future is human-led, with AI as a trusted co-analyst," Parker said in the report. But what "human-led" means in practice may shift. If AI handles most analytical tasks and researchers focus on validation and strategic interpretation, the profession may come to resemble editorial work more than scientific analysis — curating and contextualizing machine-generated insights rather than producing them from scratch."AI gives researchers the space to move up the value chain – from data gatherers to Insight Advocates, focused on maximising business impact," Topiol said in the report.Whether this transformation marks an elevation of the profession or a deskilling depends partly on how the technology evolves. If AI systems become more transparent and reliable, the verification burden may decrease and researchers can focus on higher-order thinking. If they remain opaque and error-prone, researchers may find themselves trapped in an endless cycle of checking work produced by tools they cannot fully trust or explain.The survey data suggests researchers are navigating this uncertainty by developing a form of professional muscle memory — learning which tasks AI handles well, where it tends to fail, and how much oversight each type of output requires. This tacit knowledge, accumulated through daily use and occasional failures, may become as important to the profession as statistical literacy or survey design principles.Yet the fundamental tension remains unresolved. Researchers are moving faster than ever, delivering insights in hours instead of weeks, and handling analytical tasks that would have been impossible without AI. But they're doing so while shouldering a new responsibility that previous generations never faced: serving as the quality control layer between powerful but unpredictable machines and business leaders making million-dollar decisions.The industry has made its bet. Now comes the harder part: proving that human judgment can keep pace with machine speed — and that the insights produced by this uneasy partnership are worth the trust clients place in them.
- [Forget Fine-Tuning: SAP’s RPT-1 Brings Ready-to-Use AI for Business Tasks](https://venturebeat.com/ai/forget-fine-tuning-saps-rpt-1-brings-ready-to-use-ai-for-business-tasks) — 13:00 · VentureBeat AI
  > SAP aims to displace more general large language models with the release of its own foundational “tabular” model, which the company claims will reduce training requirements for enterprises. The model, called SAP RPT-1, is a pre-trained model with business and enterprise knowledge out of the box. SAP calls it a Relational Foundation Model, meaning it can do predictions based on relational databases even without fine-tuning or additional training.Walter Sun, SAP's global head of AI, told VentureBeat in an interview that the value of the new model lies in its ability to perform various enterprise tasks, such as predictive analytics, out of the box. “Everyone knows about language models, and there’s a bunch of good ones that already exist,” Sun said. “But we trained the model on data on business transactions, basically Excel spreadsheets, and so we have a model that can do predictive analytics where the value is that it’s out of the box, meaning you don’t need to have specifics of a company to do tasks analogous to a language model.” Sun said that right out of the gate, RPT-1 can essentially build out a business model for enterprises based on its knowledge gained from data from SAP’s decades of information. Organizations can plug the model directly into applications, even without additional fine-tuning.RPT-1, SAP’s first large family of AI models, will be generally available in “Q4 of 2025” and be deployed via SAP’s AI Foundation. While RPT-1 is currently available, the company stated that additional models will be made available soon, including an open-source, state-of-the-art model. SAP will also release a no-code playground environment to experiment with the model. 
Tabular models vs LLMs
Tabular or relational AI models learned from spreadsheets, unlike LLMs, which learned from text and code. RPT-1 not only understands numbers and the relationships between different cells, but it’s also able to provide more structured and precise answers. When enterprises decide to use RPT-1, they can add more direction to the model through a bit of context engineering, since the model is semantically aware and learns based on how it is being used. SAP researchers first proposed the idea that tabular models can both exhibit semantic awareness and learn from content through a paper published in June. It proposed ConTextTab introduced context-aware pretraining. It utilizes semantic signals, such as table headers or column types, to guide model training, enabling the model to build a relational structure with the data. It’s this architecture that makes the model work best for tasks with precise answers, such as for financial or enterprise use cases.The RPT models build on the ConTextTab work that lets it learn structured business data, say from SAP’s knowledge graph, and then be able to add more context through usage. SAP researchers did test ConTextTab against benchmarks, saying it “is competitive” against similar models like TabPFN and TabIFL. Industry-specific models continue to grow
Many enterprises prefer to fine-tune general LLMs like GPT-5 or Claude, to basically retrain the model to answer only questions relevant to their business. However, a shift towards industry-specific models has begun to take root. Sun said that his experience at a previous company, building a very narrow, highly customized AI model for sentiment analysis, influenced a lot of what makes RPT-1 different. “It was a very customized model, a narrow model that takes specific feedback for specific products but it wasn’t scalable,” Sun said. “When LLMs came about, that one model measures sentiment. But there are use cases that we can do that LLMs cannot do.”He said these use cases include predictions, such as determining when a shopper will return to a grocery store, which may involve numerical analysis along with an understanding of the shopper’s buying habits. However, some LLMs have begun integrating into spreadsheets, and AI model providers encourage users to upload similar data to teach them context. Microsoft added new capabilities to Copilot, including the ability to work in Excel. Anthropic integrated its Claude model with Excel, complementing its Claude for Finance service. Chinese startup Manus also offers a data visualization tool that understands spreadsheets, and ChatGPT can create charts from uploaded spreadsheets and other data sources. However, SAP noted that it is more than just reading a spreadsheet; RPT-1 should stand out amongst its competitors because it requires fewer additional pieces of information about a business to provide its responses.
- [Inside Zendesk’s dual AI leap: From reliable agents to real-time intelligence with GPT-5 and HyperArc](https://venturebeat.com/ai/inside-zendesks-dual-ai-leap-from-reliable-agents-to-real-time-intelligence) — 12:00 · VentureBeat AI
  > Presented by ZendeskAgentic AI is currently transforming three key areas of work — creative, coding, and support — says Shashi Upadhyay, president of engineering, AI, and product at Zendesk. But he notes that support presents a distinct challenge. "Support is special because you’re putting an autonomous AI agent right in front of your customer," Upadhyay says. "You have to be confident that it’s going to do the right thing for the customer and by the customer. Every step forward in AI should make service more dependable for both customers and human agents." Zendesk, recently named a Leader in the 2025 Gartner Magic Quadrant for the CRM Customer Engagement Center, started implementing AI agents about a year and a half ago. Since then, they've seen that AI agents can solve almost 80% of all incoming customer requests on their own. For the remaining 20%, the AI agent can hand it over to a human to help solve the more complex problems. "Autonomous AI agents work 24/7, with no wait or queue time. You have a problem; they provide an answer right away. All of that adds up," he says. "Not only do you get higher resolutions, higher automation, but you can also improve the CSAT at the same time. Because 80% is such a promising number, and the results are so solid, we believe it’s only a matter of time before everyone adopts this technology. We already see that across the board."The company's efforts to advance its standard of usability, depth of insight, and time to value for organizations of all sizes require continuous testing, integration of advanced models like ChatGPT-5, and a major upgrade of its analytics capabilities and real-time, gen AI–powered insights with the acquisition of HyperArc, an AI-native analytics platform.Designing, testing, and deploying a better agent"In a support context especially, it’s important AI agents behave consistently with the brand of the company, policies, and regulatory requirements you may have," Upadhyay says. "We test every agent, every model continuously across all our customers. We do it before we release it and we do it after we release it, across five categories." Those categories — automation rate, execution, precision, latency, and safety — form the foundation of Zendesk’s ongoing benchmarking program. Each model is scored on how accurately it resolves issues, how well it follows instructions, how fast it responds, and whether it stays within clearly defined guardrails. The goal isn’t just to make AI faster — it’s to make it dependable, accountable, and aligned with the standards that define great customer service.That testing is reinforced by Zendesk’s QA agent — an automated monitor that keeps a constant eye on every conversation. If an exchange starts to drift off course, whether in tone or accuracy, the system immediately flags it and alerts a human agent to step in. It’s an added layer of assurance that keeps the customer experience on track, even when AI is running the first line of support.GPT-5 for next-level agentsIn the world of support and service, the move from simple chatbots that answer basic queries or solve uncomplicated problems, to agents that actually take action, is groundbreaking. An agent that can understand that a customer wants to return an item, confirm whether it's eligible for a return, process the return, and issue a refund, is a powerful upgrade. With the introduction of ChatGPT-5, Zendesk recognized an opportunity to integrate that ability into its Resolution Platform."We worked very closely with OpenAI because GPT-5 was a pretty big improvement in model capabilities, going from being able to answer questions, to being able to reason and take action," Upadhyay says. "First, it does a much better job at solving problems autonomously. Secondly, it's much better at understanding your intent, which improves the customer experience because you feel understood. Last but not least, it has 95%-plus reliability on executing correctly."Those gains ripple across Zendesk’s AI agents, Copilot, and App Builder. GPT-5 cuts workflow failures by 30%, thanks to its ability to adapt to unexpected complexity without losing context, and reduces fallback escalations by more than 20%, with more complete and accurate responses. The result: faster resolutions, fewer hand-offs, and AI that behaves more like a seasoned support professional than a scripted assistant.Plus, GPT-5 is better at handling ambiguity, and able to clarify vague customer input, which improves routing and increases automated workflows in over 65% of conversations. It has greater accuracy across five languages, and makes agents more productive with more concise, contextually relevant answers that align with tone guidelines.And in App Builder, GPT-5 delivered 25% to 30% faster overall performance, with more prompt iterations per minute, speeding app builder development workflows.Filling in the analytics gapTraditionally, support analytics has focused on structured data — the kind that fits neatly into a table: when a ticket was opened, who handled it, how long it took to resolve, and when it was closed. But the most valuable insights often live in unstructured data — the conversations themselves, spread across email, chat, voice, and messaging apps like WhatsApp."Customers often don’t realize how much intelligence sits in their support interactions," Upadhyay says. "What we’re pushing for with analytics is ways in which we can improve the entire company with the insights that are sitting in support data."To surface those deeper insights, Zendesk turned to HyperArc, an AI-native analytics company known for its proprietary HyperGraph engine and generative-AI-powered insights. The acquisition gave new life to Explore, Zendesk’s analytics platform, transforming it into a modern solution capable of merging structured and unstructured data, supporting conversational interfaces, and drawing on persistent memory to use past interactions as context for new queries."Your support interactions are telling you everything that’s not working in your business today, all that information is sitting in these millions of tickets that you’ve collected over time," Upadhyay says. "We wanted to make that completely visible. Now we have this genius AI agent that can analyze it all and come back with explicit recommendations. That doesn’t just improve support. It improves the entire company."That visibility now translates into actionable intelligence. The system can pinpoint where issues are most persistent, identify the patterns behind them, and suggest ways to resolve them. It can even anticipate problems before they happen. During high-pressure events like Black Friday, for example, it can analyze historical data to flag recurring issues, predict where new bottlenecks might appear, and recommend preventive measures — turning reactive support into proactive strategy."That’s where HyperArc shines," Upadhyay says. It doesn’t just help you understand the past — it helps you plan better for the future."By integrating HyperArc’s AI-native intelligence, Zendesk is moving customer service toward continuous learning — where every interaction builds trust and sharpens performance, setting the stage for AI that can see what’s coming next.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
