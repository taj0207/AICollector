# AI News for 2025-11-10 (Asia/Taipei)

Collected 19 article(s).

- [Bank of America Just Issued a Stark Warning: The AI Boom Is Hitting a Cash Crunch - Yahoo Finance](https://news.google.com/rss/articles/CBMiggFBVV95cUxQZTBaOFdJS3IxWkNObTNsRWliX2JYOGY4ZEk1bnVCcDZjWEhiVUhXQlUwaWFpRWl4bklfSnk2ZmMxN2lBcGVRUWU5SDJIMUVBLWVEeDdGd2hkZGhGMk96X2I2M0djb2hfNGJtU1JSdmFpa0VsRndVLV9TU0ZULXlwY3dB?oc=5) — 23:24 · Google News (AI)
  > Bank of America Just Issued a Stark Warning: The AI Boom Is Hitting a Cash Crunch  Yahoo Finance
- [The No. 1 Country Song in America Is AI-Generated - Newsweek](https://news.google.com/rss/articles/CBMihAFBVV95cUxPMzdvaF9felhaUGNtNFgwMHVwcmJNbHpZMkRwNXZCeFBQN3p1NGxaY0Z4X2h1RnMxYndvSjNEMHBUcHZGWW83RnJhTHZURzRZNEF2aE42aWlFTU54eEVoTkdLTUVFMmZKbXN0bDBrUHcya25wQ1hZLUl3ckpNcmg2REY4TTg?oc=5) — 23:24 · Google News (AI)
  > The No. 1 Country Song in America Is AI-Generated  Newsweek
- [Pope asks for extra care when using AI in medicine - usccb](https://news.google.com/rss/articles/CBMigAFBVV95cUxPekFkOGl2UXg1Z3FaZjl2UkFkc05yeGc0aGVfcmxwazdCTGZRRVZNRm1EVFdVWlpuT0hIMThlcUhqaUYxbjZoc1dUQU9KbU84RXh6NVU2Yi1Lck1SZ0x3a2ZkYWtQdGdSM0Z1OUxmV01jckw3NGZPbXp5aWwyQnBDZg?oc=5) — 23:18 · Google News (AI)
  > Pope asks for extra care when using AI in medicine  usccb
- [“AI, Make Me A Degree Certificate” - Hackaday](https://news.google.com/rss/articles/CBMickFVX3lxTE9jUmw2YmQ0RWNjLWRtUE5FR1l0T1QxX0FNWUtRcHE0ZVB1RzMwdHEtZlVzMm1Xdmp0WFR3M0VHdEhyaXZsU1YxNmdKTE9aaXR5SGtRT1A5Z0tzaWotN2xwMWpaQ2xKWFA5amVpTWlKOWxmUQ?oc=5) — 23:00 · Google News (AI)
  > “AI, Make Me A Degree Certificate”  Hackaday
- [How context engineering can save your company from AI vibe code overload: lessons from Qodo and Monday.com](https://venturebeat.com/ai/how-context-engineering-can-save-your-company-from-ai-vibe-code-overload) — 23:00 · VentureBeat AI
  > As cloud project tracking software monday.com’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.That’s when Guy Regev, VP of R&D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from Qodo, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a new case study released by both Qodo and monday.com today reveals. “Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work," Regev told VentureBeat in a recent video call interview, adding that it has "prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities."Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls context engineering to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. "You can call Claude Code or Cursor and in five minutes get 1,000 lines of code," said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. "You have 40 minutes, and you can't review that. So you need Qodo to actually review it.”For monday.com, this capability wasn’t just helpful — it was transformative.Code Review, at ScaleAt any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works."The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy," Regev said. "It’s context-aware in a way traditional tools aren’t."What “Context Engineering” Actually MeansQodo calls its secret sauce context engineering — a system-level approach to managing everything the model sees when making a decision. This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.As Dana Fine, Qodo’s community manager, put it in a blog post: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. "The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request," said Regev.Integration into the PipelineToday, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. “It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work," Regev noted. Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards," added Friedman.The Results: Time Saved, Bugs PreventedSince rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.From Internal Tool to Product VisionRegev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules," Regev said. "But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.What’s Next?Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines."Context engines will be the big story of 2026," Friedman said. "Every enterprise will need to build their own second brain if they want AI that actually understands and helps them."As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.
- [USC Price professor launches initiative to teach AI - USC Price School](https://news.google.com/rss/articles/CBMieEFVX3lxTE1VOFNYT2ZfamtIckNUNjlaMm0zcHdDTzZ3Mko4YmJkWFFRMDdDaS1GbjFHbEsydmVieXJfOEJPc0QwenNsQVY5ZnBVeUNJMUR0dll5WEg0UFQ2MUlELUhfLWZsVWtDWlpGU1lGMlJWbV9KYjVXemZVRA?oc=5) — 22:52 · Google News (AI)
  > USC Price professor launches initiative to teach AI  USC Price School
- [A new project aims to predict how quickly AI will progress - The Economist](https://news.google.com/rss/articles/CBMiugFBVV95cUxNclBNdW8tc2lmNlRPbF9yeFFzNktVRjEwQWhoQmpDWU9JY3NZSld4S1lqYlNIM0dLaXl1ZzNxZUZ0TW9WQTc3RHJ1eEs0cWgyeDZQbDhlZG1TZFlaM1YzbDNrTEhvZlpUcjJQUkFmV3l2NFdGckZTRFVCNmxJMFdnRklwZUtYNmJOVFZDdU5yeEJJdG43VEFLdTh4Z1dwTkJmaUY4QzhJYW1IbHRoSnVKWHhOWXlOZHppUVE?oc=5) — 22:45 · Google News (AI)
  > A new project aims to predict how quickly AI will progress  The Economist
- [AI bubble about to pop as returns on investment fall short? - DW](https://news.google.com/rss/articles/CBMiywFBVV95cUxPQjctSXVvNVVjcFBXbE9WLXdhQ05fR1A2OU9wN21WVXRUR05XaGZqd2hZSDhRQ1dzWGZHWjhNaWlVd044SEp6dmhQWlMyRmNXb2ZrOHdrOC03S3gwUkYwOVdOcFlkTEhJNkYyNzU4Q3lEQlI1YUg5d2JTOThtelBiNnJDYWd3Vy1OZ19OXzRQRDFQUG40TzNrZUlQb29KQ0dRNTlha2NvdXNINGNJbWxIU2lscDFzRlNvVUhIVHVPT1lBaV9aZ1ZDSzJGTdIBywFBVV95cUxQMTE0bXNGMVoySzJ6NERKR05xT29heUlWYW5CbWl0YzlYdjgyRi1Fal96RkNtMzZKRWFCRFdLeGdoand6RVdvUFRqazMwNElZYWxYUWZmTVZTVTV5aGMxZGsxUHhSM2ZPOHV3M29QMFQ3TERXS1gzMlZhbzRqV2tKV2E4TEVCdWZ6d3U2ck1qM0hlenRHVTZ2RVFDNDN6V2d6OFdLTnhxUnp3XzR2VUFsTU5BeGJna0Y4Q1JKR1lPOUlZdlNXXzhua3d6cw?oc=5) — 22:34 · Google News (AI)
  > AI bubble about to pop as returns on investment fall short?  DW
- [‘Roadmap’ shows the environmental impact of AI data center boom - Cornell Chronicle](https://news.google.com/rss/articles/CBMimwFBVV95cUxNUGJsUlUzNUt6ZHg5aDdwdDR5emw0Z3NDdHNHZnd0RXdnXzdSQ2w5QUNEaWFRSFhsQnBNUnQwcnFMYVJZcm9pYTlVbW5oY0tVSlplTlV0ZFdLSEhuTG02R211eG9vWlByTzVYaWF0WGY5eW8yY2hPOGtxWW93Y2NRU3FTT2VKSDc3QzRpSENmVGZZR2RyM0RTZGJ5WQ?oc=5) — 22:23 · Google News (AI)
  > ‘Roadmap’ shows the environmental impact of AI data center boom  Cornell Chronicle
- [Reimagining cybersecurity in the era of AI and quantum](https://www.technologyreview.com/2025/11/10/1127774/reimagining-cybersecurity-in-the-era-of-ai-and-quantum/) — 22:19 · MIT Technology Review (AI)
  > AI and quantum technologies are dramatically reconfiguring how cybersecurity functions, redefining the speed and scale with which digital defenders and their adversaries can operate. The weaponization of AI tools for cyberattacks is already proving a worthy opponent to current defenses. From reconnaissance to ransomware, cybercriminals can automate attacks faster than ever before with AI. This…
- [Former Google, Meta executives raise $100 million for high-capacity AI servers startup - CNBC](https://news.google.com/rss/articles/CBMifkFVX3lxTFBOYU90bmlsQXVKUU1MQ2czcmRtQlRxdHN2dEtTSWVZWFhmblFnV2dYYUpxWHRCeUZ3bTF0ckJ4dlRnNG9QcTIzTTJJNHRWNUZpZDJPZkNyYjhsblpiMVZtLWJoZC1maFJDTzUxZzFjVm5mOXUtR0NTSldvNDZvZ9IBgwFBVV95cUxPWU01dndDdGRJbGx4MU9SOWVlemdBSnpJTlFZTlVneFZ5dW5Qa0xOS01rSHQ5bWhxMEFrSkhuSDlyS1Vab1RLY3pkTmFjTHBKb01Qd25hUEZfczdNWWVvd2FWUnZCbWFXbzRoY1EyNm1XR3VkYUxueGRJZlZtbWE5NXJ3VQ?oc=5) — 22:00 · Google News (AI)
  > Former Google, Meta executives raise $100 million for high-capacity AI servers startup  CNBC
- [Lilly continues AI push, inking $100M-plus deal with Insilico - Fierce Biotech](https://news.google.com/rss/articles/CBMiowFBVV95cUxPcmVmQU5hejNNMkc3cUZsSWhHeVpfcVNTZ0tkODVZRVhZMlNMZk9QdzhyaDJJQ1NkWXBXM2FGa2lrMURWYk1yMUlRZERLRFM5QUN5TGFUSTdQREJyaW05TndPcGNXV3JPenhnR1k0ZGc0cUIxdFdaT1dCQWtOblhGdFkwVHphZkl0dFFSZ0UyWDR3SlN0UTE0YlA5N3J5ckdqZEhB?oc=5) — 22:00 · Google News (AI)
  > Lilly continues AI push, inking $100M-plus deal with Insilico  Fierce Biotech
- [Baseten takes on hyperscalers with new AI training platform that lets you own your model weights](https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you) — 22:00 · VentureBeat AI
  > Baseten, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.The San Francisco-based company announced Thursday the general availability of Baseten Training, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten's core inference business, driven by what CTO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment."We had a captive audience of customers who kept coming to us saying, 'Hey, I hate this problem,'" Haghighat said in an interview. "One of them told me, 'Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn't been working all along.'"The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from Meta, Alibaba, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI's GPT-5 or Anthropic's Claude. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.Baseten's answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It's a deliberately low-level approach born from hard-won lessons.How a failed product taught Baseten what AI training infrastructure really needsThis isn't Baseten's first foray into training. The company's previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive."We had created the abstraction layer a little too high," he explained. "We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model."The problem? Users didn't have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection."We became consultants," Haghighat said. "And that's not what we had set out to do."Baseten killed Blueprints and refocused entirely on inference, vowing to "earn the right" to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten's inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products."Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else," Haghighat said. "I understand why from their perspective — I still don't think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference."Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalersThe new Baseten Training product operates at what Haghighat calls "the infrastructure layer" — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten's inference stack.Key technical capabilities include multi-node training support across clusters of NVIDIA H100 or B200 GPUs, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten's proprietary Multi-Cloud Management (MCM) system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals."With hyperscalers, you don't get to say, 'Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don't charge me for it,'" Haghighat said. "They say, 'No, you need to sign a three-year contract.' We don't do that."Baseten's approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten's inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.The technical differentiation extends to Baseten's observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an "ML Cookbook" of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach "training success" faster.Early adopters report 84% cost savings and 50% latency improvements with custom modelsTwo early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.Oxen AI, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: "Whenever I've seen a platform try to do both hardware and software, they usually fail at one of them. That's why partnering with Baseten to handle infrastructure was the obvious choice."Oxen built its customer experience entirely on top of Baseten's infrastructure, using the Baseten CLI to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten's interface behind Oxen's own. For one Oxen customer, AlliumAI — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530."Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches," said Daniel Demillard, CEO of AlliumAI. "With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training."Parsed, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren't negotiable."Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider," said Charles O'Neill, Parsed's co-founder and chief science officer. "On top of that, we were struggling to easily download and checkpoint weights after training runs."With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten's modified vLLM inference framework and speculative decoding — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models."Fast models matter," O'Neill said. "But fast models that get better over time matter more. A model that's 2x faster but static loses to one that's slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement."Why training and inference are more interconnected than the industry realizesThe Parsed example illuminates a deeper strategic rationale for Baseten's training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.Baseten's model performance team uses the training platform extensively to create "draft models" for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI's GPT OSS 120B model — a 60% improvement over its launch performance — using EAGLE-3 speculative decoding, which requires training specialized small models to work alongside larger target models."Ultimately, inference and training plug in more ways than one might think," Haghighat said. "When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis."This technical interdependence reinforces Baseten's thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.The approach contrasts sharply with vertically integrated competitors like Replicate or Modal, which also offer training and inference but with different architectural tradeoffs. Baseten's bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependencyUnderpinning Baseten's entire strategy is a conviction about the trajectory of open-source AI models — namely, that they're getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning."Both closed and open-source models are getting better and better in terms of quality," Haghighat said. "We don't even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases."He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it "as good as the closed model, not at everything, but at this narrow band of capability that they want."That trend is already visible in Baseten's Model APIs business, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to DeepSeek V3 and R1, and has since added models like Llama 4 and Qwen 3, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten's Dedicated Deployments infrastructure.Yet Haghighat acknowledged the market remains "fuzzy" around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its Forward Deployed Engineering team, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques."As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user's needs without them having to learn too much about how RL works," he said. "Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back."The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.Baseten faces crowded field but bets developer experience and performance will win enterprise customersBaseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like AWS, Google Cloud, and Microsoft Azure offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like Hugging Face, Replicate, and Modal that bundle training, inference, and model hosting.Baseten's differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.The company's recent $150 million Series D and $2.15 billion valuation provide runway to invest in both products simultaneously. Major customers include Descript, which uses Baseten for transcription workloads; Decagon, which runs customer service AI; and Sourcegraph, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.Timing may be Baseten's biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift."There is a lot of use cases for which closed models have gotten there and open ones have not," he said. "Where I'm seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That's very palpable in the market."For enterprises navigating the complex transition from closed to open AI models, Baseten's positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company's insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company's willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver."Through and through, we're an inference company," Haghighat emphasized. "The reason that we did training is at the service of inference."That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten's most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.At least Baseten's customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.
- [The Download: busting weather myths, and AI heart attack prediction](https://www.technologyreview.com/2025/11/10/1127798/the-download-busting-weather-myths-and-ai-heart-attack-prediction/) — 21:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. Why it’s so hard to bust the weather control conspiracy theory It was October 2024, and Hurricane Helene had just devastated the US Southeast. Representative Marjorie Taylor Greene of Georgia found an abstract…
- [Rumble to buy German AI firm Northern Data for about $767 million, shares soar - Reuters](https://news.google.com/rss/articles/CBMimwFBVV95cUxPSTQ2UjYxSF9LNy0yaVBQNmhuNmhQWHFHLTJnaWZsNnRBZjhZbnhIaUVtbnFGWFAwejBveHFURUtycVZ3ZzUwdEJwNmtta09SRnlKZWd4R2FWeEJrbHBuYk5MMHhHblNaUTFPYzBuUTZya1FBUnQ4OUNxWS1ocUR5RDZWcnJSdGNSQ2djcUhQcVJ4TmZER3VMa2l2TQ?oc=5) — 20:25 · Google News (AI)
  > Rumble to buy German AI firm Northern Data for about $767 million, shares soar  Reuters
- [AI Is Upending Jobs. Corporate Tech and HR Are Teaming Up to Figure It Out. - The Wall Street Journal](https://news.google.com/rss/articles/CBMitgNBVV95cUxOZWgtRklWUlp0aUJnVnh2YTNCOFhUTUJYVF9OQ0ZoaEdUbmRRS29aNjd0dG1xdVltSVdmQVVtQnMzU0xoZVA2NkZZdFFoMmc0amhrdkpqdV9XWG44WWI3dHBVSXZxX0dLMUYxcW9yU3VnaHlVRE92dWJFaE9NSjdoVEd6TXhhWklzelJvR04wYVpPSUJ2dnBEb1ZEdW5QWEFnUEpUa0xwSVlyOG1vNzhFblZBcHpUREtySDlwaFp0ZENVaFpiZThITFpCM2M4ckZacHkzREI3NWhfdzZBR05GVzhoTFh0UWRMR1hwZkpfTEFJWGMzYldqdnprcVI3SHliMVZmVWpDVlZ4ODJWRkRZUkpxYzJWSEZmbzFzeVNDbUJPSTd0REcxNTBVTU1UbzBiM1VEdnJWVld3dGFWTkZObUNvb3lwZlRXX2FVUDdiT1NDbmdWcjlRdTViUkRLc0xvUVJVQUY5WFpVdkEtb0o3aVRpck9tcGdOLWdCUGExOElITDByeFpqeDh3U2I1emQ3dzZabjhwNzZmSFlDVk1xVEtBVTN3Ym5NeDAxTDZhd0VrM0k5akE?oc=5) — 20:00 · Google News (AI)
  > AI Is Upending Jobs. Corporate Tech and HR Are Teaming Up to Figure It Out.  The Wall Street Journal
- [Google, the sleeping AI giant, awakens - Axios](https://news.google.com/rss/articles/CBMibkFVX3lxTE5jT2paSlFGdW9kVVdFUUFoX3B5UUZLNGh0WUhCb3hfejlwby1xNUluLUlBZzA2QmxSeDVfOFFvR0Z3S3lTTEktRkEzYVJISXlWOGRrb0I1VTF4NE9VME11ekJZY3huc3JBTXJ1UlN3?oc=5) — 19:58 · Google News (AI)
  > Google, the sleeping AI giant, awakens  Axios
- [Will the AI bubble burst as investors grow wary of returns? - DW](https://news.google.com/rss/articles/CBMilgFBVV95cUxPS2NZMnZ4NGpIOWhRWjV0Y2xpTzVYUWxIbzAyY0IyT2xhS2JLSDZRcDRaRDEwaFRqMlIwYVlKSnNrMGFoVE5mM09XTmxOcFhCMDlfUTJBLTE0RHVmWEZLRVFLbThoSnl4XzVCZXI2dDZCTXZTdkVxWlVpQUJTcmlwQWR5d1JhWFp2RDEwNXdGc284akRRQkHSAZYBQVVfeXFMUG43R3BtcTN3M0h6LTNiR0h0NE85QUtEQjFRc3ZsN2g5VktqSEcybTExQ3JlanNWbmhrMU9yNWZJQWFhYV9EemJlQjR2aWZXdHVDLTNQSTB6Ul9MWlllUEI3eXpobHpYcDVJR2poQlVQZklHeExsZGpKNEpWUThpS1lPRHNxOHZKeHNJSjRDM1RNY2Y5TDd3?oc=5) — 19:52 · Google News (AI)
  > Will the AI bubble burst as investors grow wary of returns?  DW
- [Celosphere 2025: Where enterprise AI moved from experiment to execution](https://venturebeat.com/ai/celosphere-2025-where-enterprise-ai-moved-from-experiment-to-execution) — 13:00 · VentureBeat AI
  > Presented by CelonisAfter a year of boardroom declarations about “AI transformation,” this was the week where enterprise leaders came together to talk about what actually works. Speaking from the stage at Celosphere in Munich, Celonis co-founder and co-CEO Alexander Rinke set the tone early in his keynote:“Only 11 % of companies are seeing measurable benefits from AI projects today,” he said. “That’s not an adoption problem. That’s a context problem.”It’s a sentiment familiar to anyone who’s tried to deploy AI inside a large enterprise. You can’t automate what you don’t understand — and most organizations still lack a unified picture of how work in their companies really gets done.Celonis’ answer, showcased across three days at the company’s annual event, was less about new tech acronyms and more about connective tissue: how to make AI fit within the messy, living processes that drive business. The company framed it as achieving a real “Return on AI (ROAI)” — measurable impact that comes only when intelligence is grounded in process context.A living model of how the enterprise worksAt the heart of the keynote was what Rinke called a “living digital twin of your operations.” Celonis has been building toward this moment for years — but this was the first time the company made clear how far that concept has evolved.“We start by freeing the process,” said Rinke. “Freeing it from the restrictions of your current legacy systems.” Data Core, Celonis’ data infrastructure, extracts raw data from source systems. It’s capable of querying billions of records in near real time with sub-minute refresh — extending visibility beyond traditional systems of record.Built on this foundation, the Process Intelligence Graph sits at the center of the Celonis Platform. It’s a system-agnostic, graph-based model that unifies data across systems, apps, and even devices, including task-mining data that captures clicks, spreadsheets, and browser activity. It combines this data with business context—business rules, KPIs, benchmarks, and exceptions. Every transaction, rule, and process interaction becomes part of a continuously updated replica that reflects how the organization actually operates.On top of the Graph, the company’s new Build Experience allows organizations to analyze, design, and operate AI-driven, composable processes — integrating AI where it delivers business impact, not just technical demos:Analyze where processes stall or repeatDesign the future state, setting outcomes, guardrails, and AI touchpointsOperate with humans, systems, and AI agents working in sync — now orchestrated through a generally available Orchestration Engine that can trigger and monitor every step in one flowIt’s a deliberate shift from discovery-driven AI pilots to outcome-driven AI operations — and a blueprint for orchestrating agentic AI, where human teams, systems, and autonomous agents work together through shared process context rather than in silos.Real-world proof: Mercedes-Benz, Vinmar, and UniperThe Celosphere stage offered real proof of theCelonisPlatform in action, through live stories from customers already building on it.Mercedes-Benz shared how process intelligence became their “connective tissue” during the semiconductor crisis. “We had data everywhere — plants, suppliers, logistics,” recalled Dr. Jörg Burzer, Member of the Board of Management of Mercedes-Benz Group AG. “What we didn’t have was a way to see it together. Celonis helped us connect those dots fast enough to act.”The partnership has since expanded across eight of the company’s ten most critical processes, from supply chain to quality to after-sales. But what impressed the audience wasn’t just the scale — it was the cultural shift.“If you show data in context, and let teams visualize processes, you also change the culture,” Burzer said. “It’s not just process transformation — it’s people transformation.”At Vinmar, CEO Vishal Baid described Celonis as “the foundation of our automation and AI strategy.” His global plastics distribution business has already automated its entire order-to-cash process for a $3 B unit, achieving a 40 % productivity lift. But Baid wasn’t there to just celebrate finished work — he was looking ahead. “Now we’re tackling the non-algorithmic stuff,” he said. “Matching purchase and sales orders sounds simple until you have thousands of edge cases. We’re building an AI agent that can do that allocation intelligently. That’s the next frontier.”And in the energy sector, Uniper, with partner Microsoft, demonstrated how process-aware AI copilots are already reshaping operations. Using Celonis and Microsoft’s AI stack, Uniper can predict when hydropower plants will need maintenance — and cluster those jobs to reduce downtime and emissions.“Each technician, each part, each system plays a role in a living process,” said Hans Berg, Uniper’s CIO. “The human can’t see all of it. But process intelligence can — and it can nudge the system toward the best outcome.”Agnes Heftberger, CVP & CEO, Microsoft Germany & Austria, who joined Berg on stage, summed it up crisply:“The hard part isn’t building AI features — it’s scaling them responsibly,” she explained. “You need to marry intelligence with the beating heart of the company: its processes.”Across the global community, Celonis reports more than $8 billion in realized business valueand over 120 certified value champions — proof that process intelligence is driving measurable impact far beyond pilots. Rinke called it “the early proof points of a true return on AI.” From closed systems to composable intelligenceCelosphere 2025 marked a shift from architecture to interoperability — from defining enterprise AI to making it work across boundaries.Rinke’s vision for the future is unapologetically open: “Good things grow from open ecosystems,” he said. That philosophy is taking shape through deeper platform integrations — including Microsoft Fabric, Databricks, and Bloomfilter — with zero-copy, bidirectional lakehouse access that lets customers query process data in place with minimal latency. The company also announced MCP Server support for embedding the Process Intelligence Graph directly into agentic AI platforms like Amazon Bedrock and Microsoft Copilot Studio.These updates make “composable enterprise AI” tangible — organizations can now assemble and govern AI solutions across ecosystems rather than being locked into any single vendor.Rather than competing on who has the “best agent,” the message was that enterprise AI will thrive when agents work together through shared context and models that mirror how businesses actually run.“Every vendor is bringing out their own agent,” Rinke said. “But each one is limited to that vendor’s world. If they can’t work together, they can’t work for you. That’s what process intelligence fixes.”The idea drew sustained applause. For companies juggling multiple cloud platforms, ERPs, and data tools, composability isn’t just elegant; it’s survival.Beyond operations: data, democracy, and directionThe closing moments of the keynote took an unexpected turn — from enterprise architecture to human courage. Venezuelan opposition leader and Nobel Peace Prize winner María Corina Machado joined live via satellite to share how her movement used data, encrypted apps, and civic coordination to expose election fraud and mobilize millions.It was a powerful contrast: the same principles — transparency, accountability, context — at work in both business and democracy.“Technology can be a weapon or a liberator,” Machado said. “It depends on who holds the context.”Her words landed with weight in a room full of people used to talking about data, systems, and governance — a reminder that context isn’t just technical, it’s human.Why this year matteredCelosphere 2025 marked a shift in how enterprises approach AI — from experimentation to results grounded in process intelligence. The shift was evident in both tone and technology, with a more powerful Data Core, enhanced Process Intelligence Graph, and new Build Experience. But the deeper takeaway was philosophical: AI only scales when it’s grounded in how people and systems actually work together.Celonis president Carsten Thoma was candid in acknowledging that early process-mining projects often “stormed in with discovery” before understanding organizational value — a lesson that now defines the company’s measured, pragmatic approach to enterprise AI.Rinke put it best near the end of his keynote:“We’re not just automating steps,” he said. “We’re building enterprises that can adapt instantly, innovate freely, and improve continuously.”Missed it? Catch up with all the highlights from Celosphere 2025 here. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
