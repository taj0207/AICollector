# AI News for 2025-11-06 (Asia/Taipei)

Collected 7 article(s).

- [The Download: how doctors fight conspiracy theories, and your AI footprint](https://www.technologyreview.com/2025/11/06/1127666/the-download-how-doctors-fight-conspiracy-theories-and-your-ai-footprint/) — 21:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. How conspiracy theories infiltrated the doctor’s office As anyone who has googled their symptoms and convinced themselves that they’ve got a brain tumor will attest, the internet makes it very easy to self-(mis)diagnose…
- [Google debuts AI chips with 4X performance boost, secures Anthropic megadeal worth billions](https://venturebeat.com/ai/google-debuts-ai-chips-with-4x-performance-boost-secures-anthropic-megadeal) — 21:00 · VentureBeat AI
  > Google Cloud is introducing what it calls its most powerful artificial intelligence infrastructure to date, unveiling a seventh-generation Tensor Processing Unit and expanded Arm-based computing options designed to meet surging demand for AI model deployment — what the company characterizes as a fundamental industry shift from training models to serving them to billions of users.The announcement, made Thursday, centers on Ironwood, Google's latest custom AI accelerator chip, which will become generally available in the coming weeks. In a striking validation of the technology, Anthropic, the AI safety company behind the Claude family of models, disclosed plans to access up to one million of these TPU chips — a commitment worth tens of billions of dollars and among the largest known AI infrastructure deals to date.The move underscores an intensifying competition among cloud providers to control the infrastructure layer powering artificial intelligence, even as questions mount about whether the industry can sustain its current pace of capital expenditure. Google's approach — building custom silicon rather than relying solely on Nvidia's dominant GPU chips — amounts to a long-term bet that vertical integration from chip design through software will deliver superior economics and performance.Why companies are racing to serve AI models, not just train themGoogle executives framed the announcements around what they call "the age of inference" — a transition point where companies shift resources from training frontier AI models to deploying them in production applications serving millions or billions of requests daily."Today's frontier models, including Google's Gemini, Veo, and Imagen and Anthropic's Claude train and serve on Tensor Processing Units," said Amin Vahdat, vice president and general manager of AI and Infrastructure at Google Cloud. "For many organizations, the focus is shifting from training these models to powering useful, responsive interactions with them."This transition has profound implications for infrastructure requirements. Where training workloads can often tolerate batch processing and longer completion times, inference — the process of actually running a trained model to generate responses — demands consistently low latency, high throughput, and unwavering reliability. A chatbot that takes 30 seconds to respond, or a coding assistant that frequently times out, becomes unusable regardless of the underlying model's capabilities.Agentic workflows — where AI systems take autonomous actions rather than simply responding to prompts — create particularly complex infrastructure challenges, requiring tight coordination between specialized AI accelerators and general-purpose computing.Inside Ironwood's architecture: 9,216 chips working as one supercomputerIronwood is more than incremental improvement over Google's sixth-generation TPUs. According to technical specifications shared by the company, it delivers more than four times better performance for both training and inference workloads compared to its predecessor — gains that Google attributes to a system-level co-design approach rather than simply increasing transistor counts.The architecture's most striking feature is its scale. A single Ironwood "pod" — a tightly integrated unit of TPU chips functioning as one supercomputer — can connect up to 9,216 individual chips through Google's proprietary Inter-Chip Interconnect network operating at 9.6 terabits per second. To put that bandwidth in perspective, it's roughly equivalent to downloading the entire Library of Congress in under two seconds.This massive interconnect fabric allows the 9,216 chips to share access to 1.77 petabytes of High Bandwidth Memory — memory fast enough to keep pace with the chips' processing speeds. That's approximately 40,000 high-definition Blu-ray movies' worth of working memory, instantly accessible by thousands of processors simultaneously. "For context, that means Ironwood Pods can deliver 118x more FP8 ExaFLOPS versus the next closest competitor," Google stated in technical documentation.The system employs Optical Circuit Switching technology that acts as a "dynamic, reconfigurable fabric." When individual components fail or require maintenance — inevitable at this scale — the OCS technology automatically reroutes data traffic around the interruption within milliseconds, allowing workloads to continue running without user-visible disruption.This reliability focus reflects lessons learned from deploying five previous TPU generations. Google reported that its fleet-wide uptime for liquid-cooled systems has maintained approximately 99.999% availability since 2020 — equivalent to less than six minutes of downtime per year.Anthropic's billion-dollar bet validates Google's custom silicon strategyPerhaps the most significant external validation of Ironwood's capabilities comes from Anthropic's commitment to access up to one million TPU chips — a staggering figure in an industry where even clusters of 10,000 to 50,000 accelerators are considered massive."Anthropic and Google have a longstanding partnership and this latest expansion will help us continue to grow the compute we need to define the frontier of AI," said Krishna Rao, Anthropic's chief financial officer, in the official partnership agreement. "Our customers — from Fortune 500 companies to AI-native startups — depend on Claude for their most important work, and this expanded capacity ensures we can meet our exponentially growing demand."According to a separate statement, Anthropic will have access to "well over a gigawatt of capacity coming online in 2026" — enough electricity to power a small city. The company specifically cited TPUs' "price-performance and efficiency" as key factors in the decision, along with "existing experience in training and serving its models with TPUs."Industry analysts estimate that a commitment to access one million TPU chips, with associated infrastructure, networking, power, and cooling, likely represents a multi-year contract worth tens of billions of dollars — among the largest known cloud infrastructure commitments in history.James Bradbury, Anthropic's head of compute, elaborated on the inference focus: "Ironwood's improvements in both inference performance and training scalability will help us scale efficiently while maintaining the speed and reliability our customers expect."Google's Axion processors target the computing workloads that make AI possibleAlongside Ironwood, Google introduced expanded options for its Axion processor family — custom Arm-based CPUs designed for general-purpose workloads that support AI applications but don't require specialized accelerators.The N4A instance type, now entering preview, targets what Google describes as "microservices, containerized applications, open-source databases, batch, data analytics, development environments, experimentation, data preparation and web serving jobs that make AI applications possible." The company claims N4A delivers up to 2X better price-performance than comparable current-generation x86-based virtual machines.Google is also previewing C4A metal, its first bare-metal Arm instance, which provides dedicated physical servers for specialized workloads such as Android development, automotive systems, and software with strict licensing requirements.The Axion strategy reflects a growing conviction that the future of computing infrastructure requires both specialized AI accelerators and highly efficient general-purpose processors. While a TPU handles the computationally intensive task of running an AI model, Axion-class processors manage data ingestion, preprocessing, application logic, API serving, and countless other tasks in a modern AI application stack.Early customer results suggest the approach delivers measurable economic benefits. Vimeo reported observing "a 30% improvement in performance for our core transcoding workload compared to comparable x86 VMs" in initial N4A tests. ZoomInfo measured "a 60% improvement in price-performance" for data processing pipelines running on Java services, according to Sergei Koren, the company's chief infrastructure architect.Software tools turn raw silicon performance into developer productivityHardware performance means little if developers cannot easily harness it. Google emphasized that Ironwood and Axion are integrated into what it calls AI Hypercomputer — "an integrated supercomputing system that brings together compute, networking, storage, and software to improve system-level performance and efficiency."According to an October 2025 IDC Business Value Snapshot study, AI Hypercomputer customers achieved on average 353% three-year return on investment, 28% lower IT costs, and 55% more efficient IT teams.Google disclosed several software enhancements designed to maximize Ironwood utilization. Google Kubernetes Engine now offers advanced maintenance and topology awareness for TPU clusters, enabling intelligent scheduling and highly resilient deployments. The company's open-source MaxText framework now supports advanced training techniques including Supervised Fine-Tuning and Generative Reinforcement Policy Optimization.Perhaps most significant for production deployments, Google's Inference Gateway intelligently load-balances requests across model servers to optimize critical metrics. According to Google, it can reduce time-to-first-token latency by 96% and serving costs by up to 30% through techniques like prefix-cache-aware routing.The Inference Gateway monitors key metrics including KV cache hits, GPU or TPU utilization, and request queue length, then routes incoming requests to the optimal replica. For conversational AI applications where multiple requests might share context, routing requests with shared prefixes to the same server instance can dramatically reduce redundant computation.The hidden challenge: powering and cooling one-megawatt server racksBehind these announcements lies a massive physical infrastructure challenge that Google addressed at the recent Open Compute Project EMEA Summit. The company disclosed that it's implementing +/-400 volt direct current power delivery capable of supporting up to one megawatt per rack — a tenfold increase from typical deployments."The AI era requires even greater power delivery capabilities," explained Madhusudan Iyengar and Amber Huffman, Google principal engineers, in an April 2025 blog post. "ML will require more than 500 kW per IT rack before 2030."Google is collaborating with Meta and Microsoft to standardize electrical and mechanical interfaces for high-voltage DC distribution. The company selected 400 VDC specifically to leverage the supply chain established by electric vehicles, "for greater economies of scale, more efficient manufacturing, and improved quality and scale."On cooling, Google revealed it will contribute its fifth-generation cooling distribution unit design to the Open Compute Project. The company has deployed liquid cooling "at GigaWatt scale across more than 2,000 TPU Pods in the past seven years" with fleet-wide availability of approximately 99.999%.Water can transport approximately 4,000 times more heat per unit volume than air for a given temperature change — critical as individual AI accelerator chips increasingly dissipate 1,000 watts or more.Custom silicon gambit challenges Nvidia's AI accelerator dominanceGoogle's announcements come as the AI infrastructure market reaches an inflection point. While Nvidia maintains overwhelming dominance in AI accelerators — holding an estimated 80-95% market share — cloud providers are increasingly investing in custom silicon to differentiate their offerings and improve unit economics.Amazon Web Services pioneered this approach with Graviton Arm-based CPUs and Inferentia / Trainium AI chips. Microsoft has developed Cobalt processors and is reportedly working on AI accelerators. Google now offers the most comprehensive custom silicon portfolio among major cloud providers.The strategy faces inherent challenges. Custom chip development requires enormous upfront investment — often billions of dollars. The software ecosystem for specialized accelerators lags behind Nvidia's CUDA platform, which benefits from 15+ years of developer tools. And rapid AI model architecture evolution creates risk that custom silicon optimized for today's models becomes less relevant as new techniques emerge.Yet Google argues its approach delivers unique advantages. "This is how we built the first TPU ten years ago, which in turn unlocked the invention of the Transformer eight years ago — the very architecture that powers most of modern AI," the company noted, referring to the seminal "Attention Is All You Need" paper from Google researchers in 2017.The argument is that tight integration — "model research, software, and hardware development under one roof" — enables optimizations impossible with off-the-shelf components.Beyond Anthropic, several other customers provided early feedback. Lightricks, which develops creative AI tools, reported that early Ironwood testing "makes us highly enthusiastic" about creating "more nuanced, precise, and higher-fidelity image and video generation for our millions of global customers," said Yoav HaCohen, the company's research director.Google's announcements raise questions that will play out over coming quarters. Can the industry sustain current infrastructure spending, with major AI companies collectively committing hundreds of billions of dollars? Will custom silicon prove economically superior to Nvidia GPUs? How will model architectures evolve?For now, Google appears committed to a strategy that has defined the company for decades: building custom infrastructure to enable applications impossible on commodity hardware, then making that infrastructure available to customers who want similar capabilities without the capital investment.As the AI industry transitions from research labs to production deployments serving billions of users, that infrastructure layer — the silicon, software, networking, power, and cooling that make it all run — may prove as important as the models themselves.And if Anthropic's willingness to commit to accessing up to one million chips is any indication, Google's bet on custom silicon designed specifically for the age of inference may be paying off just as demand reaches its inflection point.
- [Stop worrying about your AI footprint. Look at the big picture instead.](https://www.technologyreview.com/2025/11/06/1127579/ai-footprint/) — 19:00 · MIT Technology Review (AI)
  > Picture it: I’m minding my business at a party, parked by the snack table (of course). A friend of a friend wanders up, and we strike up a conversation. It quickly turns to work, and upon learning that I’m a climate technology reporter, my new acquaintance says something like: “Should I be using AI? I’ve…
- [Why Google’s File Search could displace DIY RAG stacks in the enterprise](https://venturebeat.com/ai/why-googles-file-search-could-displace-diy-rag-stacks-in-the-enterprise) — 13:00 · VentureBeat AI
  > By now, enterprises understand that retrieval augmented generation (RAG) allows applications and agents to find the best, most grounded information for queries. However, typical RAG setups could be an engineering challenge and also exhibit undesirable traits. To help solve this, Google released the File Search Tool on the Gemini API, a fully managed RAG system “that abstracts away the retrieval pipeline.” File Search removes much of the tool and application-gathering involved in setting up RAG pipelines, so engineers don’t need to stitch together things like storage solutions and embedding creators.  This tool competes directly with enterprise RAG products from OpenAI, AWS and Microsoft, which also aim to simplify RAG architecture. Google, though, claims its offering requires less orchestration and is more standalone. “File Search provides a simple, integrated and scalable way to ground Gemini with your data, delivering responses that are more accurate, relevant and verifiable,” Google said in a blog post. Enterprises can access some features of File Search, such as storage and embedding generation, for free at query time. Users will begin paying for embeddings when these files are indexed at a fixed rate of $0.15 per 1 million tokens. Google’s Gemini Embedding model, which eventually became the top embedding model on the Massive Text Embedding Benchmark, powers File Search. File Search and integrated experiences Google said File Search works “by handling the complexities of RAG for you.” File Search manages file storage, chunking strategies and embeddings. Developers can invoke File Search within the existing generateContent API, which Google said makes the tool easier to adopt. File Search uses vector search to “understand the meaning and context of a user’s query.” Ideally, it will find the relevant information to answer a query from documents, even if the prompt contains inexact words. The feature has built-in citations that point to the specific parts of a document it used to generate answers, and also supports a variety of file formats. These include PDF, Docx, txt, JSON and “many common programming language file types," Google says. Continuous RAG experimentation Enterprises may have already begun building out a RAG pipeline as they lay the groundwork for their AI agents to actually tap the correct data and make informed decisions. Because RAG represents a key part of how enterprises maintain accuracy and tap into insights about their business, organizations must quickly have visibility into this pipeline. RAG can be an engineering pain because orchestrating multiple tools together can become complicated. Building “traditional” RAG pipelines means organizations must assemble and fine-tune a file ingestion and parsing program, including chunking, embedding generation and updates. They must then contract a vector database like Pinecone, determine its retrieval logic, and fit it all within a model’s context window. Additionally, they can, if desired, add source citations. File Search aims to streamline all of that, although competitor platforms offer similar features. OpenAI’s Assistants API allows developers to utilize a file search feature, guiding an agent to relevant documents for responses. AWS’s Bedrock unveiled a data automation managed service in December. While File Search stands similarly to these other platforms, Google’s offering abstracts all, rather than just some, elements of the RAG pipeline creation. Phaser Studio, the creator of AI-driven game generation platform Beam, said in Google’s blog that it used File Search to sift through its library of 3,000 files.“File Search allows us to instantly surface the right material, whether that’s a code snippet for bullet patterns, genre templates or architectural guidance from our Phaser ‘brain’ corpus,” said Phaser CTO Richard Davey. “The result is ideas that once took days to prototype now become playable in minutes.”Since the announcement, several users expressed interest in using the feature.
- [The compute rethink: Scaling AI where data lives, at the edge](https://venturebeat.com/ai/the-compute-rethink-scaling-ai-where-data-lives-at-the-edge) — 13:00 · VentureBeat AI
  > Presented by ArmAI is no longer confined to the cloud or data centers. Increasingly, it’s running directly where data is created — in devices, sensors, and networks at the edge. This shift toward on-device intelligence is being driven by latency, privacy, and cost concerns that companies are confronting as they continue their investments in AI. For leadership teams, the opportunity is clear, says Chris Bergey, SVP and GM, of Arm’s Client Business: Invest in AI-first platforms that complement cloud usage, deliver real-time responsiveness, and protect sensitive data. "With the explosion of connected devices and the rise of IoT, edge AI provides a significant opportunity for organizations to gain a competitive edge through faster, more efficient AI," Bergey explains. "Those who move first aren’t just improving efficiency, they’re redefining what customers expect. AI is becoming a differentiator in trust, responsiveness, and innovation. The sooner a business makes AI central to its workflows, the faster it compounds that advantage." Use cases: Deploying AI where data livesEnterprises are discovering that edge AI isn’t just a performance boost — it’s a new operational model. Processing locally means less dependency on the cloud and faster, safer decision-making in real time. For instance, a factory floor can analyze equipment data instantly to prevent downtime, while a hospital can run diagnostic models securely on-site. Retailers are deploying in-store analytics using vision systems while logistic companies are using on-device AI to optimize fleet operations. Instead of sending vast data volumes to the cloud, organizations can analyze and act on insights where they emerge. The result is a more responsive, privacy-preserving, and cost-effective AI architecture.The consumer expectation: Immediacy and trustWorking with Alibaba’s Taobao team, the largest Chinese ecommerce platform, Arm (Nasdaq:Arm) enabled on-device product recommendations that update instantly without depending on the cloud. This helped online shoppers find what they need faster while keeping browsing data private.Another example comes from consumer tech: Meta’s Ray-Ban smart glasses, which blend cloud and on-device AI. The glasses handle quick commands locally for faster responses, while heavier tasks like translation and visual recognition are processed in the cloud."Every major technology shift has created new ways to engage and monetize," Bergey says. "As AI capabilities and user expectations grow, more intelligence will need to move closer to the edge to deliver this kind of immediacy and trust that people now expect." This shift is also taking place with the tools people use every day. Assistants like Microsoft Copilot and Google Gemini are blending cloud and on-device intelligence to bring generative AI closer to the user, delivering faster, more secure, and more context-aware experiences. That same principle applies across industries: the more intelligence you move safely and efficiently to the edge, the more responsive, private, and valuable your operations become. Building smarter for scaleThe explosion of AI at the edge demands not only smarter chips but smarter infrastructure. By aligning compute power with workload demands, enterprises can reduce energy consumption while maintaining high performance. This balance of sustainability and scale is fast becoming a competitive differentiator."Compute needs, whether in the cloud or on-premises, will continue to rise sharply. The question becomes, how do you maximize value from that compute?" he said. "You can only do this by investing in compute platforms and software that scale with your AI ambitions. The real measure of progress is enterprise value creation, not raw efficiency metrics."The intelligent foundationThe rapid evolution of AI models, especially those powering edge inferencing, multimodal applications, and low-latency responses, demands not just smarter algorithms, but a foundation of highly performant, energy-efficient hardware. As workloads grow more diverse and distributed, legacy architectures designed for traditional workloads are no longer adequate. The role of CPUs is evolving, and they now sit at the center of increasingly heterogenous systems that deliver advanced on-device AI experiences. Thanks to their flexibility, efficiency, and mature software support, modern CPUs can run everything from classic machine learning to complex generative AI workloads. When paired with accelerators such as NPUs or GPUs, they intelligently coordinate compute across the system — ensuring the right workload runs on the right engine for maximum performance and efficiency. The CPU continues to be the foundation that enables scalable, efficient AI everywhere.Technologies like Arm’s Scalable Matrix Extension 2 (SME2) bring advanced matrix acceleration to Armv9 CPUs. Meanwhile, Arm KleidiAI, its intelligent software layer, is extensively integrated across leading frameworks to automatically boost performance for a wide range of AI workloads, from language models to speech recognition to computer vision, running on Arm-based edge devices — without needing developers to rewrite their code."These technologies ensure that AI frameworks can tap into the full performance of Arm-based systems without extra developer effort," he says. "It’s how we make AI both scalable and sustainable: by embedding intelligence into the foundation of modern compute, so innovation happens at the speed of software, not hardware cycles."That democratization of compute power is also what will facilitate the next wave of intelligent, real-time experiences across the enterprise, not just in flagship products, but across entire device portfolios. The evolution of edge AI As AI moves from isolated pilots to full-scale deployment, the enterprises that succeed will be those that connect intelligence across every layer of infrastructure. Agentic AI systems will depend on this seamless integration — enabling autonomous processes that can reason, coordinate, and deliver value instantly."The pattern is familiar as in every disruptive wave, incumbents that move slowly risk being overtaken by new entrants," he says. "The companies that thrive will be the ones that wake up every morning asking how to make their organization AI-first. As with the rise of the internet and cloud computing, those who lean in and truly become AI-enabled will shape the next decade."Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
- [From prototype to production: What vibe coding tools must fix for enterprise adoption](https://venturebeat.com/ai/from-prototype-to-production-what-vibe-coding-tools-must-fix-for-enterprise) — 13:00 · VentureBeat AI
  > Presented by Salesforce Vibe coding — the fast-growing trend of using generative AI to spin up code from plain-language prompts — is quick, creative, and great for instant prototypes. But many argue that it's not cut out for building production-ready business apps with the security, governance, and trusted infrastructure that enterprises require. In other words, a few saved hours in development can mean a future full of security vulnerabilities, endless maintenance, and scalability headaches, says Mohith Shrivastava, principal developer advocate at Salesforce."For rapid experimentation, building minimum viable products, and tackling creative challenges, vibe coding is a game-changer," Shrivastava says. "However, that same speed and improvisational nature are exactly what makes its application in a professional, enterprise setting a topic of intense debate. And the skepticism from the developer community is 100% justified."Risks and rewards of vibe coding The excitement is all about speed: going from a rough idea to a working prototype in hours, not weeks, is a massive advantage. But as Shrivastava shared, developers have been vocal about the potential downsides."When you apply vibe coding indiscriminately to an entire application stack, you’re not just moving fast; you’re accumulating risk at an unprecedented rate," Shrivastava explains. "The cons are significant." That includes potential security nightmares, as AI models don't typically take into consideration the company's specific security policies. They can easily introduce vulnerabilities like hardcoded secrets or use insecure, hallucinated packages. Then there’s the issue of what Shrivastava calls "spaghetti code on steroids," or verbose code that lacks a coherent architectural pattern, creating a mountain of technical debt.Equally concerning is the illusion of progress: vibe coding may complete 80% of a feature in record time, but the remaining 20% — the edge cases, performance tuning, and compliance work — becomes exponentially harder.But does this mean vibe coding has no place in the enterprise?"The idea that you can just vibe your way to a complex, secure, and maintainable enterprise application is a dangerous fantasy," Shrivastava says. "But — the pros are undeniable if it's used correctly. The key is not to avoid vibe coding, but to apply it intelligently in your enterprise."Red and green zones: Enterprise-grade vibe codingYou can't, and you absolutely should not, vibe code your entire enterprise stack with just any generic tool, Shrivastava warns. But when paired with no-, low-, or pro-code tools that are built for the enterprise, many of the gaps can be addressed. An enterprise-grade vibe coding solution, for example, can automatically scan for security issues, flag performance bottlenecks, and provide a safety net. It’s also critical to understand which parts of an application suit this approach — and which demand a higher level of trust and control. Shrivastava divides the stack into red and green zones to illustrate.The green zone is the presentation layer, or the UI and UX. It’s ideal for vibe coding, where developers can move fast and iterate quickly without much risk. In contrast is the red zone, which covers the foundational pillars of an application, including business logic and data layers.Empowering developers in the green zoneDeveloper expertise remains the foundation for effective and safe vibe coding. But developers can be amplified by AI tools and emerging agents that are grounded in business context, connected to real applications, integrations, and data flows."A generic AI agent can't grasp your company's unique processes, but a context-aware tool can act as a powerful pair programmer, helping a developer draft complex logic or model data with greater speed and accuracy," Shrivastava says. "It’s about making the expert developer more efficient, not trying to do their job for them."Some areas will always be high risk for ungoverned AI — especially infrastructure and security. Letting a generic AI agent configure firewalls or Identity and Access Management [IAM] policies without oversight, Shrivastava warns, is a recipe for disaster. The solution isn’t to avoid the red zone entirely, but to approach it with the right tools — ones that embed governance, security, and context from the ground up."The winning strategy is clear: Vibe code the green zone for agility, approach the red zone by augmenting your developers with powerful, context-aware tools, and never, ever DIY your core infrastructure with AI," he says.Embracing enterprise vibe codingTo harness the power of enterprise vibe coding, Salesforce developed Agentforce Vibes. This new vibe coding offering for the enterprise includes Agentforce, an autonomous AI agent built to collaborate like a pair programmer on the Salesforce Platform. It’s designed precisely to provide developers with the right tools for the job, covering both the green and red zones. For the green zone, it offers the speed and agility to rapidly build UIs and prototypes. But its true power lies in how it augments developers in the red zone."Enterprise vibe coding like Agentforce lets organizations take AI-assisted development to the organizational level, accelerating coding, testing, and deployment, while ensuring consistency, security, and performance," says Dan Fernandez, VP of product, developer services at Salesforce. "It's not about throwing away governance for speed; it’s about integrating AI into every stage of the application lifecycle to work smarter."Because Agentforce Vibes’ tooling is deeply integrated with your business context on the platform, it can safely assist with business logic and data modeling. Most importantly, it operates on a trusted platform. Instead of a DIY approach — jury-rigging a generic AI agent to handle your networking — developers build on a foundation that has security and governance built in, so they can innovate safely, knowing the most critical layers of the stack are secure and compliant.Major enterprises are putting vibe coding to work Agentforce Vibes users are now tapping the tool to build around 20 to 25% of their new code base, according to Salesforce data, and users are accepting around 1.2 million lines of agentic code per month. That includes companies like Coinbase, CGI, Grupo Globo, and one of the top five banks in the U.S., which is using Agentforce Vibes capabilities to develop production-ready apps faster. Agentforce Vibes is part of a suite of tools in Agentforce 360 that span from no-code and low-code to pro-code development. These tools are together helping customers develop and deploy at speeds previously unheard of.With the low-code Agent Builder in Agentforce, the Secret Escapes team was able to build, test, and launch their agent to support customer service in just two weeks, compared to the six months it had previously taken the company to build and train a bot. With Agentforce, 1-800Accountant autonomously resolved 70% of customer chat engagements during tax week in 2025, without writing a line of code, using Salesforce’s low-code tools and AI assistance. Meanwhile, media company Grupo Globo deployed agents to identify subscribers at risk of lapsing, offer personalized upgrades, cross-sell, and convert non-subscribers. As a result, Agentforce boosted Globo’s retention rates by 22% in less than three months.Innovation meets discipline Enterprise tools show that disciplined engineering and creative experimentation can coexist — and that balance, Shrivastava says, is the key to lasting innovation."Vibe coding is not a fad, but it's also not a silver bullet that will replace disciplined software engineering," Shrivastava says. "The smart path forward is a hybrid approach where human software skills are augmented with agentic intelligence. This balanced approach is how you get the best of both worlds: radical innovation at the edge and unwavering stability at the core."Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
- [Google Cloud updates its AI Agent Builder with new observability dashboard and faster build-and-deploy tools](https://venturebeat.com/ai/the-agent-builder-arms-race-continues-as-google-cloud-pushes-deeper-into) — 01:44 · VentureBeat AI
  > Google Cloud has introduced a big update in a bid to keep AI developers on its Vertex AI platform for concepting, designing, building, testing, deploying and modifying AI agents in enterprise use cases.The new features, announced today, include additional governance tools for enterprises and expanding the capabilities for creating agents with just a few lines of code, moving faster with state-of-the-art context management layers and one-click deployment, as well as managed services for scaling production and evaluation, and support for identifying agents.Agent Builder, released last year during its annual Cloud Next event, provides a no-code platform for enterprises to create agents and connect these to orchestration frameworks like LangChain.Google’s Agent Development Kit (ADK), which lets developers build agents “in under 100 lines of code,” can also be accessed through Agent Builder. “These new capabilities underscore our commitment to Agent Builder, and simplify the agent development process to meet developers where they are, no matter which tech stack they choose,” said Mike Clark, director of Product Management, Vertex AI Agent Builder. Build agents fasterPart of Google’s pitch for Agent Builder’s new features is that enterprises can bake in-orchestration even as they construct their agents. “Building an agent from a concept to a working product involves complex orchestration,” said Clark. The new capabilities, which are shipped with the ADK, include:SOTA context management layers including Static, Turn, User and Cache layers so enterprises have more control over the agents’ contextPrebuilt plugins with customizable logic. One of the new plugins allows agents to recognize failed tool calls and “self-heal” by retrying the task with a different approachAdditional language support in ADK, including Go, alongside Python and Java, that launched with ADKOne-click deployment through the ADK command line interface to move agents from a local environment to live testing with a single commandGovernance layerEnterprises require high accuracy; security; observability and auditability (what a program did and why); and steerability (control) in their production-grade AI agents.While Google had observability features in the local development environment at launch, developers can now access these tools through the Agent Engine managed runtime dashboard. The company said this brings cloud-based production monitoring to track token consumption, error rates and latency. Within this observability dashboard, enterprises can visualize the actions agents take and reproduce any issues. Agent Engine will also have a new Evaluation Layer to help “simulate agent performance across a vast array of user interactions and situations.”This governance layer will also include:Agent Identities that Google said give “agents their own unique, native identities within Google Cloud Model Armor, which would block prompt injections, screen tool calls and agent responsesSecurity Command Center, so admins can build an inventory of their agents to detect threats like unauthorized access“These native identities provide a deep, built-in layer of control and a clear audit trail for all agent actions. These certificate-backed identities further strengthen your security as they cannot be impersonated and are tied directly to the agent's lifecycle, eliminating the risk of dormant accounts,” Clark said. The battle of agent builders It’s no surprise that model providers create platforms to build agents and bring them to production. The competition lies in how fast new tools and features are added.Google’s Agent Builder competes with OpenAI’s open-source Agent Development Kit, which enables developers to create AI agents using non-OpenAI models. Additionally, there is the recently announced AgentKit, which features an Agent Builder that enables companies to integrate agents into their applications easily. Microsoft has its Azure AI Foundry, launched last year around this time for AI agent creation, and AWS also offers agent builders on its Bedrock platform, but Google is hoping is suite of new features will help give it a competitive edge. However, it isn’t just companies with their own models that court developers to build their AI agents within their platforms. Any enterprise service provider with an agent library also wants clients to make agents on their systems. Capturing developer interest and keeping them within the ecosystem is the big battle between tech companies now, with features to make building and governing agents easier.
