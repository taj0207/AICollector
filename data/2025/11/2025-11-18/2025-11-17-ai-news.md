# AI News for 2025-11-17 (America/Los_Angeles)

Collected 32 article(s).

- [Stock market sell-off continues, as Google boss warns ‘no company immune’ if AI bubble bursts – business live - The Guardian](https://news.google.com/rss/articles/CBMi0AFBVV95cUxONXNQMk9HY19YTmlXZVA1VmFRZEtscDNZdHM5WGQ3Xy0xMGZFRGZjcmk4Ym54ZDFpVk96NHJLSFZfOTZRVnEtZER4dUdUU1ZJbllzMnhsNTd3UnhPOXY4TWMtN1RHdng3aF9KeERJaEpqOG03SFNxUEFQejFsTmZNUXVvc0VYNGthLVcxR1NYZWNlQnJDOFhrRFRqUkFIdWRUMURkd0dSYW80R1owd0s2Sm9VSzU4ZTBZNmlRZGUyWU1aNUNQVG9rV2pRQjJtakRJ?oc=5) — 23:26 · Google News (AI)
  > Stock market sell-off continues, as Google boss warns ‘no company immune’ if AI bubble bursts – business live  The Guardian
- [Barry Callebaut to use NotCo AI to develop chocolate recipes - Reuters](https://news.google.com/rss/articles/CBMioAFBVV95cUxQMEhkZ21wNEFkU0JWd29oR254bExmV01ETVY5ajl6RzVCa2hRaW1fU1FLdW1JQjJrWHNxUVNwRm02WElSVFZQWU1iQ3J3NUF2Y3oyZ2s5dzFGVFdGeU5PczVEWU82RTZYWmxVb2cyOFYxczEzOGFpdUpfVWtMVFdDVkdtSURhS3pPaDlLSVJmMWtCWl9pQkd6a185LS1UVDVk?oc=5) — 23:15 · Google News (AI)
  > Barry Callebaut to use NotCo AI to develop chocolate recipes  Reuters
- [Is the AI bubble about to burst, and what's driving analyst jitters? - Euronews.com](https://news.google.com/rss/articles/CBMirwFBVV95cUxNUmtUSVJETzdidzJWaXB1RTJFeHlNMnh0QTctUl9nVm1ucmZEcnVQTnM2RUV1YlBnSmZzdDM0X296QlhGdEhmek4taDFua0hORVY0R1RBRXA3SDNlZXRyOVNCYmp5REhJQkFaNi1hcFJnVFJOU2g2MWhaTWZSYnEtdDF5djN3bTA0clg5c3RNLXI3aTNkNDRyXzRBbml0MEtqWGJNMTJLbWstZUhpbkdv?oc=5) — 22:00 · Google News (AI)
  > Is the AI bubble about to burst, and what's driving analyst jitters?  Euronews.com
- [As consumers ditch Google for ChatGPT, Peec AI raises $21M to help brands adapt - TechCrunch](https://news.google.com/rss/articles/CBMisgFBVV95cUxNc1p2WlpCY0xMU0FkYUpmQ3F6Z2RuSVI2cW1fOFk0S3FkTWd0eVYwWEgwVXh4dm14M18zVk5TR2ZzbDdCS0MzVER4VGRPNlFDNjVBQ0MtR1FQQ24zeTl3RGpsN0lCbHJDME0zZlBYYXhGV1NGOVlWTDQzOEwweG8zVUtjU2tCMmREcDU2cGg2ZS1PemJvdzh2VjFDZng1TFdOWkphazlDODd3UXNaaFlucjR3?oc=5) — 21:10 · Google News (AI)
  > As consumers ditch Google for ChatGPT, Peec AI raises $21M to help brands adapt  TechCrunch
- [Google boss Sundar Pichai warns 'no company immune' if AI bubble bursts - BBC](https://news.google.com/rss/articles/CBMiWkFVX3lxTE9kX3ZqVE9nazRmcGJ3U0pzc1dmSlIxYjJCRkJKQ0UtZG5RZnlZdmJTUEUwX2xlVDg3am9kUHRQZDFGV2QyYmw2YnQ1NWcwVEt0ZHlqNjBvNFZ0QdIBX0FVX3lxTE92RHBrVWhGVGZiVDNjU0hwMGhiWnRvS2tBdDBTZWNjXzBRNXNETFVkbkNZUnBTUTFhclpVWWxxREEwc1ROaU5pN2ZGQk0xcWFSSk9aZTJYWkIxWXNSci1V?oc=5) — 21:02 · Google News (AI)
  > Google boss Sundar Pichai warns 'no company immune' if AI bubble bursts  BBC
- [Amazon Founder Jeff Bezos Launches AI Startup Project Prometheus - PYMNTS.com](https://news.google.com/rss/articles/CBMiuwFBVV95cUxNckhFOG56RG5kMk11QWRPb0t3MV9LcWtVVjhCMm5hMjZWZTVUclN6UmxMdEdpYTlLcENnM0tkNmFDS2RwMDZMemxXZFRtSy1iRUJGU2FUc1FUTVpqalQ1dnI1aGhjRzIySE1KMW02S0ZabXpkMy1qRVZnVnc2bzIzTEplWmZPQ1pKSW5pSk9leEVQNjJsQ0phV2NNOU9BMWpvaXY3VU80X1FYYVEtM0YzMU9BbDVKN3N0WGJz?oc=5) — 19:35 · Google News (AI)
  > Amazon Founder Jeff Bezos Launches AI Startup Project Prometheus  PYMNTS.com
- [Meta AI chief Yann LeCun reportedly leaving company - Mashable](https://news.google.com/rss/articles/CBMihwFBVV95cUxPblQwbHk0eTczaEZQdXdVNG1fM3psLXRJdjJRNExMWWZKU1FlMkNjcTZsU3JYV3FvdFNESjFpX0U0VnRyZ1ExMmJEMnFYX3hlczlOYnJ0U3ZTM2ZMbGY5N3h4dndrTTJ1a1V4djd3SEl5bG95YnNuY0g5RklqVVREQ3FDby1ZdHc?oc=5) — 18:40 · Google News (AI)
  > Meta AI chief Yann LeCun reportedly leaving company  Mashable
- [The Most Joyless Tech Revolution Ever: AI Is Making Us Rich and Unhappy - The Wall Street Journal](https://news.google.com/rss/articles/CBMisANBVV95cUxNdWJPOWZlVWh0V3NRX01pQ0tPWDJDOXcybGwzU3dlMHgwVS01c3RjVW5WU1I1VkE2endySF8yTi1XVlE3VF9FUGlMUkYza2RkTmhhWVh4Mjg0azRLamgwYkROSVU0MTE2V0EzeWQzSjVDMVlJN0djblpOVXFVLU8tTWVDZENQbVp2TU9ZU2h6QWdoLVdYb3dPSzZmWGNWVmk5TkJ4T0xTM3EwRzY2WXFzZy0yTGdfbi1uS0FJNWQ0RnE3REJ4aldwY3J6NmZnWExiVXRMY01lZmdiZG5lNDRiOWI5UXk4d2JGN3lYYWdHVndpSGNZRFZIaEZfcVZrNHVKSDA2M2twaU5nbGZsSmpHMjdoaC14X0ExTmpBRzFOMHlyVkZYSDhjbm5mbVRnb3hEc3dtSFkzbzl4Y254S3JVTmxMaldKYnczcXpZX1lpV29JZjNFNGlTOXJPVzV5S0RGeXgtdkZyZko2MFV1OEl2aHV0NHFsQmZlU2hiNEtwdVRpLTVOekNucHNqN0lYTkhTWkFjd3g3dGNZMUZaNDVwbEIxb1BfNTlVN01qaFdEX0E?oc=5) — 17:00 · Google News (AI)
  > The Most Joyless Tech Revolution Ever: AI Is Making Us Rich and Unhappy  The Wall Street Journal
- [Authors dumped from New Zealand’s top book prize after AI used in cover designs - The Guardian](https://news.google.com/rss/articles/CBMiwgFBVV95cUxOa2F0LW5xREVNdVg3YUstNEpuNEYxWjZPWnFibWdSYjFPbDRkcUJITGkycVpQbFZhTWR3c1ozOVd4TkhZSEtsNml5YU5mWlFyTzdYX3RzTTM4anFZb0luektRdG45cms1eFhibEhvZTlMSHp5UElZYnJ1YkpiUjMzWHhjTHI0Yjkwdks3SGQ0bUlUdjFGNkdMTkd5ZWU4TU94UWc5Mk4weXdUdTRjWUtzZEt6QnZRV0prS2c2R3FmemlPdw?oc=5) — 16:58 · Google News (AI)
  > Authors dumped from New Zealand’s top book prize after AI used in cover designs  The Guardian
- [Jeff Bezos Brings Signature Management Style to $6 Billion AI Startup - Bloomberg.com](https://news.google.com/rss/articles/CBMiuwFBVV95cUxQSEN2WVdzNEFXY25WZXpsbHJ4VExlaWlkdW9NX0poZDlXT0p2RGNlQ2RReTJYWU45VXVqWV9QT20yZ0VVZGZoS3hyc2FQTVZFOGpKRVdsaUs1NGp4V1A1RUI4a2d6Z3EwOHBZdl9lSWc0MUdBV1YtWjNjTmRidDlidzJsaTRMRXhwajg2cXVsSV9HZkNTZ00tQUFmb25fc2d3MDJBdk5uTWpMSmI1V0lVQjRCRzlvQ0J6SlRF?oc=5) — 15:25 · Google News (AI)
  > Jeff Bezos Brings Signature Management Style to $6 Billion AI Startup  Bloomberg.com
- [Google is fighting the defamation battle Meta caved on](https://www.theverge.com/news/822636/google-robby-starbuck-defamation-lawsuit-ai-chatbot) — 15:11 · The Verge (AI)
  > Google filed a motion to dismiss a defamation lawsuit brought by anti-corporate diversity activist Robby Starbuck, who claimed Google’s AI falsely associated him with sexual assault allegations and a white nationalist. Starbuck’s claims against Google came after he filed a similar lawsuit against Meta, whose AI he claimed falsely asserted that he’d participated in the […]
- [The best foldable phone you can buy](https://www.theverge.com/tech/635839/best-foldable-phone) — 15:00 · The Verge (AI)
  > Editor’s note: Black Friday doesn’t officially take place until Friday, November 28th; however, if you want to shop ahead of time, we’ve rounded up the best early Black Friday deals you can already get. A foldable phone isn’t for the faint of heart. They’re generally heavier, pricier, and have less capable cameras than a standard slab-style phone. They’re […]
- [OpenAI is finally letting employees donate their equity to charity](https://www.theverge.com/ai-artificial-intelligence/822496/openai-employee-equity-donation-charity-rounds-share-valuation) — 14:47 · The Verge (AI)
  > Current and former OpenAI employees have grown increasingly frustrated by the fact that the company hasn't allowed them to donate their equity to charity in years. But OpenAI has finally seemed to bow to pressure, sending out an email from the company's equity team stating that current and former employees with eligible shares will be […]
- [Niall Ferguson: The AI Boom Is a House of Cards - The Free Press](https://news.google.com/rss/articles/CBMiekFVX3lxTE1PbDBTenFzczU5TWhLdjMtNDhpZUVmUWhoeEFqbVlPeDN4a1lFRGQxdWlvLU02QV9DdmlsOHR4ckhhSnB6UldaTW9BMU55YUFxcElwQnZDdEJMOVRMWTBMMTV3U0lINmlGTjB1Y3pYLXlnSG9MUk9vZERR?oc=5) — 14:42 · Google News (AI)
  > Niall Ferguson: The AI Boom Is a House of Cards  The Free Press
- [Google updates its weather forecasts with a new AI model](https://www.theverge.com/news/822489/weather-forecast-ai-model-google-weathernext) — 14:00 · The Verge (AI)
  > Google debuted a new and improved AI weather model today that will be incorporated into popular products like Search, Gemini, and Pixel phones.  The company’s AI-enhanced forecasts have proven themselves to be quite accurate so far. AI weather models have also been able to make predictions faster and more efficiently than conventional physics-based models. Until […]
- [The AI Bubble That Isn’t There - Forbes](https://news.google.com/rss/articles/CBMiigFBVV95cUxNUEo3TThrQ1pMQVZBNW1mbUw3MWk1bFlLLW5zM3FTR29fQUM2WDVkTDg2YktkV2ZEeVB5T2VqMXlwUUQ1eHZJclpMYmNONTBmcXphdy0zUGlYMDFEbWF6WXhpT1VPSUpTYWRjUzZuVUtRZEVDQ1hfMW50Y3lCV2JNZGlSVHhwazJHdnc?oc=5) — 13:31 · Google News (AI)
  > The AI Bubble That Isn’t There  Forbes
- [The best Garmin watches for training and everyday life](https://www.theverge.com/23691498/best-garmin-smartwatch-fitness-tracker-wearables) — 13:20 · The Verge (AI)
  > Editor’s note: Black Friday doesn’t officially take place until Friday, November 28th; however, if you want to shop ahead of time, we’ve rounded up the best early Black Friday deals you can already get. Few brands are as synonymous with outdoor sports as Garmin. You’ll find these fitness trackers and smartwatches on dozens of wrists at any 5K, […]
- [Goldman says the stock market has already priced in the AI boom, with $19 trillion of market value running ahead of actual economic impact so far - Fortune](https://news.google.com/rss/articles/CBMingFBVV95cUxOYm41UFlQZExMTkJIX0hLd3BrQVZuRDEzbGZkUkRJYWJqWWNVbE5CREVaeW1xYy1XN2w5M3BzVVpZZUtOUldHZWdrN2dzaGl5Y2x6dFE0R1VYYjZpWE0ta2c3d3FzXzdKckZ4Z3NYczd4WjdIaDl6ZFpUbGZyNnh5UndsUkFJQmhnNWp6OWR3enFhZGE1dU1mWFFGVTBRQQ?oc=5) — 12:56 · Google News (AI)
  > Goldman says the stock market has already priced in the AI boom, with $19 trillion of market value running ahead of actual economic impact so far  Fortune
- [NetChoice sues Virginia to block its one-hour social media limit for kids](https://www.theverge.com/news/822475/netchoice-virginia-lawsuit-social-media-time-limit-law) — 12:49 · The Verge (AI)
  > The tech industry trade group NetChoice is suing Virginia over a new law that will restrict minors from using social media for more than one hour per day. The lawsuit, filed on Monday, asks the court to block the law over claims it violates the First Amendment by putting “unlawful barriers on how and when […]
- [AI's next big leap is models that understand the world. - Axios](https://news.google.com/rss/articles/CBMib0FVX3lxTFAzQjdBUS10eHl4blFrUDhwWU9uUVUzV2VqUUh2MjQ1S0oyOUpZalAxb25lekwxM3A1bTBQZ2wxUE10UWxvcnhHcUREV25URWY4MzE5Ri1ia0xUV0lPOU5DQ000SjY0bHdtRjhvTS1wNA?oc=5) — 12:38 · Google News (AI)
  > AI's next big leap is models that understand the world.  Axios
- [AI firms must be clear on risks or repeat tobacco’s mistakes, says Anthropic chief | Artificial intelligence (AI) - The Guardian](https://news.google.com/rss/articles/CBMivAFBVV95cUxPaHhfV05mRWdYd0hOckVHdTdCdVZIdWo0cXVmVDItTlE0dGtyRk1iUWc4MU5rMFdWemYzQVZzcGhiYzlFNmY1ekFja1lQM0p0ejZLR1BwTnVMZ1hHS3Y0djRQOXF5QmRTeDBHS3l2RkU5VHdaQWVIcGoxM0N4b1FRMjdLRFNMcXVIclJBYzU3QTN4aDhuMjByQ0VEV3laWHJWenJVMUlicm1JUGZQYmozd2RQZXZ2U25yZGdSag?oc=5) — 12:09 · Google News (AI)
  > AI firms must be clear on risks or repeat tobacco’s mistakes, says Anthropic chief | Artificial intelligence (AI)  The Guardian
- [Bose’s noise-canceling QuietComfort Headphones are more than 50 percent off](https://www.theverge.com/gadgets/822365/bose-noise-canceling-quietcomfort-headphones-amazon-deal-sale) — 12:00 · The Verge (AI)
  > Traveling during the holidays can be chaotic and loud, but a quality pair of over-ear headphones can provide much-needed peace and quiet on your journey. Fortunately, you can currently snag Bose’s QuietComfort Headphones on sale at Amazon for an all-time low of $169.99 ($180 off) in select colors. That’s more than 50 percent off the […]
- [A.I. Chatbots Are Changing How Patients Get Medical Advice - The New York Times](https://news.google.com/rss/articles/CBMiigFBVV95cUxPUE1zeVFLVS1BSTc2cDF0OG1DRWRYeGIwQmhuNUFUWkpYZS1xM2lxOHdiLVhDVG1SS0VpLVNzYmRZX096YjVmOTZvRDNoREFsNXpaMHhadDRnRTk3NThNTjVtWGRGOGFvdFp5UDcxU29DNThRdFE5alF0R25VM09pU2tNNEUwQ20tdUE?oc=5) — 11:53 · Google News (AI)
  > A.I. Chatbots Are Changing How Patients Get Medical Advice  The New York Times
- [Jeff Bezos Creates A.I. Start-Up Where He Will Be Co-Chief Executive - The New York Times](https://news.google.com/rss/articles/CBMigAFBVV95cUxNdVN1bzgwRE5kMWd2R1haM29RTUtZWHNMNjdKdDBZd2xYX2M5bGRsdXFCSDl1bEVFd182cmRVUTBXSlUyWXhfaGZyelBlakQ0VWVvQnJRbTEwWDZBUnZkbmJpZTV4d09pUHdWeFJMMENfSUxQdTU4ZUN5WS1uamQ4YQ?oc=5) — 11:03 · Google News (AI)
  > Jeff Bezos Creates A.I. Start-Up Where He Will Be Co-Chief Executive  The New York Times
- [NASA, Industry Weave Data Fabric with Artificial Intelligence - NASA (.gov)](https://news.google.com/rss/articles/CBMitwFBVV95cUxPNUNWRTVkczhSdWRKdjFSbUxId2kxdDdBdzh1ZmE5TWNJbUxsQ05tYUZaaDdvaF9qX2o4ek9NQUVWRGFZZklDTUw5RWEyQUd3X1FMbElZaHpPTGFNQXFUb21xM3dDV2k0Y3hDcWpOa29OcHpiLXFMS3huZVhlMTV6bHdrczRPLXZ0VnNZaTFJanQ5Q2ZCWDZUOUZGaHAyZ0RlWHI5bk5OdGZ4d3BsTTNYcUE4Mk1jak0?oc=5) — 10:53 · Google News (AI)
  > NASA, Industry Weave Data Fabric with Artificial Intelligence  NASA (.gov)
- [Jeff Bezos’ Big Bet on A.I. Is Project Prometheus - The New York Times](https://news.google.com/rss/articles/CBMiigFBVV95cUxNWURUTHRBbXRsTXdfNnZaNnVQNXNIaVhWcmtaLXBLYjc5SXQyUG9QYXpyVDE5TXNuYXJZeUVGSUE2N2FyLTR5X05Rb2tqXzQza1prQVJranZ4eG84QmtXc0ttOUdWVV9tT2JicUVlQ2xJU3dZRWxmcVJnbE1DU0twUUt0TWhsYmNYU1E?oc=5) — 10:45 · Google News (AI)
  > Jeff Bezos’ Big Bet on A.I. Is Project Prometheus  The New York Times
- [Exclusive: Public unveils AI-powered brokerage and investing tools - Axios](https://news.google.com/rss/articles/CBMiZkFVX3lxTE13aGlVR1l0d0hiWHBacUdUU0dWYnUybkR6el9tbXkyQnJfc3JNODFSMXR2ZTBXTHpBQmVKTnJuVXRMYm4zVUpkVTJTWkN1OVV2UHN5cDVHM253ZFE1dHNuVUZlNGZEUQ?oc=5) — 10:03 · Google News (AI)
  > Exclusive: Public unveils AI-powered brokerage and investing tools  Axios
- [New ways to plan travel with AI in Search - blog.google](https://news.google.com/rss/articles/CBMihwFBVV95cUxQaW9mTTc4cjg3QTBFdTNPM0VXd1NveHpyQjdoTGt2T2tZVHNISDdQY05TaVFJTl9INmlBSkJYVi1DZUNwXzhKbjZMcXM2V2JZcy1ZSzAyQXhzODl0UXdETTRJZnoyVExhcDdUV2d1Y3lXWEJodEQ5M1NMY0t5RGswNnBOcUx0Q2s?oc=5) — 09:09 · Google News (AI)
  > New ways to plan travel with AI in Search  blog.google
- [The State of AI: How war will be changed forever](https://www.technologyreview.com/2025/11/17/1127514/the-state-of-ai-the-new-rules-of-war/) — 08:30 · MIT Technology Review (AI)
  > Welcome back to The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday, writers from both publications debate one aspect of the generative AI revolution reshaping global power. In this conversation, Helen Warrell, FT investigations reporter and former defense and security editor, and James O’Donnell, MIT Technology Review’s…
- [The Download: the risk of falling space debris, and how to debunk a conspiracy theory](https://www.technologyreview.com/2025/11/17/1127988/the-download-the-risk-of-falling-space-debris-and-how-to-debunk-a-conspiracy-theory/) — 05:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. What is the chance your plane will be hit by space debris? The risk of flights being hit by space junk is still small, but it’s growing. About three pieces of old space…
- [What is the chance your plane will be hit by space debris?](https://www.technologyreview.com/2025/11/17/1127980/what-is-the-chance-your-plane-will-be-hit-by-space-debris/) — 02:00 · MIT Technology Review (AI)
  > MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. You can read more from the series here. In mid-October, a mysterious object cracked the windshield of a packed Boeing 737 cruising at 36,000 feet above Utah, forcing the pilots into an emergency landing.…
- [Phi-4 proves that a 'data-first' SFT methodology is the new differentiator](https://venturebeat.com/ai/phi-4-proves-that-a-data-first-sft-methodology-is-the-new-differentiator) — 00:00 · VentureBeat AI
  > AI engineers often chase performance by scaling up LLM parameters and data, but the trend toward smaller, more efficient, and better-focused models has accelerated. The Phi-4 fine-tuning methodology is the cleanest public example of a training approach that smaller enterprise teams can copy. It shows how a carefully chosen dataset and fine-tuning strategy can make a 14B model compete with much larger ones.The Phi-4 model was trained on just 1.4 million carefully chosen prompt-response pairs. Instead of brute force, the Microsoft Phi-4 research team focused on “teachable” examples at the edge of the model’s abilities and rigorous data curation. The Phi-4 reasoning smart data playbook demonstrates how strategic data curation with replicable SFT and RL can elevate a 14B model beyond much larger counterparts.Why Phi-4 stands apartSmaller reasoning models, such as OpenAI’s o1-mini and Google’s Gemma, are becoming more common, and models like Alibaba’s Qwen3 (8B and 14B) are seeing wide adoption across use cases. That adoption is important, but it doesn’t displace the value of Phi-4 as an experimental proof: Phi-4 was designed as a testbed for a data-first training methodology, and its documentation reads like a smart data playbook for teams that want to replicate that approach.The Phi-4 team has shared a repeatable SFT playbook that includes a 1.4-million-prompt response set. It’s built around “teachable” edge examples, questions that are neither too easy nor too difficult, chosen to push the model’s reasoning. Each topic, such as math or code, is tuned separately and then combined with synthetic rewrites that turn complex tasks into forms that can be checked automatically. The paper outlines the data selection and filtering process in enough detail for smaller teams to reproduce it with open-source models and evaluators. For enterprise teams, that level of transparency turns a research result into a practical, copyable training recipe they can implement and measure quickly.The data-first philosophy: Why less can be moreTraditional approaches to LLM reasoning have often relied on scaling datasets massively to encourage generalization. Phi-4 reasoning takes a different path, showing that carefully curated data can achieve similar or even better results with far less.The team assembled a dataset covering STEM, coding, and safety. Despite its small size, it outperformed models trained on orders of magnitude more data. In benchmarks, the 14B Phi-4 reasoning model outperformed OpenAI’s o1-mini and DeepSeek’s 70B distilled model across most reasoning tasks, and approached the full DeepSeek-R1 (671B) on challenging math (AIME) questions. With just 14 billion parameters, Phi-4 reasoning delivers the following results when compared to other leading models:Benchmark (task)Phi-4 reasoningComparison model (size)Comparison scoreDate / SourceAIME 2024 (math olympiad)75.3%o1-mini63.6%Microsoft Phi-4 model card (Apr 2025). (Hugging Face)AIME 2025 (math olympiad)62.9%DeepSeek-R1-Distill-70B51.5%Microsoft Phi-4 model card (April 2025). (Hugging Face)OmniMath76.6%DeepSeek-R1-Distill-70B63.4%Microsoft Phi-4 model card (April 2025). (Hugging Face)GPQA-Diamond (graduate-level science)65.8%o1-mini60.0%Microsoft Phi-4 model card (April 2025). (Hugging Face)OmniMath (same benchmark, different comparison)76.6%Claude-3.7-Sonnet54.6%Microsoft Phi-4 model card (April 2025). (Hugging Face)Table: Phi-4 reasoning performance across benchmarks compared to other models. Source: MicrosoftThe key to this is filtering for quality over quantity. Much of the generic data is either too easy (the base model already knows it) or too hard (no learning signal). The Phi-4 team explicitly discards such examples. “Given the strong baseline reasoning capabilities of Phi-4, many initial seed questions are already handled competently,” they note. “To make further learning impactful, we specifically target seeds situated at the edge of Phi-4’s current abilities.” In practice, they rely on LLM-based evaluation. For each candidate question, a strong reference model (like GPT-4) generates an “answer key,” and the answers from weaker models are compared. If the weaker model disagrees enough, it indicates a teachable gap. Those questions are retained, while trivially solved or utterly unsolvable questions are dropped. For example, a simple arithmetic problem might be dropped (too easy), and an extremely obscure theorem proof might be dropped (too hard) as well. But a moderately challenging geometry problem that Phi-4 gets wrong is included.This “sweet spot” approach ensures every example forces the model to stretch its reasoning. By focusing on multi-step problems rather than rote recall, they pack maximum learning into 1.4M examples. As the authors explain, training on these carefully chosen seeds “leads to broad generalization across both reasoning-specific and general-purpose tasks.” In effect, Phi-4 reasoning demonstrates that intelligent data selection can outperform brute force scaling. Independent domain optimizationPhi-4 reasoning’s data are grouped by domain (math, coding, puzzles, safety, etc.). Rather than blending everything at once, the team tunes each domain’s mix separately and then merges them. This relies on an “additive property”: Optimizing math data in isolation and code data in isolation yields weights that, when concatenated, still give gains in both areas. In practice, they first tuned the math dataset to saturation on math benchmarks, then did the same for code, and finally simply added the code data into the math recipe. The result was improved performance on both math and coding tasks, without retraining from scratch.This modular approach offers clear practical advantages. This means a small team can first refine just the math dataset, achieve strong math performance, and then later add the coding data without redoing the math tuning.However, the Phi-4 authors caution that scaling this method to many domains remains an open question. While the approach “worked very well” for their math+code mix, they note, “it is not known whether this method can scale to dozens or hundreds of domains,” a direction they acknowledge as a valuable area for future research. In short, the additive strategy is effective, but expanding into new domains must be approached carefully, as it may introduce unforeseen interactions.Despite potential pitfalls, the additive strategy proved effective in Phi-4 reasoning. By treating each domain independently, the team avoided complex joint optimization and narrowed the search space for data mixtures. This approach allows incremental scaling of domains. Teams can begin by tuning the math SFT, then incorporate the code dataset, and later expand to additional specialized tasks, all while maintaining prior performance gains. This is a practical advantage for resource-constrained teams. Instead of requiring a large group of experts to manage a complex, multi-domain dataset, a small team can focus on one data silo at a time.Synthetic data transformationSome reasoning problems, such as abstract proofs or creative tasks, are difficult to verify automatically. Yet automated verification (for RL reward shaping) is very valuable. Phi-4 reasoning tackled this by transforming hard prompts into easier-to-check forms. For example, the team rewrote a subset of coding problems as word puzzles or converted some math problems to have concise numeric answers. These “synthetic seed data” preserve the underlying reasoning challenge but make correctness easier to test. Think of it as giving the model a simplified version of the riddle that still teaches the same logic. This engineering hack enables downstream RL to use clear reward signals on tasks that would otherwise be too open-ended. Here’s an example of synthetic data transformation:Raw web dataSynthetic dataOn the sides AB and BC of triangle ABC, points M and N are taken, respectively. It turns out that the perimeter of △AMC is equal to the perimeter of △CNA, and the perimeter of △ANB is equal to the perimeter of △CMB. Prove that △ABC is isosceles.ABC is a triangle with AB=13 and BC=10. On the sides AB and BC of triangle ABC, points M and N are taken, respectively. It turns out that the perimeter of △AMC is equal to the perimeter of △CNA, and the perimeter of △ANB is equal to the perimeter of △CMB. What is AC?Table: Rewriting seed data from the web (left) into verifiable synthetic questions for SFT and RL (right). Source: MicrosoftNote that by assigning numeric values (AB=13, BC=10) and asking “What is AC?”, the answer becomes a single number, which can be easily checked for correctness.Other teams have applied similar domain-specific tricks. For example, chemistry LLMs like FutureHouse’s ether0 model generate molecules under strict pKa or structural constraints, using crafted reward functions to ensure valid chemistry. In mathematics, the Kimina-Prover model by Numina translates natural-language theorems into the Lean formal system, so reinforcement learning can verify correct proofs. These examples highlight how synthetic augmentation, when paired with verifiable constraints, can push models to perform well in highly specialized domains.In practical terms, engineers should embrace synthetic data but keep it grounded. Heuristics like “convert to numeric answers” or “decompose a proof into checkable steps” can make training safer and more efficient. At the same time, maintain a pipeline of real (organic) problems as well, to ensure breadth. The key is balance. Use synthetic transformations to unlock difficult verification problems, but don’t rely on them exclusively. Real-world diversity still matters. Following this approach, the model is guided toward a clearly defined, discrete objective.Here are some results on Phi-4 reasoning models:Practical implementation for enterprisesAI teams looking to apply Phi-4 reasoning’s insights can follow a series of concrete steps to implement the approach effectively.Identifying the model’s edgeDetect your model’s “edge” by identifying where the base LLM struggles. One way is to use its confidence or agreement scores. For example, generate several answers per prompt (using a tool like Hugging Face’s vLLM for fast sampling) and see where consensus breaks. Those prompts at the margin of confidence are your teachable examples. By focusing on these low-confidence questions rather than the questions it already gets right, you ensure each new example is worth learning.Isolating domains for targeted tuningTune one domain at a time rather than mixing all data genres upfront. Pick the highest-value domain for your app (math, code, legal, etc.) and craft a small SFT dataset for just that. Iterate on the mix (balancing difficulty, source types, etc.) until performance saturates on domain-specific benchmarks. Then freeze that mix and add the next domain. This modular tuning follows Phi-4 reasoning’s “additive” strategy. It avoids cross-talk since you preserve gains in domain A even as you improve domain B.Expanding with synthetic augmentationLeverage synthetic augmentation when gold-standard answers are scarce or unverifiable. For instance, if you need to teach a proof assistant but can’t autocheck proofs, transform them into arithmetic puzzles or shorter proofs that can be verified. Use your LLM to rewrite or generate these variants (Phi-4 used this to turn complex word problems into numeric ones). Synthetic augmentation also lets you expand data cheaply. Once you have a validated small set, you can “multiply” it by having the LLM generate paraphrases, variations, or intermediate reasoning steps.Scaling through a two-phase strategyUse a two-phase training strategy that begins with exploration followed by scaling. In Phase 1 (exploration), run short fine-tuning experiments on a focused dataset (e.g., one domain) with limited compute. Track a few key metrics (benchmarks or held-out tasks) each run. Rapidly iterate hyperparameters and data mixes. The Phi-4 paper demonstrates that this speeds up progress, as small experiments helped the team discover a robust recipe before scaling up. Only once you see consistent gains do you move to Phase 2 (scaling), where you combine your verified recipes across domains and train longer (in Phi-4’s case, ~16 billion tokens). Although this stage is more compute-intensive, the risk is significantly reduced by the prior experimentation.Monitor for trigger points such as a significant uplift on validation tasks or stable metric trends. When those appear, it’s time to scale. If not, refine the recipe more first. This disciplined two-phase loop saves resources and keeps the team agile.In practice, many teams at Hugging Face and elsewhere have followed similar advice. For example, while developing conversational model SmolLM2, the team noticed poor chat performance in Phase 1. They then generated ~500K synthetic multi-turn dialogues and re-trained, which “significantly improved both downstream performance and its overall ‘vibes,’” as one researcher reports. This represents a concrete win, achieved through a targeted synthetic data injection based on an initial feedback loop.How to do this nowHere’s a simple checklist that you can follow to put these ideas into action.Pick a target domain/task. Choose one area (e.g., math, coding, or a specific application) where you need better performance. This keeps the project focused.Collect a small seed dataset. Gather, say, a few thousand prompt–answer pairs in that domain from existing sources (textbooks, GitHub, etc.).Filter for edge-of-ability examples. Use a strong model (e.g., GPT-4) to create an answer key for each prompt. Run your base model on those prompts. Keep examples that the base model often misses, discard ones it already solves or is hopeless on. This yields “teachable” examples.Fine-tune your model (Phase 1). Run a short SFT job on this curated data. Track performance on a held-out set or benchmark. Iterate: Refine the data mix, remove easy questions, add new teachable ones, until gains taper off.Add synthetic examples if needed. If some concepts lack auto-verifiable answers (like long proofs), create simpler numeric or single-answer variants using your LLM. This gives clear rewards for RL. Keep a balance with real problems.Expand to the next domain. Once one domain is tuned, “freeze” its dataset. Pick a second high-value domain and repeat steps 3 to 5 to tune that data mix. Finally, merge the data for both domains, and do a final longer training run (Phase 2).Monitor benchmarks carefully. Use a consistent evaluation methodology (like  majority-voting runs) to avoid misleading results. Only proceed to a full-scale training if small experiments show clear improvements.Limits and trade-offsDespite the effectiveness of the Phi-4 training method, several limitations and practical considerations remain. One key challenge is domain scaling. While Phi-4’s additive method worked well for math and code, it has yet to be proven across many domains. The authors acknowledge that it remains an open question whether this approach can scale smoothly to dozens of topics. Another concern is the use of synthetic data. Relying too heavily on synthetic rewrites can reduce the diversity of the dataset, so it’s crucial to maintain a balance between real and synthetic examples to preserve the model's ability to reason effectively. Lastly, while the repeatable SFT method helps reduce computational costs, it doesn’t eliminate the need for thoughtful curation. Even though the approach is more efficient than brute-force scaling, it still requires careful data selection and iteration.Lessons from Phi-4The Phi-4 reasoning story is clear: Bigger isn’t always better for reasoning models. Instead of blindly scaling, the team asked where learning happens and engineered their data to hit that sweet spot. They show that “the benefit of careful data curation for supervised fine-tuning extends to reasoning models.” In other words, with a smart curriculum, you can squeeze surprising capability out of modest models.For engineers, the takeaway is actionable. You don’t need a billion-dollar cluster or an endless internet crawl to improve reasoning. For resource-strapped teams, this is good news, as a careful data strategy lets you punch above your weight.Phi-4 reasoning proves that methodical data and training design, not sheer parameter count, drives advanced reasoning. Focusing on teachable data and iterative tuning, even a 14B model surpassed much larger rivals. For AI teams today, this offers a practical blueprint. Refine the data, iterate fast, and scale only when the signals are right. These steps can unlock breakthrough reasoning performance without breaking the bank.
