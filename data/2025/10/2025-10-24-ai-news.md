# AI News for 2025-10-24 (Asia/Taipei)

Collected 9 article(s).

- [What we know about energy use at U.S. data centers amid the AI boom - Pew Research Center](https://news.google.com/rss/articles/CBMiuAFBVV95cUxPb1lqZC1Wdnk4aEwzVVFZZ01DTmxycVRBWENTTUFpSGdZZ2NWYlFnWDdWVXBzbjhIZnJpZ1V6akc5YnVQY2pTVjFPSDQ1dUlLN3ZiVjhaM2dXTVplU29hWndlSU9SeTNGc2JqRVQ3b1lWUnJoVXdQRmR4dC1ITkNIdDg5TWpwVVJrc1lDZVJ4X2dRNzlqaWJOdGpodS1Va1pQeFRTRGhLZUJUQVhvUlBEbVFlM2gwSlRY?oc=5) — 22:52 · Google News (AI)
  > What we know about energy use at U.S. data centers amid the AI boom  Pew Research Center
- [The Download: carbon removal’s future, and measuring pain using an app](https://www.technologyreview.com/2025/10/24/1126569/the-download-carbon-removals-future-and-measuring-pain-using-an-app/) — 20:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. What’s next for carbon removal? After years of growth that spawned hundreds of startups, the nascent carbon removal sector appears to be facing a reckoning. Running Tide, a promising aquaculture company, shut down…
- [Thinking Machines challenges OpenAI's AI scaling strategy: 'First superintelligence will be a superhuman learner'](https://venturebeat.com/ai/thinking-machines-challenges-openais-ai-scaling-strategy-first) — 17:30 · VentureBeat AI
  > While the world's leading artificial intelligence companies race to build ever-larger models, betting billions that scale alone will unlock artificial general intelligence, a researcher at one of the industry's most secretive and valuable startups delivered a pointed challenge to that orthodoxy this week: The path forward isn't about training bigger — it's about learning better."I believe that the first superintelligence will be a superhuman learner," Rafael Rafailov, a reinforcement learning researcher at Thinking Machines Lab, told an audience at TED AI San Francisco on Tuesday. "It will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process."This breaks sharply with the approach pursued by OpenAI, Anthropic, Google DeepMind, and other leading laboratories, which have bet billions on scaling up model size, data, and compute to achieve increasingly sophisticated reasoning capabilities. Rafailov argues these companies have the strategy backwards: what's missing from today's most advanced AI systems isn't more scale — it's the ability to actually learn from experience."Learning is something an intelligent being does," Rafailov said, citing a quote he described as recently compelling. "Training is something that's being done to it."The distinction cuts to the core of how AI systems improve — and whether the industry's current trajectory can deliver on its most ambitious promises. Rafailov's comments offer a rare window into the thinking at Thinking Machines Lab, the startup co-founded in February by former OpenAI chief technology officer Mira Murati that raised a record-breaking $2 billion in seed funding at a $12 billion valuation.Why today's AI coding assistants forget everything they learned yesterdayTo illustrate the problem with current AI systems, Rafailov offered a scenario familiar to anyone who has worked with today's most advanced coding assistants."If you use a coding agent, ask it to do something really difficult — to implement a feature, go read your code, try to understand your code, reason about your code, implement something, iterate — it might be successful," he explained. "And then come back the next day and ask it to implement the next feature, and it will do the same thing."The issue, he argued, is that these systems don't internalize what they learn. "In a sense, for the models we have today, every day is their first day of the job," Rafailov said. "But an intelligent being should be able to internalize information. It should be able to adapt. It should be able to modify its behavior so every day it becomes better, every day it knows more, every day it works faster — the way a human you hire gets better at the job."The duct tape problem: How current training methods teach AI to take shortcuts instead of solving problemsRafailov pointed to a specific behavior in coding agents that reveals the deeper problem: their tendency to wrap uncertain code in try/except blocks — a programming construct that catches errors and allows a program to continue running."If you use coding agents, you might have observed a very annoying tendency of them to use try/except pass," he said. "And in general, that is basically just like duct tape to save the entire program from a single error."Why do agents do this? "They do this because they understand that part of the code might not be right," Rafailov explained. "They understand there might be something wrong, that it might be risky. But under the limited constraint—they have a limited amount of time solving the problem, limited amount of interaction—they must only focus on their objective, which is implement this feature and solve this bug."The result: "They're kicking the can down the road."This behavior stems from training systems that optimize for immediate task completion. "The only thing that matters to our current generation is solving the task," he said. "And anything that's general, anything that's not related to just that one objective, is a waste of computation."Why throwing more compute at AI won't create superintelligence, according to Thinking Machines researcherRafailov's most direct challenge to the industry came in his assertion that continued scaling won't be sufficient to reach AGI."I don't believe we're hitting any sort of saturation points," he clarified. "I think we're just at the beginning of the next paradigm—the scale of reinforcement learning, in which we move from teaching our models how to think, how to explore thinking space, into endowing them with the capability of general agents."In other words, current approaches will produce increasingly capable systems that can interact with the world, browse the web, write code. "I believe a year or two from now, we'll look at our coding agents today, research agents or browsing agents, the way we look at summarization models or translation models from several years ago," he said.But general agency, he argued, is not the same as general intelligence. "The much more interesting question is: Is that going to be AGI? And are we done — do we just need one more round of scaling, one more round of environments, one more round of RL, one more round of compute, and we're kind of done?"His answer was unequivocal: "I don't believe this is the case. I believe that under our current paradigms, under any scale, we are not enough to deal with artificial general intelligence and artificial superintelligence. And I believe that under our current paradigms, our current models will lack one core capability, and that is learning."Teaching AI like students, not calculators: The textbook approach to machine learningTo explain the alternative approach, Rafailov turned to an analogy from mathematics education."Think about how we train our current generation of reasoning models," he said. "We take a particular math problem, make it very hard, and try to solve it, rewarding the model for solving it. And that's it. Once that experience is done, the model submits a solution. Anything it discovers—any abstractions it learned, any theorems—we discard, and then we ask it to solve a new problem, and it has to come up with the same abstractions all over again."That approach misunderstands how knowledge accumulates. "This is not how science or mathematics works," he said. "We build abstractions not necessarily because they solve our current problems, but because they're important. For example, we developed the field of topology to extend Euclidean geometry — not to solve a particular problem that Euclidean geometry couldn't handle, but because mathematicians and physicists understood these concepts were fundamentally important."The solution: "Instead of giving our models a single problem, we might give them a textbook. Imagine a very advanced graduate-level textbook, and we ask our models to work through the first chapter, then the first exercise, the second exercise, the third, the fourth, then move to the second chapter, and so on—the way a real student might teach themselves a topic."The objective would fundamentally change: "Instead of rewarding their success — how many problems they solved — we need to reward their progress, their ability to learn, and their ability to improve."This approach, known as "meta-learning" or "learning to learn," has precedents in earlier AI systems. "Just like the ideas of scaling test-time compute and search and test-time exploration played out in the domain of games first" — in systems like DeepMind's AlphaGo — "the same is true for meta learning. We know that these ideas do work at a small scale, but we need to adapt them to the scale and the capability of foundation models."The missing ingredients for AI that truly learns aren't new architectures—they're better data and smarter objectivesWhen Rafailov addressed why current models lack this learning capability, he offered a surprisingly straightforward answer."Unfortunately, I think the answer is quite prosaic," he said. "I think we just don't have the right data, and we don't have the right objectives. I fundamentally believe a lot of the core architectural engineering design is in place."Rather than arguing for entirely new model architectures, Rafailov suggested the path forward lies in redesigning the data distributions and reward structures used to train models."Learning, in of itself, is an algorithm," he explained. "It has inputs — the current state of the model. It has data and compute. You process it through some sort of structure, choose your favorite optimization algorithm, and you produce, hopefully, a stronger model."The question: "If reasoning models are able to learn general reasoning algorithms, general search algorithms, and agent models are able to learn general agency, can the next generation of AI learn a learning algorithm itself?"His answer: "I strongly believe that the answer to this question is yes."The technical approach would involve creating training environments where "learning, adaptation, exploration, and self-improvement, as well as generalization, are necessary for success.""I believe that under enough computational resources and with broad enough coverage, general purpose learning algorithms can emerge from large scale training," Rafailov said. "The way we train our models to reason in general over just math and code, and potentially act in general domains, we might be able to teach them how to learn efficiently across many different applications."Forget god-like reasoners: The first superintelligence will be a master studentThis vision leads to a fundamentally different conception of what artificial superintelligence might look like."I believe that if this is possible, that's the final missing piece to achieve truly efficient general intelligence," Rafailov said. "Now imagine such an intelligence with the core objective of exploring, learning, acquiring information, self-improving, equipped with general agency capability—the ability to understand and explore the external world, the ability to use computers, ability to do research, ability to manage and control robots."Such a system would constitute artificial superintelligence. But not the kind often imagined in science fiction."I believe that intelligence is not going to be a single god model that's a god-level reasoner or a god-level mathematical problem solver," Rafailov said. "I believe that the first superintelligence will be a superhuman learner, and it will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process."This vision stands in contrast to OpenAI's emphasis on building increasingly powerful reasoning systems, or Anthropic's focus on "constitutional AI." Instead, Thinking Machines Lab appears to be betting that the path to superintelligence runs through systems that can continuously improve themselves through interaction with their environment.The $12 billion bet on learning over scaling faces formidable challengesRafailov's appearance comes at a complex moment for Thinking Machines Lab. The company has assembled an impressive team of approximately 30 researchers from OpenAI, Google, Meta, and other leading labs. But it suffered a setback in early October when Andrew Tulloch, a co-founder and machine learning expert, departed to return to Meta after the company launched what The Wall Street Journal called a "full-scale raid" on the startup, approaching more than a dozen employees with compensation packages ranging from $200 million to $1.5 billion over multiple years.Despite these pressures, Rafailov's comments suggest the company remains committed to its differentiated technical approach. The company launched its first product, Tinker, an API for fine-tuning open-source language models, in October. But Rafailov's talk suggests Tinker is just the foundation for a much more ambitious research agenda focused on meta-learning and self-improving systems."This is not easy. This is going to be very difficult," Rafailov acknowledged. "We'll need a lot of breakthroughs in memory and engineering and data and optimization, but I think it's fundamentally possible."He concluded with a play on words: "The world is not enough, but we need the right experiences, and we need the right type of rewards for learning."The question for Thinking Machines Lab — and the broader AI industry — is whether this vision can be realized, and on what timeline. Rafailov notably did not offer specific predictions about when such systems might emerge.In an industry where executives routinely make bold predictions about AGI arriving within years or even months, that restraint is notable. It suggests either unusual scientific humility — or an acknowledgment that Thinking Machines Lab is pursuing a much longer, harder path than its competitors.For now, the most revealing detail may be what Rafailov didn't say during his TED AI presentation. No timeline for when superhuman learners might emerge. No prediction about when the technical breakthroughs would arrive. Just a conviction that the capability was "fundamentally possible" — and that without it, all the scaling in the world won't be enough.
- [An AI app to measure pain is here](https://www.technologyreview.com/2025/10/24/1126451/an-ai-app-to-measure-pain-is-here/) — 17:00 · MIT Technology Review (AI)
  > How are you feeling? I’m genuinely interested in the well-being of all my treasured Checkup readers, of course. But this week I’ve also been wondering how science and technology can help answer that question—especially when it comes to pain. In the latest issue of MIT Technology Review magazine, Deena Mousa describes how an AI-powered smartphone app…
- [What’s next for carbon removal?](https://www.technologyreview.com/2025/10/24/1126478/whats-next-for-carbon-removal/) — 17:00 · MIT Technology Review (AI)
  > MIT Technology Review’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them here. In the early 2020s, a little-known aquaculture company in Portland, Maine, snagged more than $50 million by pitching a plan to harness nature to fight back…
- [Mistral launches its own AI Studio for quick development with its European open source, proprietary models](https://venturebeat.com/ai/mistral-launches-its-own-ai-studio-for-quick-development-with-its-european) — 14:55 · VentureBeat AI
  > The next big trend in AI providers appears to be "studio" environments on the web that allow users to spin up agents and AI applications within minutes. Case in point, today the well-funded French AI startup Mistral launched its own Mistral AI Studio, a new production platform designed to help enterprises build, observe, and operationalize AI applications at scale atop Mistral's growing family of proprietary and open source large language models (LLMs) and multimodal models.It's an evolution of its legacy API and AI building platorm, "Le Platforme," initially launched in late 2023, and that brand name is being retired for now. The move comes just days after U.S. rival Google updated its AI Studio, also launched in late 2023, to be easier for non-developers to use and build and deploy apps with natural language, aka "vibe coding."But while Google's update appears to target novices who want to tinker around, Mistral appears more fully focused on building an easy-to-use enterprise AI app development and launchpad, which may require some technical knowledge or familiarity with LLMs, but far less than that of a seasoned developer. In other words, those outside the tech team at your enterprise could potentially use this to build and test simple apps, tools, and workflows — all powered by E.U.-native AI models operating on E.U.-based infrastructure. That may be a welcome change for companies concerned about the political situation in the U.S., or who have large operations in Europe and prefer to give their business to homegrown alternatives to U.S. and Chinese tech giants.In addition, Mistral AI Studio appears to offer an easier way for users to customize and fine-tune AI models for use at specific tasks.Branded as “The Production AI Platform,” Mistral's AI Studio extends its internal infrastructure, bringing enterprise-grade observability, orchestration, and governance to teams running AI in production.The platform unifies tools for building, evaluating, and deploying AI systems, while giving enterprises flexible control over where and how their models run — in the cloud, on-premise, or self-hosted. Mistral says AI Studio brings the same production discipline that supports its own large-scale systems to external customers, closing the gap between AI prototyping and reliable deployment. It's available here with developer documentation here.Extensive Model CatalogAI Studio’s model selector reveals one of the platform’s strongest features: a comprehensive and versioned catalog of Mistral models spanning open-weight, code, multimodal, and transcription domains.Available models include the following, though note that even for the open source ones, users will still be running a Mistral-based inference and paying Mistral for access through its API.ModelLicense TypeNotes / SourceMistral LargeProprietaryMistral’s top-tier closed-weight commercial model (available via API and AI Studio only).Mistral MediumProprietaryMid-range performance, offered via hosted API; no public weights released.Mistral SmallProprietaryLightweight API model; no open weights.Mistral TinyProprietaryCompact hosted model optimized for latency; closed-weight.Open Mistral 7BOpenFully open-weight model (Apache 2.0 license), downloadable on Hugging Face.Open Mixtral 8×7BOpenReleased under Apache 2.0; mixture-of-experts architecture.Open Mixtral 8×22BOpenLarger open-weight MoE model; Apache 2.0 license.Magistral MediumProprietaryNot publicly released; appears only in AI Studio catalog.Magistral SmallProprietarySame; internal or enterprise-only release.Devstral MediumProprietary / LegacyOlder internal development models, no open weights.Devstral SmallProprietary / LegacySame; used for internal evaluation.Ministral 8BOpenOpen-weight model available under Apache 2.0; basis for Mistral Moderation model.Pixtral 12BProprietaryMultimodal (text-image) model; closed-weight, API-only.Pixtral LargeProprietaryLarger multimodal variant; closed-weight.Voxtral SmallProprietarySpeech-to-text/audio model; closed-weight.Voxtral MiniProprietaryLightweight version; closed-weight.Voxtral Mini Transcribe 2507ProprietarySpecialized transcription model; API-only.Codestral 2501OpenOpen-weight code-generation model (Apache 2.0 license, available on Hugging Face).Mistral OCR 2503ProprietaryDocument-text extraction model; closed-weight.This extensive model lineup confirms that AI Studio is both model-rich and model-agnostic, allowing enterprises to test and deploy different configurations according to task complexity, cost targets, or compute environments.Bridging the Prototype-to-Production DivideMistral’s release highlights a common problem in enterprise AI adoption: while organizations are building more prototypes than ever before, few transition into dependable, observable systems. Many teams lack the infrastructure to track model versions, explain regressions, or ensure compliance as models evolve.AI Studio aims to solve that. The platform provides what Mistral calls the “production fabric” for AI — a unified environment that connects creation, observability, and governance into a single operational loop. Its architecture is organized around three core pillars: Observability, Agent Runtime, and AI Registry.1. ObservabilityAI Studio’s Observability layer provides transparency into AI system behavior. Teams can filter and inspect traffic through the Explorer, identify regressions, and build datasets directly from real-world usage. Judges let teams define evaluation logic and score outputs at scale, while Campaigns and Datasets automatically transform production interactions into curated evaluation sets.Metrics and dashboards quantify performance improvements, while lineage tracking connects model outcomes to the exact prompt and dataset versions that produced them. Mistral describes Observability as a way to move AI improvement from intuition to measurement.2. Agent Runtime and RAG supportThe Agent Runtime serves as the execution backbone of AI Studio. Each agent — whether it’s handling a single task or orchestrating a complex multi-step business process — runs within a stateful, fault-tolerant runtime built on Temporal. This architecture ensures reproducibility across long-running or retry-prone tasks and automatically captures execution graphs for auditing and sharing.Every run emits telemetry and evaluation data that feed directly into the Observability layer. The runtime supports hybrid, dedicated, and self-hosted deployments, allowing enterprises to run AI close to their existing systems while maintaining durability and control.While Mistral's blog post doesn’t explicitly reference retrieval-augmented generation (RAG), Mistral AI Studio clearly supports it under the hood. Screenshots of the interface show built-in workflows such as RAGWorkflow, RetrievalWorkflow, and IngestionWorkflow, revealing that document ingestion, retrieval, and augmentation are first-class capabilities within the Agent Runtime system. These components allow enterprises to pair Mistral’s language models with their own proprietary or internal data sources, enabling contextualized responses grounded in up-to-date information. By integrating RAG directly into its orchestration and observability stack—but leaving it out of marketing language—Mistral signals that it views retrieval not as a buzzword but as a production primitive: measurable, governed, and auditable like any other AI process.3. AI RegistryThe AI Registry is the system of record for all AI assets — models, datasets, judges, tools, and workflows. It manages lineage, access control, and versioning, enforcing promotion gates and audit trails before deployments.Integrated directly with the Runtime and Observability layers, the Registry provides a unified governance view so teams can trace any output back to its source components.Interface and User ExperienceThe screenshots of Mistral AI Studio show a clean, developer-oriented interface organized around a left-hand navigation bar and a central Playground environment.The Home dashboard features three core action areas — Create, Observe, and Improve — guiding users through model building, monitoring, and fine-tuning workflows.Under Create, users can open the Playground to test prompts or build agents.Observe and Improve link to observability and evaluation modules, some labeled “coming soon,” suggesting staged rollout.The left navigation also includes quick access to API Keys, Batches, Evaluate, Fine-tune, Files, and Documentation, positioning Studio as a full workspace for both development and operations.Inside the Playground, users can select a model, customize parameters such as temperature and max tokens, and enable integrated tools that extend model capabilities.Users can try the Playground for free, but will need to sign up with their phone number to receive an access code.Integrated Tools and CapabilitiesMistral AI Studio includes a growing suite of built-in tools that can be toggled for any session:Code Interpreter — lets the model execute Python code directly within the environment, useful for data analysis, chart generation, or computational reasoning tasks.Image Generation — enables the model to generate images based on user prompts.Web Search — allows real-time information retrieval from the web to supplement model responses.Premium News — provides access to verified news sources via integrated provider partnerships, offering fact-checked context for information retrieval.These tools can be combined with Mistral’s function calling capabilities, letting models call APIs or external functions defined by developers. This means a single agent could, for example, search the web, retrieve verified financial data, run calculations in Python, and generate a chart — all within the same workflow.Beyond Text: Multimodal and Programmatic AIWith the inclusion of Code Interpreter and Image Generation, Mistral AI Studio moves beyond traditional text-based LLM workflows. Developers can use the platform to create agents that write and execute code, analyze uploaded files, or generate visual content — all directly within the same conversational environment.The Web Search and Premium News integrations also extend the model’s reach beyond static data, enabling real-time information retrieval with verified sources. This combination positions AI Studio not just as a playground for experimentation but as a full-stack environment for production AI systems capable of reasoning, coding, and multimodal output.Deployment FlexibilityMistral supports four main deployment models for AI Studio users:Hosted Access via AI Studio — pay-as-you-go APIs for Mistral’s latest models, managed through Studio workspaces.Third-Party Cloud Integration — availability through major cloud providers.Self-Deployment — open-weight models can be deployed on private infrastructure under the Apache 2.0 license, using frameworks such as TensorRT-LLM, vLLM, llama.cpp, or Ollama.Enterprise-Supported Self-Deployment — adds official support for both open and proprietary models, including security and compliance configuration assistance.These options allow enterprises to balance operational control with convenience, running AI wherever their data and governance requirements demand.Safety, Guardrailing, and ModerationAI Studio builds safety features directly into its stack. Enterprises can apply guardrails and moderation filters at both the model and API levels.The Mistral Moderation model, based on Ministral 8B (24.10), classifies text across policy categories such as sexual content, hate and discrimination, violence, self-harm, and PII. A separate system prompt guardrail can be activated to enforce responsible AI behavior, instructing models to “assist with care, respect, and truth” while avoiding harmful or unethical content.Developers can also employ self-reflection prompts, a technique where the model itself classifies outputs against enterprise-defined safety categories like physical harm or fraud. This layered approach gives organizations flexibility in enforcing safety policies while retaining creative or operational control.From Experimentation to Dependable OperationsMistral positions AI Studio as the next phase in enterprise AI maturity. As large language models become more capable and accessible, the company argues, the differentiator will no longer be model performance but the ability to operate AI reliably, safely, and measurably.AI Studio is designed to support that shift. By integrating evaluation, telemetry, version control, and governance into one workspace, it enables teams to manage AI with the same discipline as modern software systems — tracking every change, measuring every improvement, and maintaining full ownership of data and outcomes.In the company’s words, “This is how AI moves from experimentation to dependable operations — secure, observable, and under your control.”Mistral AI Studio is available starting October 24, 2025, as part of a private beta program. Enterprises can sign up on Mistral’s website to access the platform, explore its model catalog, and test observability, runtime, and governance features before general release.
- [Inside Ring-1T: Ant engineers solve reinforcement learning bottlenecks at trillion scale](https://venturebeat.com/ai/inside-ring-1t-ant-engineers-solve-reinforcement-learning-bottlenecks-at) — 12:00 · VentureBeat AI
  > China’s Ant Group, an affiliate of Alibaba, detailed technical information around its new model, Ring-1T, which the company said is “the first open-source reasoning model with one trillion total parameters.”Ring-1T aims to compete with other reasoning models like GPT-5 and the o-series from OpenAI, as well as Google’s Gemini 2.5. With the new release of the latest model, Ant extends the geopolitical debate over who will dominate the AI race: China or the US. Ant Group said Ring-1T is optimized for mathematical and logical problems, code generation and scientific problem-solving. “With approximately 50 billion activated parameters per token, Ring-1T achieves state-of-the-art performance across multiple challenging benchmarks — despite relying solely on natural language reasoning capabilities,” Ant said in a paper.Ring-1T, which was first released on preview in September, adopts the same architecture as Ling 2.0 and trained on the Ling-1T-base model the company released earlier this month. Ant said this allows the model to support up to 128,000 tokens.To train a model as large as Ring-1T, researchers had to develop new methods to scale reinforcement learning (RL).New methods of training
Ant Group developed three “interconnected innovations” to support the RL and training of Ring-1T, a challenge given the model's size and the typically large compute requirements it entails. These three are IcePop, C3PO++ and ASystem.IcePop removes noisy gradient updates to stabilize training without slowing inference. It helps eliminate catastrophic training-inference misalignment in RL. The researchers noted that when training models, particularly those using a mixture-of-experts (MoE) architecture like Ring-1T, there can often be a discrepancy in probability calculations. “This problem is particularly pronounced in the training of MoE models with RL due to the inherent usage of the dynamic routing mechanism. Additionally, in long CoT settings, these discrepancies can gradually accumulate across iterations and become further amplified,” the researchers said. IcePop “suppresses unstable training updates through double-sided masking calibration.”The next new method the researchers had to develop is C3PO++, an improved version of the C3PO system that Ant previously established. The method manages how Ring-1T and other extra-large parameter models generate and process training examples, or what they call rollouts, so GPUs don’t sit idle. The way it works would break work in rollouts into pieces to process in parallel. One group is the inference pool, which generates new data, and the other is the training pool, which collects results to update the model. C3PO++ creates a token budget to control how much data is processed, ensuring GPUs are used efficiently.The last new method, ASystem, adopts a SingleController+SPMD (Single Program, Multiple Data) architecture to enable asynchronous operations.  Benchmark resultsAnt pointed Ring-1T to benchmarks measuring performance in mathematics, coding, logical reasoning and general tasks. They tested it against models such as DeepSeek-V3.1-Terminus-Thinking, Qwen-35B-A22B-Thinking-2507, Gemini 2.5 Pro and GPT-5 Thinking. In benchmark testing, Ring-1T performed strongly, coming in second to OpenAI’s GPT-5 across most benchmarks. Ant said that Ring-1T showed the best performance among all the open-weight models it tested. The model posted a 93.4% score on the AIME 25 leaderboard, second only to GPT-5. In coding, Ring-1T outperformed both DeepSeek and Qwen.“It indicates that our carefully synthesized dataset shapes Ring-1T’s robust performance on programming applications, which forms a strong foundation for future endeavors on agentic applications,” the company said. Ring-1T shows how much Chinese companies are investing in models Ring-1T is just the latest model from China aiming to dethrone GPT-5 and Gemini. Chinese companies have been releasing impressive models at a quick pace since the surprise launch of DeepSeek in January. Ant's parent company, Alibaba, recently released Qwen3-Omni, a multimodal model that natively unifies text, image, audio and video. DeepSeek has also continued to improve its models and earlier this month, launched DeepSeek-OCR. This new model reimagines how models process information. With Ring-1T and Ant’s development of new methods to train and scale extra-large models, the battle for AI dominance between the US and China continues to heat up.
- [OpenAI launches company knowledge in ChatGPT, letting you access your firm's data from Google Drive, Slack, GitHub](https://venturebeat.com/ai/openai-launches-company-knowledge-in-chatgpt-letting-you-access-your-firms) — 06:19 · VentureBeat AI
  > Is the Google Search for internal enterprise knowledge finally here...but from OpenAI? It certainly seems that way. Today, OpenAI has launched company knowledge in ChatGPT, a major new capability for subscribers to ChatGPT's paid Business, Enterprise, and Edu plans that lets them call up their company's data directly from third-party workplace apps including Slack, SharePoint, Google Drive, Gmail, GitHub, HubSpot and combine it in ChatGPT outputs to them. As OpenAI's CEO of Applications Fidji Simo put it in a post on the social network X: "it brings all the context from your apps (Slack, Google Drive, GitHub, etc) together in ChatGPT so you can get answers that are specific to your business."Intriguingly, OpenAI's blog post on the feature states that is "powered by a version of GPT‑5 that’s trained to look across multiple sources to give more comprehensive and accurate answers," which sounds to me like a new fine-tuned version of the model family the company released back in August, though there are no additional details on how it was trained or its size, techniques, etc.OpenAI tells VentureBeat it's a version of GPT-5 that specifically powers company knowledge in ChatGPT Business, Enterprise, and Edu. Nonetheless, company knowledge in ChatGPT is rolling out globally and is designed to make ChatGPT a central point of access for verified organizational information, supported by secure integrations and enterprise-grade compliance controls, and give employees way faster access to their company's information while working.Now, instead of toggling over to Slack to find the assignment you were given and instructions, or tabbing over to Google Drive and opening up specific files to find the names and numbers you need to call, ChatGPT can deliver all that type of information directly into your chat session — if your company enables the proper connections.As OpenAI Chief Operating Officer Brad Lightcap wrote in a post on the social network X: "company knowledge has changed how i use chatgpt at work more than anything we have built so far - let us know what you think!"It builds upon the third-party app connectors unveiled back in August 2025, though those were only for individual users on the ChatGPT Plus plans.Connecting ChatGPT to Workplace SystemsEnterprise teams often face the challenge of fragmented data across various internal tools—email, chat, file storage, project management, and customer platforms. Company knowledge bridges those silos by enabling ChatGPT to connect to approved systems like, and other supported apps through enterprise-managed connectors.Each response generated with company knowledge includes citations and direct links to the original sources, allowing teams to verify where specific details originated. This transparency helps organizations maintain data trustworthiness while increasing productivity.The sidebar shows a live view of the sources being examined and what it is getting from them. When it’s done, you’ll see exactly the sources used, along with the specific snippets it drew from. You can then click on any citation to open the original source for more details.Built for Enterprise Control and SecurityCompany knowledge was designed from the ground up for enterprise governance and compliance. It respects existing permissions within connected apps — ChatGPT can only access what a user is already authorized to view— and never trains on company data by default.Security features include industry-standard encryption, support for SSO and SCIM for account provisioning, and IP allowlisting to restrict access to approved corporate networks. Enterprise administrators can also define role-based access control (RBAC) policies and manage permissions at a group or department level.OpenAI’s Enterprise Compliance API provides a full audit trail, allowing administrators to review conversation logs for reporting and regulatory purposes. This capability helps enterprises meet internal governance standards and industry-specific requirements such as SOC 2 and ISO 27001 compliance.Admin Configuration and Connector ManagementFor enterprise deployment, administrators must enable company knowledge and its connectors within the ChatGPT workspace. Once connectors are active, users can authenticate their own accounts for each work app they need to access.In Enterprise and Edu plans, connectors are off by default and require explicit admin approval before employees can use them. Admins can selectively enable connectors, manage access by role, and require SSO-based authentication for enhanced control.Business plan users, by contrast, have connectors enabled automatically if available in their workspace. Admins can still oversee which connectors are approved, ensuring alignment with internal IT and data policies.Company knowledge becomes available to any user with at least one active connector, and admins can configure group-level permissions for different teams — such as restricting GitHub access to engineering while enabling Google Drive or HubSpot for marketing and sales.Organizations who turn on the feature can also elect to turn it off just as easily. Once you disconnect a connector, ChatGPT does not have access to that data.How Company Knowledge Works in PracticeActivating company knowledge is straightforward. Users can start a new or existing conversation in ChatGPT and select “Company knowledge” under the message composer or from the tools menu. It must be turned on proactively for each new conversation or chat session, even from the same user.After authenticating their connected apps, they can ask questions as usual—such as “Summarize this account’s latest feedback and risks” or “Compile a Q4 performance summary from project trackers.”ChatGPT searches across the connected tools, retrieves relevant context, and produces an answer with full citations and source links. The system can combine data across apps — for instance, blending Slack updates, Google Docs notes, and HubSpot CRM records — to create an integrated view of a project, client, or initiative.When company knowledge is not selected, ChatGPT may still use connectors in a limited capacity as part of the default experience, but responses will not include detailed citations or multi-source synthesis.Advanced Use Cases for Enterprise TeamsFor development and operations leaders, company knowledge can act as a centralized intelligence layer that surfaces real-time updates and dependencies across complex workflows. ChatGPT can, for example, summarize open GitHub pull requests, highlight unresolved Linear tickets, and cross-reference Slack engineering discussions—all in a single output.Technical teams can also use it for incident retrospectives or release planning by pulling relevant information from issue trackers, logs, and meeting notes. Procurement or finance leaders can use it to consolidate purchase requests or budget updates across shared drives and internal communications.Because the model can reference structured and unstructured data simultaneously, it supports wide-ranging scenarios—from compliance documentation reviews to cross-departmental performance summaries.Privacy, Data Residency, and ComplianceEnterprise data protection is a central design element of company knowledge. ChatGPT processes data in line with OpenAI’s enterprise-grade security model, ensuring that no connected app data leaves the secure boundary of the organization’s authorized environment.Data residency policies vary by connector. Certain integrations, such as Slack, support region-specific data storage, while others—like Google Drive and SharePoint—are available for U.S.-based customers with or without at-rest data residency. Organizations with regional compliance obligations can review connector-specific security documentation for details.No geo restrictions apply to company knowledge, making it suitable for multinational organizations operating across multiple jurisdictions.Limitations and Future EnhancementsAt present, users must manually enable company knowledge in each new ChatGPT conversation. OpenAI is developing a unified interface that will automatically integrate company knowledge with other ChatGPT tools—such as browsing and chart generation—so that users won’t need to toggle between modes.When enabled, company knowledge temporarily disables web browsing and visual output generation, though users can switch modes within the same conversation to re-enable those features.OpenAI also continues to expand the network of supported tools. Recent updates have added connectors for Asana, GitLab Issues, and ClickUp, and OpenAI plans to support future MCP (Model Context Protocol) connectors to enable custom, developer-built integrations.Availability and Getting StartedCompany knowledge is now available to all ChatGPT Business, Enterprise, and Edu users. Organizations can begin by enabling the feature under the ChatGPT message composer and connecting approved work apps.For enterprise rollouts, OpenAI recommends a phased deployment: first enabling core connectors (such as Google Drive and Slack), configuring RBAC and SSO, then expanding to specialized systems once data access policies are verified.Procurement and security leaders evaluating the feature should note that company knowledge is covered under existing ChatGPT Enterprise terms and uses the same encryption, compliance, and service-level guarantees.With company knowledge, OpenAI aims to make ChatGPT not just a conversational assistant but an intelligent interface to enterprise data—delivering secure, context-aware insights that help technical and business leaders act with confidence.
- [Microsoft Copilot gets 12 big updates for fall, including new AI assistant character Mico](https://venturebeat.com/ai/microsoft-copilot-gets-12-big-updates-for-fall-including-new-ai-assistant) — 03:01 · VentureBeat AI
  > Microsoft today held a live announcement event online for its Copilot AI digital assistant, with Mustafa Suleyman, CEO of Microsoft's AI division, and other presenters unveiling a new generation of features that deepen integration across Windows, Edge, and Microsoft 365, positioning the platform as a practical assistant for people during work and off-time, while allowing them  to preserve control and safety of their data.The new Copilot 2025 Fall Update features also up the ante in terms of capabilities and the accessibility of generative AI assistance from Microsoft to users, so businesses relying on Microsoft products, and those who seek to offer complimentary or competing products, would do well to review them.Suleyman emphasized that the updates reflect a shift from hype to usefulness. “Technology should work in service of people, not the other way around,” he said. “Copilot is not just a product—it’s a promise that AI can be helpful, supportive, and deeply personal.”Intriguingly, the announcement also sought to shine a greater spotlight on Microsoft's own homegrown AI models, as opposed to those of its partner and investment OpenAI, which previously powered the entire Copilot experience. Instead, Suleyman wrote today in a blog post: “At the foundation of it all is our strategy to put the best models to work for you – both those we build and those we don’t. Over the past few months, we have released in-house models like MAI-Voice-1, MAI-1-Preview and MAI-Vision-1, and are rapidly iterating.”12 Features That Redefine CopilotThe Fall Release consolidates Copilot’s identity around twelve key capabilities—each with potential to streamline organizational knowledge work, development, or support operations.Groups – Shared Copilot sessions where up to 32 participants can brainstorm, co-author, or plan simultaneously. For distributed teams, it effectively merges a meeting chat, task board, and generative workspace. Copilot maintains context, summarizes decisions, and tracks open actions. Imagine – A collaborative hub for creating and remixing AI-generated content. In an enterprise setting, Imagine enables rapid prototyping of visuals, marketing drafts, or training materials.Mico – A new character identity for Copilot that introduces expressive feedback and emotional expression in the form of a cute, amorphous blob. Echoing Microsoft’s historic character interfaces like Clippy (Office 97) or Cortana (2014), Mico serves as a unifying UX layer across modalities.Real Talk – A conversational mode that adapts to a user’s communication style and offers calibrated pushback — ending the sycophancy that some users have complained about with other AI models such as prior versions of OpenAI's ChatGPT. For professionals, it allows Socratic problem-solving rather than passive answer generation, making Copilot more credible in technical collaboration.Memory & Personalization – Long-term contextual memory that lets Copilot recall key details—training plans, dates, goals—at the user’s direction.Connectors – Integration with OneDrive, Outlook, Gmail, Google Drive, and Google Calendar for natural-language search across accounts.Proactive Actions (Preview) – Context-based prompts and next-step suggestions derived from recent activity.Copilot for Health – Health information grounded in credible medical sources such as Harvard Health, with tools allowing users to locate and compare doctors.Learn Live – A Socratic, voice-driven tutoring experience using questions, visuals, and whiteboards.Copilot Mode in Edge – Converts Microsoft Edge into an “AI browser” that summarizes, compares, and executes web actions by voice.Copilot on Windows – Deep integration across Windows 11 PCs with “Hey Copilot” activation, Copilot Vision guidance, and quick access to files and apps.Copilot Pages and Copilot Search – A collaborative file canvas plus a unified search experience combining AI-generated, cited answers with standard web results.The Fall Release is immediately available in the United States, with rollout to the UK, Canada, and other markets in progress. Some functions—such as Groups, Journeys, and Copilot for Health—remain U.S.-only for now. Proactive Actions requires a Microsoft 365 Personal, Family, or Premium subscription.Together these updates illustrate Microsoft’s pivot from static productivity suites to contextual AI infrastructure, with the Copilot brand acting as the connective tissue across user roles.From Clippy to Mico: The Return of a Guided InterfaceOne of the most notable introductions is Mico, a small animated companion that is available within Copilot’s voice-enabled experiences, including the Copilot app on Windows, iOS, and Android, as well as in Study Mode and other conversational contexts. It serves as an optional visual companion that appears during interactive or voice-based sessions, rather than across all Copilot interfaces.Mico listens, reacts with expressions, and changes color to reflect tone and emotion — bringing a visual warmth to an AI assistant experience that has traditionally been text-heavy.Mico’s design recalls earlier eras of Microsoft’s history with character-based assistants. In the mid-1990s, Microsoft experimented with Microsoft Bob (1995), a software interface that used cartoon characters like a dog named Rover to guide users through everyday computing tasks. While innovative for its time, Bob was discontinued after a year due to performance and usability issues.A few years later came Clippy, the Office Assistant introduced in Microsoft Office 97. Officially known as “Clippit,” the animated paperclip would pop up to offer help and tips within Word and other Office applications. Clippy became widely recognized—sometimes humorously so—for interrupting users with unsolicited advice. Microsoft retired Clippy from Office in 2001, though the character remains a nostalgic symbol of early AI-driven assistance.More recently, Cortana, launched in 2014 as Microsoft’s digital voice assistant for Windows and mobile devices, aimed to provide natural-language interaction similar to Apple’s Siri or Amazon’s Alexa. Despite positive early reception, Cortana’s role diminished as Microsoft refocused on enterprise productivity and AI integration. The service was officially discontinued on Windows in 2023.Mico, by contrast, represents a modern reimagining of that tradition—combining the personality of early assistants with the intelligence and adaptability of contemporary AI models. Where Clippy offered canned responses, Mico listens, learns, and reflects a user’s mood in real time. The goal, as Suleyman framed it, is to create an AI that feels “helpful, supportive, and deeply personal.”Groups Are Microsoft's Version of Claude and ChatGPT ProjectsDuring Microsoft’s launch video, product researcher Wendy described Groups as a transformative shift: “You can finally bring in other people directly to the conversation that you’re having with Copilot,” she said. “It’s the only place you can do this.”Up to 32 users can join a shared Copilot session, brainstorming, editing, or planning together while the AI manages logistics such as summarizing discussion threads, tallying votes, and splitting tasks. Participants can enter or exit sessions using a link, maintaining full visibility into ongoing work.Instead of a single user prompting an AI and later sharing results, Groups lets teams prompt and iterate together in one unified conversation. In some ways, it's an answer to Anthropic’s Claude Projects and OpenAI’s ChatGPT Projects, both launched within the last year as tools to centralize team workspaces and shared AI context. Where Claude and ChatGPT Projects allow users to aggregate files, prompts, and conversations into a single container, Groups extends that model into real-time, multi-participant collaboration. Unlike Anthropic’s and OpenAI’s implementations, Groups is deeply embedded within Microsoft’s productivity environment.  Like other Copilot experiences connected to Outlook and OneDrive, Groups operates within Microsoft’s enterprise identity framework, governed by Microsoft 365 and Entra ID (formerly Azure Active Directory) authentication and consent modelsThis means conversations, shared artifacts, and generated summaries are governed under the same compliance policies that already protect Outlook, Teams, and SharePoint data.Hours after the unveiling, OpenAI hit back against its own investor in the escalating AI competition between the "frenemies" by expanding its Shared Projects feature beyond its current Enterprise, Team, and Edu subscriber availability to users of its free, Plus, and Pro subscription tiers. Operational Impact for AI and Data TeamsMemory & Personalization and Connectors effectively extend a lightweight orchestration layer across Microsoft’s ecosystem. Instead of building separate context-stores or retrieval APIs, teams can leverage Copilot’s secure integration with OneDrive or SharePoint as a governed data backbone. A presenter explained that Copilot’s memory “naturally picks up on important details and remembers them long after you’ve had the conversation,” yet remains editable. For data engineers, Copilot Search and Connectors reduce friction in data discovery across multiple systems. Natural-language retrieval from internal and cloud repositories may lower the cost of knowledge management initiatives by consolidating search endpoints.For security directors, Copilot’s explicit consent requirements and on/off toggles in Edge and Windows help maintain data residency standards. The company reiterated during the livestream that Copilot “acts only with user permission and within organizational privacy controls.”Copilot Mode in Edge: The AI Browser for Research and AutomationCopilot Mode in Edge stands out for offering AI-assisted information workflows. The browser can now parse open tabs, summarize differences, and perform transactional steps.“Historically, browsers have been static—just endless clicking and tab-hopping,” said a presenter during Microsoft’s livestream. “We asked not how browsers should work, but how people work.”In practice, an analyst could prompt Edge to compare supplier documentation, extract structured data, and auto-fill procurement forms—all with consistent citation. Voice-only navigation enables accessibility and multitasking, while Journeys, a companion feature, organizes browsing sessions into storylines for later review.Copilot on Windows: The Operating System as an AI SurfaceIn Windows 11, Copilot now functions as an embedded assistant. With the wake-word “Hey Copilot,” users can initiate context-aware commands without leaving the desktop—drafting documentation, troubleshooting configuration issues, or summarizing system logs.A presenter described it as a “super assistant plugged into all your files and applications.” For enterprises standardizing on Windows 11, this positions Copilot as a native productivity layer rather than an add-on, reducing training friction and promoting secure, on-device reasoning.Copilot Vision, now in early deployment, adds visual comprehension. IT staff can capture a screen region and ask Copilot to interpret error messages, explain configuration options, or generate support tickets automatically.Combined with Copilot Pages, which supports up to twenty concurrent file uploads, this enables more efficient cross-document analysis for audits, RFPs, or code reviews.Leveraging MAI Models for Multimodal WorkflowsAt the foundation of these capabilities are Microsoft’s proprietary MAI-Voice-1, MAI-1 Preview, and MAI-Vision-1 models—trained in-house to handle text, voice, and visual inputs cohesively.For engineering teams managing LLM orchestration, this architecture introduces several potential efficiencies:Unified multimodal reasoning – Reduces the need for separate ASR (speech-to-text) and image-parsing services.Fine-tuning continuity – Because Microsoft owns the model stack, updates propagate across Copilot experiences without re-integration.Predictable latency and governance – In-house hosting under Azure compliance frameworks simplifies security certification for regulated industries.A presenter described the new stack as “the foundation for immersive, creative, and dynamic experiences that still respect enterprise boundaries.”A Strategic Pivot Toward Contextual AIFor years, Microsoft positioned Copilot primarily as a productivity companion. With the Fall 2025 release, it crosses into operational AI infrastructure—a set of extensible services for reasoning over data and processes.Suleyman described this evolution succinctly: “Judge an AI by how much it elevates human potential, not just by its own smarts.” For CIOs and technical leads, the elevation comes from efficiency and interoperability.Copilot now acts as:A connective interface linking files, communications, and cloud data.A reasoning agent capable of understanding context across sessions and modalities.A secure orchestration layer compatible with Microsoft’s compliance and identity framework.Suleyman’s insistence that “technology should work in service of people” now extends to organizations as well: technology that serves teams, not workloads; systems that adapt to enterprise context rather than demand it.
