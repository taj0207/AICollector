# AI News for 2025-10-22 (Asia/Taipei)

Collected 10 article(s).

- [Kai-Fu Lee's brutal assessment: America is already losing the AI hardware war to China](https://venturebeat.com/ai/kai-fu-lees-brutal-assessment-america-is-already-losing-the-ai-hardware-war) — 20:30 · VentureBeat AI
  > China is on track to dominate consumer artificial intelligence applications and robotics manufacturing within years, but the United States will maintain its substantial lead in enterprise AI adoption and cutting-edge research, according to Kai-Fu Lee, one of the world's most prominent AI scientists and investors.In a rare, unvarnished assessment delivered via video link from Beijing to the TED AI conference in San Francisco Tuesday, Lee — a former executive at Apple, Microsoft, and Google who now runs both a major venture capital firm and his own AI company — laid out a technology landscape splitting along geographic and economic lines, with profound implications for both commercial competition and national security."China's robotics has the advantage of having integrated AI into much lower costs, better supply chain and fast turnaround, so companies like Unitree are actually the farthest ahead in the world in terms of building affordable, embodied humanoid AI," Lee said, referring to a Chinese robotics manufacturer that has undercut Western competitors on price while advancing capabilities.The comments, made to a room filled with Silicon Valley executives, investors, and researchers, represented one of the most detailed public assessments from Lee about the comparative strengths and weaknesses of the world's two AI superpowers — and suggested that the race for artificial intelligence leadership is becoming less a single contest than a series of parallel competitions with different winners.Why venture capital is flowing in opposite directions in the U.S. and ChinaAt the heart of Lee's analysis lies a fundamental difference in how capital flows in the two countries' innovation ecosystems. American venture capitalists, Lee said, are pouring money into generative AI companies building large language models and enterprise software, while Chinese investors are betting heavily on robotics and hardware."The VCs in the US don't fund robotics the way the VCs do in China," Lee said. "Just like the VCs in China don't fund generative AI the way the VCs do in the US."This investment divergence reflects different economic incentives and market structures. In the United States, where companies have grown accustomed to paying for software subscriptions and where labor costs are high, enterprise AI tools that boost white-collar productivity command premium prices. In China, where software subscription models have historically struggled to gain traction but manufacturing dominates the economy, robotics offers a clearer path to commercialization.The result, Lee suggested, is that each country is pulling ahead in different domains — and may continue to do so."China's got some challenges to overcome in getting a company funded as well as OpenAI or Anthropic," Lee acknowledged, referring to the leading American AI labs. "But I think U.S., on the flip side, will have trouble developing the investment interest and value creation in the robotics" sector.Why American companies dominate enterprise AI while Chinese firms struggle with subscriptionsLee was explicit about one area where the United States maintains what appears to be a durable advantage: getting businesses to actually adopt and pay for AI software."The enterprise adoption will clearly be led by the United States," Lee said. "The Chinese companies have not yet developed a habit of paying for software on a subscription."This seemingly mundane difference in business culture — whether companies will pay monthly fees for software — has become a critical factor in the AI race. The explosion of spending on tools like GitHub Copilot, ChatGPT Enterprise, and other AI-powered productivity software has fueled American companies' ability to invest billions in further research and development.Lee noted that China has historically overcome similar challenges in consumer technology by developing alternative business models. "In the early days of internet software, China was also well behind because people weren't willing to pay for software," he said. "But then advertising models, e-commerce models really propelled China forward."Still, he suggested, someone will need to "find a new business model that isn't just pay per software per use or per month basis. That's going to not happen in China anytime soon."The implication: American companies building enterprise AI tools have a window — perhaps a substantial one — where they can generate revenue and reinvest in R&D without facing serious Chinese competition in their core market.How ByteDance, Alibaba and Tencent will outpace Meta and Google in consumer AIWhere Lee sees China pulling ahead decisively is in consumer-facing AI applications — the kind embedded in social media, e-commerce, and entertainment platforms that billions of people use daily."In terms of consumer usage, that's likely to happen," Lee said, referring to China matching or surpassing the United States in AI deployment. "The Chinese giants, like ByteDance and Alibaba and Tencent, will definitely move a lot faster than their equivalent in the United States, companies like Meta, YouTube and so on."Lee pointed to a cultural advantage: Chinese technology companies have spent the past decade obsessively optimizing for user engagement and product-market fit in brutally competitive markets. "The Chinese giants really work tenaciously, and they have mastered the art of figuring out product market fit," he said. "Now they have to add technology to it. So that is inevitably going to happen."This assessment aligns with recent industry observations. ByteDance's TikTok became the world's most downloaded app through sophisticated AI-driven content recommendation, and Chinese companies have pioneered AI-powered features in areas like live-streaming commerce and short-form video that Western companies later copied.Lee also noted that China has already deployed AI more widely in certain domains. "There are a lot of areas where China has also done a great job, such as using computer vision, speech recognition, and translation more widely," he said.The surprising open-source shift that has Chinese models beating Meta's LlamaPerhaps Lee's most striking data point concerned open-source AI development — an area where China appears to have seized leadership from American companies in a remarkably short time."The 10 highest rated open source [models] are from China," Lee said. "These companies have now eclipsed Meta's Llama, which used to be number one."This represents a significant shift. Meta's Llama models were widely viewed as the gold standard for open-source large language models as recently as early 2024. But Chinese companies — including Lee's own firm, 01.AI, along with Alibaba, Baidu, and others — have released a flood of open-source models that, according to various benchmarks, now outperform their American counterparts.The open-source question has become a flashpoint in AI development. Lee made an extensive case for why open-source models will prove essential to the technology's future, even as closed models from companies like OpenAI command higher prices and, often, superior performance."I think open source has a number of major advantages," Lee argued. With open-source models, "you can examine it, tune it, improve it. It's yours, and it's free, and it's important for building if you want to build an application or tune the model to do something specific."He drew an analogy to operating systems: "People who work in operating systems loved Linux, and that's why its adoption went through the roof. And I think in the future, open source will also allow people to tune a sovereign model for a country, make it work better for a particular language."Still, Lee predicted both approaches will coexist. "I don't think open source models will win," he said. "I think just like we have Apple, which is closed, but provides a somewhat better experience than Android... I think we're going to see more apps using open-source models, more engineers wanting to build open-source models, but I think more money will remain in the closed model."Why China's manufacturing advantage makes the robotics race 'not over, but' nearly decidedOn robotics, Lee's message was blunt: the combination of China's manufacturing prowess, lower costs, and aggressive investment has created an advantage that will be difficult for American companies to overcome.When asked directly whether the robotics race was already over with China victorious, Lee hedged only slightly. "It's not over, but I think the U.S. is still capable of coming up with the best robotic research ideas," he said. "But the VCs in the U.S. don't fund robotics the way the VCs do in China."The challenge is structural. Building robots requires not just software and AI, but hardware manufacturing at scale — precisely the kind of integrated supply chain and low-cost production that China has spent decades perfecting. While American labs at universities and companies like Boston Dynamics continue to produce impressive research prototypes, turning those prototypes into affordable commercial products requires the manufacturing ecosystem that China possesses.Companies like Unitree have demonstrated this advantage concretely. The company's humanoid robots and quadrupedal robots cost a fraction of their American-made equivalents while offering comparable or superior capabilities — a price-to-performance ratio that could prove decisive in commercial markets.What worries Lee most: not AGI, but the race itselfDespite his generally measured tone about China's AI development, Lee expressed concern about one area where he believes the global AI community faces real danger — not the far-future risk of superintelligent AI, but the near-term consequences of moving too fast.When asked about AGI risks, Lee reframed the question. "I'm less afraid of AI becoming self-aware and causing danger for humans in the short term," he said, "but more worried about it being used by bad people to do terrible things, or by the AI race pushing people to work so hard, so fast and furious and move fast and break things that they build products that have problems and holes to be exploited."He continued: "I'm very worried about that. In fact, I think some terrible event will happen that will be a wake up call from this sort of problem."Lee's perspective carries unusual weight because of his unique vantage point spanning both Chinese and American AI development. Over a career spanning more than three decades, he has held senior positions at Apple, Microsoft, and Google, while also founding Sinovation Ventures, which has invested in more than 400 companies across both countries. His AI company, 01.AI, founded in 2023, has released several open-source models that rank among the most capable in the world.For American companies and policymakers, Lee's analysis presents a complex strategic picture. The United States appears to have clear advantages in enterprise AI software, fundamental research, and computing infrastructure. But China is moving faster in consumer applications, manufacturing robotics at lower costs, and potentially pulling ahead in open-source model development.The bifurcation suggests that rather than a single "winner" in AI, the world may be heading toward a technology landscape where different countries excel in different domains — with all the economic and geopolitical complications that implies.As the TED AI conference continued Wednesday, Lee's assessment hung over subsequent discussions. His message seemed clear: the AI race is not one contest, but many — and the United States and China are each winning different races.Standing in the conference hall afterward, one venture capitalist, who asked not to be named, summed up the mood in the room: "We're not competing with China anymore. We're competing on parallel tracks." Whether those tracks eventually converge — or diverge into entirely separate technology ecosystems — may be the defining question of the next decade.
- [Introducing: the body issue](https://www.technologyreview.com/2025/10/22/1126376/introducing-the-body-issue/) — 20:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. Introducing: the body issue We’re thrilled to share the latest edition of MIT Technology Review magazine, digging into the future of the human body, and how it could change in the years ahead…
- [Dispatch: Partying at one of Africa’s largest AI gatherings](https://www.technologyreview.com/2025/10/22/1125361/africa-ai-policymaking-nyalleng-moorosi/) — 18:00 · MIT Technology Review (AI)
  > It’s late August in Rwanda’s capital, Kigali, and people are filling a large hall at one of Africa’s biggest gatherings of minds in AI and machine learning. The room is draped in white curtains, and a giant screen blinks with videos created with generative AI. A classic East African folk song by the Tanzanian singer…
- [Job titles of the future: AI embryologist](https://www.technologyreview.com/2025/10/22/1125367/ai-embryologist-reproduction-future-jobs/) — 18:00 · MIT Technology Review (AI)
  > Embryologists are the scientists behind the scenes of in vitro fertilization who oversee the development and selection of embryos, prepare them for transfer, and maintain the lab environment. They’ve been a critical part of IVF for decades, but their job has gotten a whole lot busier in recent years as demand for the fertility treatment…
- [Inside the archives of the NASA Ames Research Center](https://www.technologyreview.com/2025/10/22/1125378/nasa-ames-research-center-archives/) — 18:00 · MIT Technology Review (AI)
  > At the southern tip of San Francisco Bay, surrounded by the tech giants Google, Apple, and Microsoft, sits the historic NASA Ames Research Center. Its rich history includes a grab bag of fascinating scientific research involving massive wind tunnels, experimental aircraft, supercomputing, astrobiology, and more. Founded in 1939 as a West Coast lab for the…
- [Simplifying the AI stack: The key to scalable, portable intelligence from cloud to edge](https://venturebeat.com/ai/simplifying-the-ai-stack-the-key-to-scalable-portable-intelligence-from) — 12:00 · VentureBeat AI
  > Presented by ArmA simpler software stack is the key to portable, scalable AI across cloud and edge. AI is now powering real-world applications, yet fragmented software stacks are holding it back. Developers routinely rebuild the same models for different hardware targets, losing time to glue code instead of shipping features. The good news is that a shift is underway. Unified toolchains and optimized libraries are making it possible to deploy models across platforms without compromising performance.Yet one critical hurdle remains: software complexity. Disparate tools, hardware-specific optimizations, and layered tech stacks continue to bottleneck progress. To unlock the next wave of AI innovation, the industry must pivot decisively away from siloed development and toward streamlined, end-to-end platforms.This transformation is already taking shape. Major cloud providers, edge platform vendors, and open-source communities are converging on unified toolchains that simplify development and accelerate deployment, from cloud to edge. In this article, we’ll explore why simplification is the key to scalable AI, what’s driving this momentum, and how next-gen platforms are turning that vision into real-world results.The bottleneck: fragmentation, complexity, and inefficiencyThe issue isn’t just hardware variety; it’s duplicated effort across frameworks and targets that slows time-to-value.Diverse hardware targets: GPUs, NPUs, CPU-only devices, mobile SoCs, and custom accelerators.Tooling and framework fragmentation: TensorFlow, PyTorch, ONNX, MediaPipe, and others.Edge constraints: Devices require real-time, energy-efficient performance with minimal overhead.According to Gartner Research, these mismatches create a key hurdle: over 60% of AI initiatives stall before production, driven by integration complexity and performance variability. What software simplification looks likeSimplification is coalescing around five moves that cut re-engineering cost and risk:Cross-platform abstraction layers that minimize re-engineering when porting models.Performance-tuned libraries integrated into major ML frameworks.Unified architectural designs that scale from datacenter to mobile.Open standards and runtimes (e.g., ONNX, MLIR) reducing lock-in and improving compatibility.Developer-first ecosystems emphasizing speed, reproducibility, and scalability.These shifts are making AI more accessible, especially for startups and academic teams that previously lacked the resources for bespoke optimization. Projects like Hugging Face’s Optimum and MLPerf benchmarks are also helping standardize and validate cross-hardware performance.Ecosystem momentum and real-world signals Simplification is no longer aspirational; it’s happening now. Across the industry, software considerations are influencing decisions at the IP and silicon design level, resulting in solutions that are production-ready from day one. Major ecosystem players are driving this shift by aligning hardware and software development efforts, delivering tighter integration across the stack.A key catalyst is the rapid rise of edge inference, where AI models are deployed directly on devices rather than in the cloud. This has intensified demand for streamlined software stacks that support end-to-end optimization, from silicon to system to application. Companies like Arm are responding by enabling tighter coupling between their compute platforms and software toolchains, helping developers accelerate time-to-deployment without sacrificing performance or portability. The emergence of multi-modal and general-purpose foundation models (e.g., LLaMA, Gemini, Claude) has also added urgency. These models require flexible runtimes that can scale across cloud and edge environments. AI agents, which interact, adapt, and perform tasks autonomously, further drive the need for high-efficiency, cross-platform software.MLPerf Inference v3.1 included over 13,500 performance results from 26 submitters, validating multi-platform benchmarking of AI workloads. Results spanned both data center and edge devices, demonstrating the diversity of optimized deployments now being tested and shared.Taken together, these signals make clear that the market’s demand and incentives are aligning around a common set of priorities, including maximizing performance-per-watt, ensuring portability, minimizing latency, and delivering security and consistency at scale.What must happen for successful simplificationTo realize the promise of simplified AI platforms, several things must occur:Strong hardware/software co-design: hardware features that are exposed in software frameworks (e.g., matrix multipliers, accelerator instructions), and conversely, software that is designed to take advantage of underlying hardware.Consistent, robust toolchains and libraries: developers need reliable, well-documented libraries that work across devices. Performance portability is only useful if the tools are stable and well supported.Open ecosystem: hardware vendors, software framework maintainers, and model developers need to cooperate. Standards and shared projects help avoid re-inventing the wheel for every new device or use case.Abstractions that don’t obscure performance: while high-level abstraction helps developers, they must still allow tuning or visibility where needed. The right balance between abstraction and control is key.Security, privacy, and trust built in: especially as more compute shifts to devices (edge/mobile), issues like data protection, safe execution, model integrity, and privacy matter.Arm as one example of ecosystem-led simplification Simplifying AI at scale now hinges on system-wide design, where silicon, software, and developer tools evolve in lockstep. This approach enables AI workloads to run efficiently across diverse environments, from cloud inference clusters to battery-constrained edge devices. It also reduces the overhead of bespoke optimization, making it easier to bring new products to market faster. Arm (Nasdaq:Arm) is advancing this model with a platform-centric focus that pushes hardware-software optimizations up through the software stack. At COMPUTEX 2025, Arm demonstrated how its latest Arm9 CPUs, combined with AI-specific ISA extensions and the Kleidi libraries, enable tighter integration with widely used frameworks like PyTorch, ExecuTorch, ONNX Runtime, and MediaPipe. This alignment reduces the need for custom kernels or hand-tuned operators, allowing developers to unlock hardware performance without abandoning familiar toolchains. The real-world implications are significant. In the data center, Arm-based platforms are delivering improved performance-per-watt, critical for scaling AI workloads sustainably. On consumer devices, these optimizations enable ultra-responsive user experiences and background intelligence that’s always on, yet power efficient.More broadly, the industry is coalescing around simplification as a design imperative, embedding AI support directly into hardware roadmaps, optimizing for software portability, and standardizing support for mainstream AI runtimes. Arm’s approach illustrates how deep integration across the compute stack can make scalable AI a practical reality.Market validation and momentumIn 2025, nearly half of the compute shipped to major hyperscalers will run on Arm-based architectures, a milestone that underscores a significant shift in cloud infrastructure. As AI workloads become more resource-intensive, cloud providers are prioritizing architectures that deliver superior performance-per-watt and support seamless software portability. This evolution marks a strategic pivot toward energy-efficient, scalable infrastructure optimized for the performance and demands of modern AI.At the edge, Arm-compatible inference engines are enabling real-time experiences, such as live translation and always-on voice assistants, on battery-powered devices. These advancements bring powerful AI capabilities directly to users, without sacrificing energy efficiency.Developer momentum is accelerating as well. In a recent collaboration, GitHub and Arm introduced native Arm Linux and Windows runners for GitHub Actions, streamlining CI workflows for Arm-based platforms. These tools lower the barrier to entry for developers and enable more efficient, cross-platform development at scale. What comes nextSimplification doesn’t mean removing complexity entirely; it means managing it in ways that empower innovation. As the AI stack stabilizes, winners will be those who deliver seamless performance across a fragmented landscape.From a future-facing perspective, expect:Benchmarks as guardrails: MLPerf + OSS suites guide where to optimize next.More upstream, fewer forks: Hardware features land in mainstream tools, not custom branches.Convergence of research + production: Faster handoff from papers to product via shared runtimes.ConclusionAI’s next phase isn’t about exotic hardware; it’s also about software that travels well. When the same model lands efficiently on cloud, client, and edge, teams ship faster and spend less time rebuilding the stack.Ecosystem-wide simplification, not brand-led slogans, will separate the winners. The practical playbook is clear: unify platforms, upstream optimizations, and measure with open benchmarks. Explore how Arm AI software platforms are enabling this future — efficiently, securely, and at scale.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
- [Infinite folds](https://www.technologyreview.com/2025/10/21/1124723/infinite-folds/) — 05:00 · MIT Technology Review (AI)
  > When Madonna Yoder ’17 was eight years old, she learned how to fold a square piece of paper over and over and over again. After about 16 folds, she held a bird in her hands. The first time she pulled the tail of a flapping crane, she says, she realized: Oh, I folded this, and…
- [Qwen's new Deep Research update lets you turn its reports into webpages, podcasts in seconds](https://venturebeat.com/ai/qwens-new-deep-research-update-lets-you-turn-its-reports-into-webpages) — 02:32 · VentureBeat AI
  > Chinese e-commerce giant Alibaba’s famously prolific Qwen Team of AI model researchers and engineers has introduced a major expansion to its Qwen Deep Research tool, which is available as an optional modality the user can activate on the web-based Qwen Chat (a competitor to ChatGPT).The update lets users generate not only comprehensive research reports with well-organized citations, but also interactive web pages and multi-speaker podcasts — all within 1-2 clicks.This functionality is part of a proprietary release, distinct from many of Qwen’s previous open-source model offerings. While the feature relies on the open-source models Qwen3-Coder, Qwen-Image, and Qwen3-TTS to power its core capabilities, the end-to-end experience — including research execution, web deployment, and audio generation — is hosted and operated by Qwen. This means users benefit from a managed, integrated workflow without needing to configure infrastructure. That said, developers with access to the open-source models could theoretically replicate similar functionality on private or commercial systems.The update was announced via the team’s official X account (@Alibaba_Qwen) today, October 21, 2025, stating:“Qwen Deep Research just got a major upgrade. It now creates not only the report, but also a live webpage and a podcast — powered by Qwen3-Coder, Qwen-Image, and Qwen3-TTS. Your insights, now visual and audible.”Multi-Format Research OutputThe core workflow begins with a user request inside the Qwen Chat interface. From there, Qwen collaborates by asking clarifying questions to shape the research scope, pulls data from the web and official sources, and analyzes or resolves any inconsistencies it finds — even generating custom code when needed.A demo video posted by Qwen on X walks through this process on Qwen Chat using the U.S. SaaS market as an example. In it, Qwen retrieves data from multiple industry sources, identifies discrepancies in market size estimates (e.g., $206 billion vs. $253 billion), and highlights ambiguities in the U.S. share of global figures. The assistant comments on differences in scope between sources and calculates a compound annual growth rate (CAGR) of 19.8% from 2020 to 2023, providing contextual analysis to back up the raw numbers.Once the research is complete, users can click on the "eyeball" icon below the output result (see screenshot), which will bring up a PDF-style report in the right hand pane.Then, when viewing the report in the right-hand pane, the user can click the "Create" button in the upper-right hand corner and select from the following two options:"Web Dev" which produces a live, professional-grade web page, automatically deployed and hosted by Qwen, using Qwen3-Coder for structure and Qwen-Image for visuals."Podcast," which, as it states, produces an audio podcast, featuring dynamic, multi-speaker narration generated by Qwen3-TTS, also hosted by Qwen for easy sharing and playback.This enables users to quickly convert a single research project into multiple forms of content — written, visual, and audible — with minimal extra input.The website includes inline graphics generated by Qwen Image, making it suitable for use in public presentations, classrooms, or publishing. The podcast feature allows users to select between 17 different speaker names as the host and 7 as the co-host, though I wasn't able to find a way to preview the voice outputs before selecting them. It appears designed for deep listening on the go. There was no way to change the language output that I could see, so mine came out in English, like my reports and initial prompts, though the Qwen LLMs are multi-modal. The voices were slightly more robotic than other AI tools I've used.Here's an example of a web page I generated on commonalities in authoritarian regimes throughout history, another one on UFO or UAP sightings, and below this paragraph, a podcast on UFO or UAP sightings. While the website is hosted via a public link, the podcast must be downloaded by the user and can't be linked to publicly, from what I could tell in my brief usage so far.Note the podcast is much different than the actual report — not just a straight read-through audio version of it, rather, a new format of two hosts discussing and bantering about the subject using the report as the jumping off point. The web page versions of the report also include new graphics not found in the PDF report.Comparisons to Google's NotebookLMWhile the new capabilities have been well received by many early users, comparisons to other research assistants have surfaced — particularly Google’s NotebookLM, which recently exited beta.AI commentator and newsletter writer Chubby (@kimmonismus) noted on X:“I am really grateful that Qwen provides regular updates. That’s great.But the attempt to build a NotebookLM clone inside Qwen-3-max doesn’t sound very promising compared to Google’s version.”While NotebookLM is built around organizing and querying existing documents and web pages, Qwen Deep Research focuses more on generating new research content from scratch, aggregating sources from the open web, and presenting it across multiple modalities. The comparison suggests that while the two tools overlap in general concept — AI-assisted research — they diverge in approach and target user experience.AvailabilityQwen Deep Research is now live and available through the Qwen Chat app. The feature can be accessed with the following URL.No pricing details have been provided for Qwen3-Max or the specific Deep Research capabilities as of this writing.What's Next For Qwen Deep Research?By combining research guidance, data analysis, and multi-format content creation into a single tool, Qwen Deep Research aims to streamline the path from idea to publishable output. The integration of code, visuals, and voice makes it especially attractive to content creators, educators, and independent analysts who want to scale their research into web- or podcast-friendly forms without switching platforms.Still, comparisons to more specialized offerings like NotebookLM raise questions about how Qwen’s generalized approach stacks up on depth, precision, and refinement. Whether the strength of its multi-format execution outweighs those concerns may come down to user priorities — and whether they value single-click publishing over tight integration with existing notes and materials.For now, Qwen is signaling that research doesn’t end with a document — it begins with one.Let me know if you want this repackaged into something shorter or tailored to a particular audience — newsletter, press-style blog, internal team explainer, etc.
- [DeepSeek drops open-source model that compresses text 10x through images, defying conventions](https://venturebeat.com/ai/deepseek-drops-open-source-model-that-compresses-text-10x-through-images) — 02:30 · VentureBeat AI
  > DeepSeek, the Chinese artificial intelligence research company that has repeatedly challenged assumptions about AI development costs, has released a new model that fundamentally reimagines how large language models process information—and the implications extend far beyond its modest branding as an optical character recognition tool.The company's DeepSeek-OCR model, released Monday with full open-source code and weights, achieves what researchers describe as a paradigm inversion: compressing text through visual representation up to 10 times more efficiently than traditional text tokens. The finding challenges a core assumption in AI development and could pave the way for language models with dramatically expanded context windows, potentially reaching tens of millions of tokens."We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping," the research team wrote in their technical paper. "Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio The implications have resonated across the AI research community. Andrej Karpathy, co-founder of OpenAI and former director of AI at Tesla, said in a post that the work raises fundamental questions about how AI systems should process information. "Maybe it makes more sense that all inputs to LLMs should only ever be images," Karpathy wrote. "Even if you happen to have pure text input, maybe you'd prefer to render it and then feed that in."How DeepSeek achieved 10x compression by treating text as imagesWhile DeepSeek marketed the release as an OCR model — a technology for converting images of text into digital characters — the research paper reveals more ambitious goals. The model demonstrates that visual representations can serve as a superior compression medium for textual information, inverting the conventional hierarchy where text tokens were considered more efficient than vision tokens."Traditionally, vision LLM tokens almost seemed like an afterthought or 'bolt on' to the LLM paradigm," wrote Jeffrey Emanuel, an AI researcher, in a detailed analysis of the paper. "And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens...But that gets inverted now from the ideas in this paper."The model's architecture consists of two primary components: DeepEncoder, a novel 380-million-parameter vision encoder, and a 3-billion-parameter mixture-of-experts language decoder with 570 million activated parameters. DeepEncoder combines Meta's Segment Anything Model (SAM) for local visual perception with OpenAI's CLIP model for global visual understanding, connected through a 16x compression module.To validate their compression claims, DeepSeek researchers tested the model on the Fox benchmark, a dataset of diverse document layouts. The results were striking: using just 100 vision tokens, the model achieved 97.3% accuracy on documents containing 700-800 text tokens — representing an effective compression ratio of 7.5x. Even at compression ratios approaching 20x, accuracy remained around 60%.The practical impact: Processing 200,000 pages per day on a single GPUThe efficiency gains translate directly to production capabilities. According to the company, a single Nvidia A100-40G GPU can process more than 200,000 pages per day using DeepSeek-OCR. Scaling to a cluster of 20 servers with eight GPUs each, throughput reaches 33 million pages daily — sufficient to rapidly construct training datasets for other AI models.On OmniDocBench, a comprehensive document parsing benchmark, DeepSeek-OCR outperformed GOT-OCR2.0 (which uses 256 tokens per page) while using only 100 vision tokens. More dramatically, it surpassed MinerU2.0 — which requires more than 6,000 tokens per page on average — while using fewer than 800 vision tokens.DeepSeek designed the model to support five distinct resolution modes, each optimized for different compression ratios and use cases. The "Tiny" mode operates at 512×512 resolution with just 64 vision tokens, while "Gundam" mode combines multiple resolutions dynamically for complex documents. "Gundam mode consists of n×640×640 tiles (local views) and a 1024×1024 global view," the researchers wrote.Why this breakthrough could unlock 10 million token context windowsThe compression breakthrough has immediate implications for one of the most pressing challenges in AI development: expanding the context windows that determine how much information language models can actively consider. Current state-of-the-art models typically handle context windows measured in hundreds of thousands of tokens. DeepSeek's approach suggests a path to windows ten times larger."The potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting," Emanuel wrote. "You could basically cram all of a company's key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective."The researchers explicitly frame their work in terms of context compression for language models. "Through DeepSeek-OCR, we demonstrate that vision-text compression can achieve significant token reduction (7-20×) for different historical context stages, offering a promising direction for addressing long-context challenges in large language models," they wrote.The paper includes a speculative but intriguing diagram illustrating how the approach could implement memory decay mechanisms similar to human cognition. Older conversation rounds could be progressively downsampled to lower resolutions, consuming fewer tokens while maintaining key information — a form of computational forgetting that mirrors biological memory.How visual processing could eliminate the 'ugly' tokenizer problemBeyond compression, Karpathy highlighted how the approach challenges fundamental assumptions about how language models should process text. Traditional tokenizers—the systems that break text into units for processing—have long been criticized for their complexity and limitations."I already ranted about how much I dislike the tokenizer," Karpathy wrote. "Tokenizers are ugly, separate, not end-to-end stage. It 'imports' all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network."Visual processing of text could eliminate these issues while enabling new capabilities. The approach naturally handles formatting information lost in pure text representations: bold text, colors, layout, embedded images. "Input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful," Karpathy noted.The implications resonate with human cognitive science. Emanuel drew a parallel to Hans Bethe, the renowned physicist who memorized vast amounts of reference data: "Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more."The model's training: 30 million PDF pages across 100 languagesThe model's capabilities rest on an extensive training regimen using diverse data sources. DeepSeek collected 30 million PDF pages covering approximately 100 languages, with Chinese and English accounting for 25 million pages. The training data spans nine document types — academic papers, financial reports, textbooks, newspapers, handwritten notes, and others.Beyond document OCR, the training incorporated what the researchers call "OCR 2.0" data: 10 million synthetic charts, 5 million chemical formulas, and 1 million geometric figures. The model also received 20% general vision data for tasks like image captioning and object detection, plus 10% text-only data to maintain language capabilities.The training process employed pipeline parallelism across 160 Nvidia A100-40G GPUs (20 nodes with 8 GPUs each), with the vision encoder divided between two pipeline stages and the language model split across two others. "For multimodal data, the training speed is 70B tokens/day," the researchers reported.Open source release accelerates research and raises competitive questionsTrue to DeepSeek's pattern of open development, the company released the complete model weights, training code, and inference scripts on GitHub and Hugging Face. The GitHub repository gained over 4,000 stars within 24 hours of release, according to Dataconomy.The breakthrough raises questions about whether other AI labs have developed similar techniques but kept them proprietary. Emanuel speculated that Google's Gemini models, which feature large context windows and strong OCR performance, might employ comparable approaches. "For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks," Emanuel wrote.Google's Gemini 2.5 Pro offers a 1-million-token context window, with plans to expand to 2 million, though the company has not publicly detailed the technical approaches enabling this capability. OpenAI's GPT-5 supports 400,000 tokens, while Anthropic's Claude 4.5 offers 200,000 tokens, with a 1-million-token window available in beta for eligible organizations.The unanswered question: Can AI reason over compressed visual tokens?While the compression results are impressive, researchers acknowledge important open questions. "It's not clear how exactly this interacts with the other downstream cognitive functioning of an LLM," Emanuel noted. "Can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality?"The DeepSeek paper focuses primarily on the compression-decompression capability, measured through OCR accuracy, rather than downstream reasoning performance. This leaves open whether language models could reason effectively over large contexts represented primarily as compressed visual tokens.The researchers acknowledge their work represents "an initial exploration into the boundaries of vision-text compression." They note that "OCR alone is insufficient to fully validate true context optical compression" and plan future work including "digital-optical text interleaved pretraining, needle-in-a-haystack testing, and other evaluations."DeepSeek has established a pattern of achieving competitive results with dramatically lower computational resources than Western AI labs. The company's earlier DeepSeek-V3 model reportedly cost just $5.6 million to train—though this figure represents only the final training run and excludes R&D and infrastructure costs—compared to hundreds of millions for comparable models from OpenAI and Anthropic.Industry analysts have questioned the $5.6 million figure, with some estimates placing the company's total infrastructure and operational costs closer to $1.3 billion, though still lower than American competitors' spending.The bigger picture: Should language models process text as images?DeepSeek-OCR poses a fundamental question for AI development: should language models process text as text, or as images of text? The research demonstrates that, at least for compression purposes, visual representation offers significant advantages. Whether this translates to effective reasoning over vast contexts remains to be determined."From another perspective, optical contexts compression still offers substantial room for research and improvement, representing a promising new direction," the researchers concluded in their paper.For the AI industry, the work adds another dimension to the race for longer context windows — a competition that has intensified as language models are applied to increasingly complex tasks requiring vast amounts of information. The open-source release ensures the technique will be widely explored, tested, and potentially integrated into future AI systems.As Karpathy framed the deeper implication: "OCR is just one of many useful vision -> text tasks. And text -> text tasks can be made to be vision ->text tasks. Not vice versa." In other words, the path forward for AI might not run through better tokenizers — it might bypass text tokens altogether.
- [Google's new vibe coding AI Studio experience lets anyone build, deploy apps live in minutes](https://venturebeat.com/ai/googles-new-vibe-coding-ai-studio-experience-lets-anyone-build-deploy-apps) — 01:45 · VentureBeat AI
  > Google AI Studio has gotten a big vibe coding upgrade with a new interface, buttons, suggestions and community features that allow anyone with an idea for an app — even complete novices, laypeople, or non-developers like yours truly — to bring it into existence and deploy it live, on the web, for anyone to use, within minutes.The updated Build tab is available now at ai.studio/build, and it’s free to start. Users can experiment with building applications without needing to enter payment information upfront, though certain advanced features like Veo 3.1 and Cloud Run deployment require a paid API key.The new features appear to me to make Google's AI models and offerings even more competitive, perhaps preferred, for many general users to dedicated AI startup rivals like Anthropic's Claude Code and OpenAI's Codex, respectively, two "vibe coding" focused products that are beloved by developers — but seem to have a higher barrier to entry or may require more technical know-how.A Fresh Start: Redesigned Build ModeThe updated Build tab serves as the entry point to vibe coding. It introduces a new layout and workflow where users can select from Google’s suite of AI models and features to power their applications. The default is Gemini 2.5 Pro, which is great for most cases.Once selections are made, users simply describe what they want to build, and the system automatically assembles the necessary components using Gemini’s APIs.This mode supports mixing capabilities like Nano Banana (a lightweight AI model), Veo (for video understanding), Imagine (for image generation), Flashlight (for performance-optimized inference), and Google Search.Patrick Löber, Developer Relations at Google DeepMind, highlighted that the experience is meant to help users “supercharge your apps with AI” using a simple prompt-to-app pipeline.In a video demo he posted on X and LinedIn, he showed how just a few clicks led to the automatic generation of a garden planning assistant app, complete with layouts, visuals, and a conversational interface.From Prompt to Production: Building and Editing in Real TimeOnce an app is generated, users land in a fully interactive editor. On the left, there’s a traditional code-assist interface where developers can chat with the AI model for help or suggestions. On the right, a code editor displays the full source of the app.Each component—such as React entry points, API calls, or styling files—can be edited directly. Tooltips help users understand what each file does, which is especially useful for those less familiar with TypeScript or frontend frameworks.Apps can be saved to GitHub, downloaded locally, or shared directly. Deployment is possible within the Studio environment or via Cloud Run if advanced scaling or hosting is needed.Inspiration on Demand: The ‘I’m Feeling Lucky’ ButtonOne standout feature in this update is the “I’m Feeling Lucky” button. Designed for users who need a creative jumpstart, it generates randomized app concepts and configures the app setup accordingly. Each press yields a different idea, complete with suggested AI features and components.Examples produced during demos include:An interactive map-based chatbot powered by Google Search and conversational AI.A dream garden designer using image generation and advanced planning tools.A trivia game app with an AI host whose personality users can define, integrating both Imagine and Flashlight with Gemini 2.5 Pro for conversation and reasoning.Logan Kilpatrick, Lead of Product for Google AI Studio and Gemini AI, noted in a demo video of his own that this feature encourages discovery and experimentation. “You get some really, really cool, different experiences,” he said, emphasizing its role in helping users find novel ideas quickly.Hands-On Test: From Prompt to App in 65 SecondsTo test the new workflow, I prompted Gemini with:A randomized dice rolling web application where the user can select between common dice sizes (6 sides, 10 sides, etc) and then see an animated die rolling and choose the color of their die as well.Within 65 seconds (just over a minute) AI Studio returned a fully working web app featuring:Dice size selector (d4, d6, d8, d10, d12, d20)Color customization options for the dieAnimated rolling effect with randomized resultsClean, modern UI built with React, TypeScript, and Tailwind CSSThe platform also generated a complete set of structured files, including App.tsx, constants.ts, and separate components for dice logic and controls. After generation, it was easy to iterate: adding sound effects for each interaction (rolling, choosing a die, changing color) required only a single follow-up prompt to the built-in assistant. This was also suggested by Gemini, too, by the way. From there, the app can be previewed live or exported using built-in controls to:Save to GitHubDownload the full codebaseCopy the project for remixingDeploy via integrated toolsMy brief, hands-on test showed just how quickly even small utility apps can go from idea to interactive prototype—without leaving the browser or writing boilerplate code manually.AI-Suggested Enhancements and Feature RefinementIn addition to code generation, Google AI Studio now offers context-aware feature suggestions. These recommendations, generated by Gemini’s Flashlight capability, analyze the current app and propose relevant improvements.In one example, the system suggested implementing a feature that displays the history of previously generated images in an image studio tab. These iterative enhancements allow builders to expand app functionality over time without starting from scratch.Kilpatrick emphasized that users can continue to refine their projects as they go, combining both automatic generation and manual adjustments. “You can go in and continue to edit and sort of refine the experience that you want iteratively,” he said.Free to Start, Flexible to GrowThe new experience is available at no cost for users who want to experiment, prototype, or build lightweight apps. There’s no requirement to enter credit card information to begin using vibe coding.However, more powerful capabilities — such as using models like Veo 3.1 or deploying through Cloud Run — do require switching to a paid API key.This pricing structure is intended to lower the barrier to entry for experimentation while providing a clear path to scale when needed.Built for All Skill LevelsOne of the central goals of the vibe coding launch is to make AI app development accessible to more people. The system supports both high-level visual builders and low-level code editing, creating a workflow that works for developers across experience levels.Kilpatrick mentioned that while he’s more familiar with Python than TypeScript, he still found the editor useful because of the helpful file descriptions and intuitive layout. This focus on usability could make AI Studio a compelling option for developers exploring AI for the first time.More to Come: A Week of LaunchesThe launch of vibe coding is the first in a series of announcements expected throughout the week. While specific future features haven’t been revealed yet, both Kilpatrick and Löber hinted that additional updates are on the way.With this update, Google AI Studio positions itself as a flexible, user-friendly environment for building AI-powered applications—whether for fun, prototyping, or production deployment. The focus is clear: make the power of Gemini’s APIs accessible without unnecessary complexity.
