# AI News for 2025-12-17 (America/Los_Angeles)

Collected 6 article(s).

- [What to Know About Trump’s Executive Order to Curtail State AI Regulations - Insurance Journal](https://news.google.com/rss/articles/CBMid0FVX3lxTE0tOHVXOVdPVlBZUUhYbEZHbVkwcU9wMkRnNjFWaUh0OHA3dDhWOEpHNGFJa2ZpX3FvMmJNcDI4Zy1CY0xHNzgwY2hGSWlEVEFEMkRYXzZIZzUwZlZvVW9zSHdBN0RubkYxbWRkaFhDcjhWRnl3RFpN?oc=5) — 23:31 · Google News (AI)
  > What to Know About Trump’s Executive Order to Curtail State AI Regulations  Insurance Journal
- [Exclusive: FTC probes Instacart's AI pricing tool, source says; shares drop - Reuters](https://news.google.com/rss/articles/CBMirgFBVV95cUxNaHZmV2NBZXUtZWZqS0ZMWE42dVlXZE1xakF2UjZvUnYtc0c5cnNtMEp2UGV6WGdxRHlLb19PbVNiRHkycnEzQUFzOFMxMmRUbVMtSGpFeWVZQndkN2ttdU5NX2ktbmhpakJKaGxMSjl4X2JiQkw1bW16SEYxUUFLVGRPWmZUbmdkR0NHaE5vc3kzZjctWGZZWUlTUEtsdVNnUUZpbnZmdld5Qk9FZ2c?oc=5) — 18:38 · Google News (AI)
  > Exclusive: FTC probes Instacart's AI pricing tool, source says; shares drop  Reuters
- [SoftBank leads decline in Japanese tech stocks as worries over AI spending spill over to Asia - CNBC](https://news.google.com/rss/articles/CBMiqAFBVV95cUxOY0VwNlN2R3czZUtJMWpBS0RPMlNqMWpJUWxDOWRxT283YnVTWjZHV2I4VmxJNkRLNGxDd1BtazV4U3JZYVdJT1ZHNEZjQUNqbjlaOUhtWG1jU2s2OURCMTA4NEpFQ0s3dlN0dUlZUy1STHp1UWJCR1RMeHNtYVZGbXZiTjZOMG10cF80Z2tjSks3N3hXVWg4SkkzMEJSVUxSU2F2cER0c3rSAa4BQVVfeXFMUHo1WkRDcW1SLW9pcFB4eW15bWZUaGctTFplX04tVy1lNlJ1bWFjUFo3UVhtZU9UUUE4UGRWTXA2ejBrZndnYmdWZ3Baa0o1TFBuYzcydnhvYVh6Q0RjOXVJN1M2VXRaU09sd3VGZklRS2JFMGVTaEc1ejY1ODE5WWhFQVdKQ2hOTVNiWDgwcU83VHprbndKa1FOOFRmUFlOb29xbFJFRkNmbzlLTERB?oc=5) — 18:17 · Google News (AI)
  > SoftBank leads decline in Japanese tech stocks as worries over AI spending spill over to Asia  CNBC
- [Gemini 3 Flash arrives with reduced costs and latency — a powerful combo for enterprises](https://venturebeat.com/technology/gemini-3-flash-arrives-with-reduced-costs-and-latency-a-powerful-combo-for) — 11:24 · VentureBeat AI
  > Enterprises can now harness the power of a large language model that's near that of the state-of-the-art Google’s Gemini 3 Pro, but at a fraction of the cost and with increased speed, thanks to the newly released Gemini 3 Flash.The model joins the flagship Gemini 3 Pro, Gemini 3 Deep Think, and Gemini Agent, all of which were announced and released last month.Gemini 3 Flash, now available on Gemini Enterprise, Google Antigravity, Gemini CLI, AI Studio, and on preview in Vertex AI, processes information in near real-time and helps build quick, responsive agentic applications. The company said in a blog post that Gemini 3 Flash “builds on the model series that developers and enterprises already love, optimized for high-frequency workflows that demand speed, without sacrificing quality.The model is also the default for AI Mode on Google Search and the Gemini application. Tulsee Doshi, senior director, product management on the Gemini team, said in a separate blog post that the model “demonstrates that speed and scale don’t have to come at the cost of intelligence.”“Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows,” Doshi said. “It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.”Early adoption by specialized firms proves the model's reliability in high-stakes fields. Harvey, an AI platform for law firms, reported a 7% jump in reasoning on their internal 'BigLaw Bench,' while Resemble AI discovered that Gemini 3 Flash could process complex forensic data for deepfake detection 4x faster than Gemini 2.5 Pro. These aren't just speed gains; they are enabling 'near real-time' workflows that were previously impossible.More efficient at a lower costEnterprise AI builders have become more aware of the cost of running AI models, especially as they try to convince stakeholders to put more budget into agentic workflows that run on expensive models. Organizations have turned to smaller or distilled models, focusing on open models or other research and prompting techniques to help manage bloated AI costs.For enterprises, the biggest value proposition for Gemini 3 Flash is that it offers the same level of advanced multimodal capabilities, such as complex video analysis and data extraction, as its larger Gemini counterparts, but is far faster and cheaper. While Google’s internal materials highlight a 3x speed increase over the 2.5 Pro series, data from independent benchmarking firm Artificial Analysis adds a layer of crucial nuance. In the latter organization's pre-release testing, Gemini 3 Flash Preview recorded a raw throughput of 218 output tokens per second. This makes it 22% slower than the previous 'non-reasoning' Gemini 2.5 Flash, but it is still significantly faster than rivals including OpenAI's GPT-5.1 high (125 t/s) and DeepSeek V3.2 reasoning (30 t/s).Most notably, Artificial Analysis crowned Gemini 3 Flash as the new leader in their AA-Omniscience knowledge benchmark, where it achieved the highest knowledge accuracy of any model tested to date. However, this intelligence comes with a 'reasoning tax': the model more than doubles its token usage compared to the 2.5 Flash series when tackling complex indexes. This high token density is offset by Google's aggressive pricing: when accessing through the Gemini API, Gemini 3 Flash costs $0.50 per 1 million input tokens, compared to $1.25/1M input tokens for Gemini 2.5 Pro, and $3/1M output tokens, compared to $ 10/1 M output tokens for Gemini 2.5 Pro. This allows Gemini 3 Flash to claim the title of the most cost-efficient model for its intelligence tier, despite being one of the most 'talkative' models in terms of raw token volume. Here's how it stacks up to rival LLM offerings:ModelInput (/1M)Output (/1M)Total CostSourceQwen 3 Turbo$0.05$0.20$0.25Alibaba CloudGrok 4.1 Fast (reasoning)$0.20$0.50$0.70xAIGrok 4.1 Fast (non-reasoning)$0.20$0.50$0.70xAIdeepseek-chat (V3.2-Exp)$0.28$0.42$0.70DeepSeekdeepseek-reasoner (V3.2-Exp)$0.28$0.42$0.70DeepSeekQwen 3 Plus$0.40$1.20$1.60Alibaba CloudERNIE 5.0$0.85$3.40$4.25QianfanGemini 3 Flash Preview$0.50$3.00$3.50GoogleClaude Haiku 4.5$1.00$5.00$6.00AnthropicQwen-Max$1.60$6.40$8.00Alibaba CloudGemini 3 Pro (≤200K)$2.00$12.00$14.00GoogleGPT-5.2$1.75$14.00$15.75OpenAIClaude Sonnet 4.5$3.00$15.00$18.00AnthropicGemini 3 Pro (>200K)$4.00$18.00$22.00GoogleClaude Opus 4.5$5.00$25.00$30.00AnthropicGPT-5.2 Pro$21.00$168.00$189.00OpenAIMore ways to saveBut enterprise developers and users can cut costs further by eliminating the lag most larger models often have, which racks up token usage. Google said the model “is able to modulate how much it thinks,” so that it uses more thinking and therefore more tokens for more complex tasks than for quick prompts. The company noted Gemini 3 Flash uses 30% fewer tokens than Gemini 2.5 Pro. To balance this new reasoning power with strict corporate latency requirements, Google has introduced a 'Thinking Level' parameter. Developers can toggle between 'Low'—to minimize cost and latency for simple chat tasks—and 'High'—to maximize reasoning depth for complex data extraction. This granular control allows teams to build 'variable-speed' applications that only consume expensive 'thinking tokens' when a problem actually demands PhD-level loThe economic story extends beyond simple token prices. With the standard inclusion of Context Caching, enterprises processing massive, static datasets—such as entire legal libraries or codebase repositories—can see a 90% reduction in costs for repeated queries. When combined with the Batch API’s 50% discount, the total cost of ownership for a Gemini-powered agent drops significantly below the threshold of competing frontier models“Gemini 3 Flash delivers exceptional performance on coding and agentic tasks combined with a lower price point, allowing teams to deploy sophisticated reasoning costs across high-volume processes without hitting barriers,” Google said. By offering a model that delivers strong multimodal performance at a more affordable price, Google is making the case that enterprises concerned with controlling their AI spend should choose its models, especially Gemini 3 Flash. Strong benchmark performance But how does Gemini 3 Flash stack up against other models in terms of its performance? Doshi said the model achieved a score of 78% on the SWE-Bench Verified benchmark testing for coding agents, outperforming both the preceding Gemini 2.5 family and the newer Gemini 3 Pro itself!For enterprises, this means high-volume software maintenance and bug-fixing tasks can now be offloaded to a model that is both faster and cheaper than previous flagship models, without a degradation in code quality.The model also performed strongly on other benchmarks, scoring 81.2% on the MMMU Pro benchmark, comparable to Gemini 3 Pro. While most Flash type models are explicitly optimized for short, quick tasks like generating code, Google claims Gemini 3 Flash’s performance “in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.”First impressions from early usersSo far, early users have been largely impressed with the model, particularly its benchmark performance. What It Means for Enterprise AI UsageWith Gemini 3 Flash now serving as the default engine across Google Search and the Gemini app, we are witnessing the "Flash-ification" of frontier intelligence. By making Pro-level reasoning the new baseline, Google is setting a trap for slower incumbents. The integration into platforms like Google Antigravity suggests that Google isn't just selling a model; it's selling the infrastructure for the autonomous enterprise. As developers hit the ground running with 3x faster speeds and a 90% discount on context caching, the "Gemini-first" strategy becomes a compelling financial argument. In the high-velocity race for AI dominance, Gemini 3 Flash may be the model that finally turns "vibe coding" from an experimental hobby into a production-ready reality.
- [This Nobel Prize–winning chemist dreams of making water from thin air](https://www.technologyreview.com/2025/12/17/1129259/omar-yaghi-chemist-nobel-prize-crystals-water-air/) — 03:00 · MIT Technology Review (AI)
  > Omar Yaghi was a quiet child, diligent, unlikely to roughhouse with his nine siblings. So when he was old enough, his parents tasked him with one of the family’s most vital chores: fetching water. Like most homes in his Palestinian neighborhood in Amman, Jordan, the Yaghis’ had no electricity or running water. At least once…
- [AI is moving to the edge – and network security needs to catch up](https://venturebeat.com/ai/ai-is-moving-to-the-edge-and-network-security-needs-to-catch-up) — 00:00 · VentureBeat AI
  > Presented by T-Mobile for BusinessSmall and mid-sized businesses are adopting AI at a pace that would have seemed unrealistic even a few years ago. Smart assistants that greet customers, predictive tools that flag inventory shortages before they happen, and on-site analytics that help staff make decisions faster — these used to be features of the enterprise. Now they’re being deployed in retail storefronts, regional medical clinics, branch offices, and remote operations hubs.What’s changed is not just the AI itself, but where it runs. Increasingly, AI workloads are being pushed out of centralized data centers and into the real world — into the places where employees work and customers interact. This shift to the edge promises faster insights and more resilient operations, but it also transforms the demands placed on the network. Edge sites need consistent bandwidth, real-time data pathways, and the ability to process information locally rather than relying on the cloud for every decision.The catch is that as companies race to connect these locations, security often lags behind. A store may adopt AI-enabled cameras or sensors long before it has the policies to manage them. A clinic may roll out mobile diagnostic devices without fully segmenting their traffic. A warehouse may rely on a mix of Wi-Fi, wired, and cellular connections that weren’t designed to support AI-driven operations. When connectivity scales faster than security, it creates cracks — unmonitored devices, inconsistent access controls, and unsegmented data flows that make it hard to see what’s happening, let alone protect it.Edge AI only delivers its full value when connectivity and security evolve together.Why AI is moving to the edge — and what that breaksBusinesses are shifting AI to the edge for three core reasons:Real-time responsiveness: Some decisions can’t wait for a round trip to the cloud. Whether it’s identifying an item on a shelf, detecting an abnormal reading from a medical device, or recognizing a safety risk in a warehouse aisle, the delay introduced by centralized processing can mean missed opportunities or slow reactions.Resilience and privacy: Keeping data and inference local makes operations less vulnerable to outages or latency spikes, and it reduces the flow of sensitive information across networks. This helps SMBs meet data sovereignty and compliance requirements without rewriting their entire infrastructure.Mobility and deployment speed: Many SMBs operate across distributed footprints — remote workers, pop-up locations, seasonal operations, or mobile teams. Wireless-first connectivity, including 5G business lines, lets them deploy AI tools quickly without waiting for fixed circuits or expensive buildouts.Technologies like Edge Control from T-Mobile for Business fit naturally into this model. By routing traffic directly along the paths it needs — keeping latency-sensitive workloads local and bypassing the bottlenecks that traditional VPNs introduce — businesses can adopt edge AI without dragging their network into constant contention.Yet the shift introduces new risk. Every edge site becomes, in effect, its own small data center. A retail store may have cameras, sensors, POS systems, digital signage, and staff devices all sharing the same access point. A clinic may run diagnostic tools, tablets, wearables, and video consult systems side by side. A manufacturing floor might combine robotics, sensors, handheld scanners, and on-site analytics platforms.This diversity increases the attack surface dramatically. Many SMBs roll out connectivity first, then add piecemeal security later — leaving the blind spots attackers rely on.Zero trust becomes essential at the edgeWhen AI is distributed across dozens or hundreds of sites, the old idea of a single secure “inside” network breaks down. Every store, clinic, kiosk, or field location becomes its own micro-environment — and every device within it becomes its own potential entry point.Zero trust offers a framework to make this manageable.At the edge, zero trust means:Verifying identity rather than location — access is granted because a user or device proves who it is, not because it sits behind a corporate firewall.Continuous authentication — trust isn’t permanent; it’s re-evaluated throughout a session.Segmentation that limits movement — if something goes wrong, attackers can’t jump freely from system to system.This approach is especially critical given that many edge devices can’t run traditional security clients. SIM-based identity and secure mobile connectivity — areas where T-Mobile for Business brings significant strength — help verify IoT devices, 5G routers, and sensors that otherwise sit outside the visibility of IT teams.This is why connectivity providers are increasingly combining networking and security into a single approach. T-Mobile for Business embeds segmentation, device visibility, and zero-trust safeguards directly into its wireless-first connectivity offerings, reducing the need for SMBs to stitch together multiple tools.Secure-by-default networks reshape the landscapeA major architectural shift is underway: networks that assume every device, session, and workload must be authenticated, segmented, and monitored from the start. Instead of building security on top of connectivity, the two are fused.T-Mobile for Business solutions shows how this is evolving. Its SASE platform, powered by Palo Alto Networks Prisma SASE 5G, blends secure access with connectivity into one cloud-delivered service. Private Access gives users the least-privileged access they need, nothing more. T-SIMsecure authenticates devices at the SIM layer, allowing IoT sensors and 5G routers to be verified automatically. Security Slice isolates sensitive SASE traffic on a dedicated portion of the 5G network, ensuring consistency even during heavy demand.A unified dashboard like T-Platform brings it together, offering real-time visibility across SASE, IoT, business internet, and edge control — simplifying operations for SMBs with limited staff.The future: AI that runs the edge and protects itAs AI models become more dynamic and autonomous, we’ll see the relationship flip: the edge won’t just support AI; AI will actively run and secure the edge — optimizing traffic paths, adjusting segmentation automatically, and spotting anomalies that matter to one specific store or site.Self-healing networks and adaptive policy engines will move from experimental to expected.For SMBs, this is a pivotal moment. The organizations that modernize their connectivity and security foundations now will be the ones best positioned to scale AI everywhere — safely, confidently, and without unnecessary complexity.Partners like T-Mobile for Business are already moving in this direction, giving SMBs a way to deploy AI at the edge without sacrificing control or visibility.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
