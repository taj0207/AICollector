# AI News for 2025-12-10 (America/Los_Angeles)

Collected 20 article(s).

- [Oracle shares slide as earnings fail to ease AI bubble fears - BBC](https://news.google.com/rss/articles/CBMiWkFVX3lxTFA0UWJZYnJZRDdtaVR3Y2NEQnZ0YmhtMlc1dTZHOFVGVGRnbVp6TDF0WDBQS05wOEVRM0k1SG8ya2dnbHo1SHZOUFpZaG9sZ1dBVmtYLVdsYjVjZ9IBX0FVX3lxTFBKcnJoREltSkg0OTlvLU00M3dtNE5JSmMyN2c1VmVWSlF3WFJhTFRmakdiRE1Rd3p6a0VfcWo5V3cxVDBnU0k0ZFdiLW5tWnlCTnJNLWR2Q3JjOHFySzBR?oc=5) — 23:08 · Google News (AI)
  > Oracle shares slide as earnings fail to ease AI bubble fears  BBC
- [Over $50 billion in under 24 hours: Why Big Tech is doubling down on investing in India - CNBC](https://news.google.com/rss/articles/CBMinwFBVV95cUxNdzNxYkVqZHA3UkxGQW81OFE3eWhWYXpnZ2VoalFQbFpqaWJEa0ZfNXlzOW1aYUNBVTJBaGtFYzNwOHpzWERwRk1jZnN4Mi1ScDZZQjVRMURQaWMzdjNZQ3AzRTdkMmtmV3ZRR2tKVFhLeUU5RlFSc011aktrUGdpYk1NakZ1RE5RM0t1YmJKSWl5dGU1Z2hvMEIwX1B2NEHSAaQBQVVfeXFMT2FraDVqR2JielZITVlvWjI4aGp3ZXNsMFJSV21USlowX0JFd2h1elpmLW5fSzBKeVlXR0xPd3hiblZ3NVBwaFRhNC00OUlGbG1seERuVERhdlRuUEV5czM2b0lzRDR1dlNUNzVJaDR1b2V0YjZJOW40b0g3dGxvMU1YVHdjZVg0TjJ6N1A2MXVIdnJLWDk2ajZDaVZ3TXRDNy04NlM?oc=5) — 21:25 · Google News (AI)
  > Over $50 billion in under 24 hours: Why Big Tech is doubling down on investing in India  CNBC
- [Creating a glass box: How NetSuite is engineering trust into AI](https://venturebeat.com/ai/creating-a-glass-box-how-netsuite-is-engineering-trust-into-ai) — 21:00 · VentureBeat AI
  > Presented by Oracle NetSuiteWhen any company tells you it is their biggest product release in almost three decades, it’s worth listening. When the person saying it founded the world’s first cloud computing company, it’s time to take note. At SuiteWorld 2025, Evan Goldberg, founder and EVP of Oracle NetSuite, did just that when he called NetSuite Next the company’s biggest product evolution in nearly three decades. But behind that sweeping vision lies a quieter shift — one centered on how AI behaves, not just what it can do. “Every company is experimenting with AI,” says Brian Chess, SVP of Technology and AI at NetSuite. “Some ideas hit the mark, and some don’t, but each one teaches us something. That’s how innovation works.”For Chess and Gary Wiessinger, SVP of Application Development at NetSuite, the challenge lies in governing AI responsibly. Rather than reinventing its system, NetSuite is extending the same principles into the AI era that have guided its strategy for 27 years — security, control, and auditability. The goal is to make AI actions traceable, permissions enforceable, and outcomes auditable.The philosophy underpins what Chess calls a “glass-box” approach to enterprise AI, where decisions are visible and every agent operates within human-defined guardrails.Built on Oracle’s foundationNetSuite Next is the result of five years of development. It is built on Oracle Cloud Infrastructure (OCI), which is relied on by many of the world’s most important AI model providers, and has AI capabilities integrated directly into its core rather than added as a separate layer.“We are building a fantastic foundation on OCI,” Chess says. “That infrastructure provides more than compute power.” Built on the same OCI foundation that powers NetSuite today, NetSuite Next gives customers access to Oracle’s latest AI innovations along with the performance, scalability, and security of OCI’s enterprise-grade platform.Wiessinger emphasizes the team's approach as “needs first, technology second.”“We don’t take a technology-first approach,” he says. “We take a customer-needs-first approach and then figure out how to use the latest technology to solve those needs better.”That philosophy extends across Oracle’s ecosystem. NetSuite’s collaboration with Oracle’s AI Database, Fusion Applications, Analytics, and Cloud Infrastructure teams helps NetSuite deliver capabilities that independent vendors can’t match, he says — an AI system that is both open to innovation and grounded in Oracle’s security and scale.The data structure advantageAt the heart of the platform is a structured data model that serves as a critical advantage.“One of the great things about NetSuite is, because the data comes in and it gets structured, the connections between the data are explicit,” Chess explains. “That means the AI can start exploring that knowledge graph that the company has been building up.”Where general LLMs sift through unstructured text, NetSuite’s AI works from structured data, identifying precise links between transactions, accounts, and workflows to deliver context-aware insights. Wiessinger adds, “The data we have spans financials, CRM, commerce, and HR. We can do more for customers because we see more of their business in one place.” Combined with built-in business logic and metadata, that scope allows NetSuite to generate recommendations and insights that are accurate and explainable.Oracle’s Redwood design system provides the visual layer for this data intelligence, creating what Goldberg described as a "modern, clean and intuitive" workspace where AI and humans collaborate naturally.Designing for accountabilityOne downside of enterprise AI is that many systems still function as a black box — they produce results but offer little visibility into how they reached them. NetSuite is different. It is designing its systems around transparency, making visibility a defining feature.“When users can see how AI reached a decision — tracing the path from A to B — they don’t just verify accuracy,” Chess says. “They learn how the AI knew to do that.”That visibility turns AI into a learning engine. As Chess puts it, transparency becomes a “fantastic teacher,” helping organizations understand, improve, and trust automation over time.But Chess cautions against blind trust: “What’s disturbing is when someone presents something to me and says, ‘Look what AI gave me,’ as if that makes it authoritative. People need to ask, ‘What grounded this? Why is it correct?’” NetSuite’s answer is traceability. When someone asks, “Where did this number come from?” the system can show them the full reasoning behind it.Governance by designAI agents inside NetSuite Next follow the same governance model as employees: roles, permissions, and escalation rules. Role-based security embedded directly into workflows helps ensure that agents act only within authorized boundaries.Wiessinger puts it plainly: “If AI generates a narrative summary of a report and it’s 80% of what the user would have written, that’s fine. We’ll learn from their feedback and make it even better. But booking to the general ledger is different. That has to be 100% correct and is where controls and human review really matter.”Auditing the algorithmAuditing has always been part of ERP’s DNA, and NetSuite now extends that discipline to AI. Every agent action, workflow adjustment, and model-generated code snippet is recorded within the system’s existing audit framework. As Chess explains, “It’s the same audit trail you might use to figure out what the humans did. Code is auditable. When the LLM creates code and something happens in the system, we can trace back.”That traceability transforms AI from a black box into a glass box. When an algorithm accelerates a payment or flags an anomaly, teams can see exactly which inputs and logic produced the decision — an essential safeguard for regulated industries and finance teams.Safe extensibilityThe other half of trust is freedom — the ability to extend AI without risking data exposure.The NetSuite AI Connector Service and SuiteCloud Platform make that possible. Through standards like the Model Context Protocol (MCP), customers can connect external language models while keeping sensitive data secure inside Oracle’s environment.“Businesses are hungry for AI,” Chess says. “They want to start putting it to work. But they also want to know those experiments can’t go off the rails. The NetSuite AI Connector Service and governance model give partners the freedom to innovate while maintaining the same audit and permission logic that govern native features.”Culture, experimentation, and guardrailsGovernance frameworks only work if people use them wisely. Both executives see AI adoption as a top-down and bottom-up process.“The board is telling the CEO they need an AI strategy,” Chess says. “Meanwhile, employees are already using AI. If I were a CEO, I’d start by asking: what are you already doing, and what’s working?”Wiessinger agrees that balance is key: “Some companies go all-in on a centralized AI team while others let everyone experiment freely. Neither works by itself. You need structure for major initiatives and freedom for grassroots innovation.”He offers a simple example: “Write an email? Go crazy. Touch financials or employee data? Don’t go crazy with that.”Experimentation, both emphasize, is imperative. “No one should wait for us or anyone else,” Wiessinger says. “Start testing, learn quickly, and be intentional about making it work for your business.”Why transparent AI winsAs AI moves deeper into enterprise operations, governance will define competitive advantage as much as innovation. NetSuite’s approach — extending its heritage of ERP controls into the age of autonomous systems, built on Oracle’s secure cloud infrastructure and structured-data foundation — positions it to lead in both.In a world of opaque models and risky promises, the companies that win won’t just build smarter AI. They’ll build AI you can trust.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
- [AI: Microsoft, Amazon bet big, but where does India stand in the global race? - BBC](https://news.google.com/rss/articles/CBMiWkFVX3lxTE1mQmpyMXRTOVM2TzdKN1RBSlhtYXdoQWtGRGZXMDI1TXF1TkxvaDlkejFNMnpacFpjOTFzNm5lV3NnMWgwdnltUkotdkV3aVNwYjlPYnp2Z2hHQdIBX0FVX3lxTFBxYURTS1V1VjlvYTNMZzV2WUZscjdhR05Gc2hDa1JmclVraGlNWTJlNmIwOXg1eU5lTW5JNGFYTEg1TTJSNG1iLWJ4R2RKc0dxczF1S3NRX0xwOWVTVzlj?oc=5) — 18:46 · Google News (AI)
  > AI: Microsoft, Amazon bet big, but where does India stand in the global race?  BBC
- [China’s AI Power Play: Cheap Electricity From World’s Biggest Grid - The Wall Street Journal](https://news.google.com/rss/articles/CBMi-wJBVV95cUxNUXptQ1F1b2JxbDRSU2pjWm5WcVVkZlNha0hGX3Q2bkxQb3RFNTB0VU00RGRqcTNOS24tVFk1M1g2RVAxcHBBUzlNUXNkaV91dGRXUFZ6QXkxbTFNX2w0c21QUmJyRXJKVGE0TGppZDdCY0xLN0JGUjBOcjZGZjZwbjV4c3NxS3lNTmJtZzdhYmtESXVnSGl6MkhxRGlCVjNUMWhMTC0wenpqOEw0R2R6eUZ5azFsa2NMa2JZaEZYRWN1ZUhveXVTS3dDTmQ5QUVFUGdCZFhXU2QtQjktMk83dlM3NVFFSWtvdU9PS1FoWk1qTWI2SF83NlMtN1VRN213RlJNV1FWeEo1SGFkREpONGFKTHVTc3FYMkJ3a3RJUldpeFZKaXdCb1BNcTRuTFZNNlR5NGd4SENhZ0hfWUFwWktTemQxMDMwNmFvWGNuY1ozSjFMWWtEamFNWGdMdG5rOUlBcmtrQ0swX2RzNm9Xc2tyUzJDNXFkWlFF?oc=5) — 18:00 · Google News (AI)
  > China’s AI Power Play: Cheap Electricity From World’s Biggest Grid  The Wall Street Journal
- [Newsom posts AI video of Trump in handcuffs - thehill.com](https://news.google.com/rss/articles/CBMijAFBVV95cUxNY1ZHM3RuSFc4WmV4ODZVYUdGRmliYjMyZHhtUi1tX0hBMURoOHZLWkdoWjNRNVdWeW8wb3U3VVU4Rlh6SnVEVlpIN0dJZFJsQ2ctZFJYMWtwUkZiazJOVFJVSG1GcURjV0JXQWhTMzR0MzhoT2pzdmJkWm9DMEZiVmItcmJlWnhQYXo4QdIBkgFBVV95cUxNa3FjdGtVeXllcWMxZE1uZFRCVHROaTBFdUVaU25RbHhFUjZpNWtXNExJbHB5QjdjZ2hSaGppWU5HWU1OczhNUVQtZ0NoSlNQSnhIOVZJWVZtNW84aE44Sm1GRFFxR05scGhvVVZVY3FURnNMOWxYZTctYTVVNk9VcHJyRENSbXpZbmhnV1lhMzVuQQ?oc=5) — 17:52 · Google News (AI)
  > Newsom posts AI video of Trump in handcuffs  thehill.com
- [Google’s answer to the AI arms race — promote the guy behind its data center tech - TechCrunch](https://news.google.com/rss/articles/CBMiswFBVV95cUxNbXpNdzM4V0JzSFJ3ek44Z01UX0RjRXBpTEUxMFRyTGlmMnplUTdydTlhbjJ4WS1iMHZHNHlNcV9JOTd6X3pQMW9jcXRMSGRfa0JKZkhjSDJpWHRPNWV1ZkZCS0JSdmJBNVh2OFZ2bXZkS0NxR3ZldjVDbXB4QlBsOTZlbHFpTk8xUl9yM2I5YVVzc2YwRHlrNm4wY1pHcDZmV0c3cW9mLW05RVF6QXFQTmgwUQ?oc=5) — 17:10 · Google News (AI)
  > Google’s answer to the AI arms race — promote the guy behind its data center tech  TechCrunch
- [State attorneys general warn Microsoft, OpenAI, Google, and other AI giants to fix 'delusional' outputs - TechCrunch](https://news.google.com/rss/articles/CBMizgFBVV95cUxPV0NSRDJfdjFQR3ZTUk9PNVNXbnpIcVFYVjc5a2JoZG84c3VlUE03Zk1ndzV5Ni1YOGU4MUpyUUhrejNNY25VT1U4cU9Sb040eTI4M3BBMkFRemhWTkxJMHBFWUxxT1dOZV9GVlhoQ2dDcHp5S2Jyd2REZUhKaHAwcWotMHFEVkZrbW5ydzBJVEg3S3BEOVF5ZUE4dW9JU1BSbFFpbHkyX1V4LWZSVGk0bHZEV3RuRThzNlBaeHlmek4xU3g4VjkxY3FpVldMdw?oc=5) — 16:13 · Google News (AI)
  > State attorneys general warn Microsoft, OpenAI, Google, and other AI giants to fix 'delusional' outputs  TechCrunch
- [McDonald's pulls 'creepy' AI-generated Christmas ad after viewer backlash - Fox Business](https://news.google.com/rss/articles/CBMipwFBVV95cUxPa3J1ZXFzeGQ0S3FCMkcxbC12aHQ2eWM3UXZWTmlDbVl4ZjFsSzZQY3dmLW43bEo0U3Bweld6d0F2R0o3OE1lZ2hJWkJkTDIyVHhkcXdGXzhBZkFyZ2hKQzRWZXhxSzNpd3FFQWdFcTNMVGZsam9WSUFqcGlWVk16ZU5sQkVFUXhlMFhPOXIwMXlyaU1FSjFCOXZqZzZiTDVPUzlqOE1CMNIBrAFBVV95cUxPcGVuVW0yZFN5aTJHdHJEUWN4MWU1WFNvSVNNbFBrSGExTjc3cmNsY2F3Y2FRc2w3NzNJOUdUYU1vVlVoWUlQY3JuUmpScllrMUUtY3pXSTVjc2YzR2JSZVVGR0VtUzdPM2haZ09fN20xUVoxOVFhMVZ0QXRPaElaTEtFU01kOTZnb3R2M2Z1aXVOQTR5UXQxaDJiLXdMaHlySVJqXzlQMDJWV29I?oc=5) — 16:00 · Google News (AI)
  > McDonald's pulls 'creepy' AI-generated Christmas ad after viewer backlash  Fox Business
- [Donald Trump reminds the entire world he has no idea what 6G means](https://www.theverge.com/policy/842087/trump-does-not-understand-6g-5g) — 15:37 · The Verge (AI)
  > When business leaders spout buzzwords like "AI," "8K" and "5G," sometimes in the same sentence, we often get a sneaking suspicion they don't know what they mean! With President Donald Trump, there's no need to wonder: he clearly has no idea. "What does [6G] do? Give you a little bit deeper view into somebody's skin?" […]
- [The 70% factuality ceiling: why Google’s new ‘FACTS’ benchmark is a wake-up call for enterprise AI](https://venturebeat.com/ai/the-70-factuality-ceiling-why-googles-new-facts-benchmark-is-a-wake-up-call) — 15:00 · VentureBeat AI
  > There's no shortage of generative AI benchmarks designed to measure the performance and accuracy of a given model on completing various helpful enterprise tasks — from coding to instruction following to agentic web browsing and tool use. But many of these benchmarks have one major shortcoming: they measure the AI's ability to complete specific problems and requests, not how factual the model is in its outputs — how well it generates objectively correct information tied to real-world data — especially when dealing with information contained in imagery or graphics.For industries where accuracy is paramount — legal, finance, and medical — the lack of a standardized way to measure factuality has been a critical blind spot.That changes today: Google’s FACTS team and its data science unit Kaggle released the FACTS Benchmark Suite, a comprehensive evaluation framework designed to close this gap. The associated research paper reveals a more nuanced definition of the problem, splitting "factuality" into two distinct operational scenarios: "contextual factuality" (grounding responses in provided data) and "world knowledge factuality" (retrieving information from memory or the web).While the headline news is Gemini 3 Pro’s top-tier placement, the deeper story for builders is the industry-wide "factuality wall."According to the initial results, no model—including Gemini 3 Pro, GPT-5, or Claude 4.5 Opus—managed to crack a 70% accuracy score across the suite of problems. For technical leaders, this is a signal: the era of "trust but verify" is far from over.Deconstructing the BenchmarkThe FACTS suite moves beyond simple Q&A. It is composed of four distinct tests, each simulating a different real-world failure mode that developers encounter in production:Parametric Benchmark (Internal Knowledge): Can the model accurately answer trivia-style questions using only its training data?Search Benchmark (Tool Use): Can the model effectively use a web search tool to retrieve and synthesize live information?Multimodal Benchmark (Vision): Can the model accurately interpret charts, diagrams, and images without hallucinating?Grounding Benchmark v2 (Context): Can the model stick strictly to the provided source text?Google has released 3,513 examples to the public, while Kaggle holds a private set to prevent developers from training on the test data—a common issue known as "contamination."The Leaderboard: A Game of InchesThe initial run of the benchmark places Gemini 3 Pro in the lead with a comprehensive FACTS Score of 68.8%, followed by Gemini 2.5 Pro (62.1%) and OpenAI’s GPT-5 (61.8%).However, a closer look at the data reveals where the real battlegrounds are for engineering teams.ModelFACTS Score (Avg)Search (RAG Capability)Multimodal (Vision)Gemini 3 Pro68.883.846.1Gemini 2.5 Pro62.163.946.9GPT-561.877.744.1Grok 453.675.325.7Claude 4.5 Opus51.373.239.2Data sourced from the FACTS Team release notes.For Builders: The "Search" vs. "Parametric" GapFor developers building RAG (Retrieval-Augmented Generation) systems, the Search Benchmark is the most critical metric.The data shows a massive discrepancy between a model's ability to "know" things (Parametric) and its ability to "find" things (Search). For instance, Gemini 3 Pro scores a high 83.8% on Search tasks but only 76.4% on Parametric tasks. This validates the current enterprise architecture standard: do not rely on a model's internal memory for critical facts.If you are building an internal knowledge bot, the FACTS results suggest that hooking your model up to a search tool or vector database is not optional—it is the only way to push accuracy toward acceptable production levels.The Multimodal WarningThe most alarming data point for product managers is the performance on Multimodal tasks. The scores here are universally low. Even the category leader, Gemini 2.5 Pro, only hit 46.9% accuracy.The benchmark tasks included reading charts, interpreting diagrams, and identifying objects in nature. With less than 50% accuracy across the board, this suggests that Multimodal AI is not yet ready for unsupervised data extraction. Bottom line: If your product roadmap involves having an AI automatically scrape data from invoices or interpret financial charts without human-in-the-loop review, you are likely introducing significant error rates into your pipeline.Why This Matters for Your StackThe FACTS Benchmark is likely to become a standard reference point for procurement. When evaluating models for enterprise use, technical leaders should look beyond the composite score and drill into the specific sub-benchmark that matches their use case:Building a Customer Support Bot? Look at the Grounding score to ensure the bot sticks to your policy documents. (Gemini 2.5 Pro actually outscored Gemini 3 Pro here, 74.2 vs 69.0).Building a Research Assistant? Prioritize Search scores.Building an Image Analysis Tool? Proceed with extreme caution.As the FACTS team noted in their release, "All evaluated models achieved an overall accuracy below 70%, leaving considerable headroom for future progress."For now, the message to the industry is clear: The models are getting smarter, but they aren't yet infallible. Design your systems with the assumption that, roughly one-third of the time, the raw model might just be wrong.
- [Spotify&#8217;s Prompted Playlists use AI to control your algorithm](https://www.theverge.com/news/842053/spotify-ai-prompted-playlists) — 14:11 · The Verge (AI)
  > A new beta feature on Spotify takes the idea behind the Discover Weekly playlist and puts the control in users' hands through AI prompts. Prompted Playlists, which are initially launching in New Zealand on Thursday, let users type out exactly what they want to listen to, with as much or little detail as they like. […]
- [Big Tech warned over AI 'delusional' outputs by US attorneys general - Reuters](https://news.google.com/rss/articles/CBMi0AFBVV95cUxPOTZPNW82UVFfVk45Y21yVDloRDB6Q0szYWIxb0RUNU1CbEt1SWQ1dnMxTjJ4R3FlRHQ2dktWck5UTmp1dkxnazczcGxfOGhPUFhiYzQzMW5xU1owLWQzdkZuWjJxTktlMUlEX2phRmhTa2VRWTV3UFpsVXJiNVRmSnNmMWE3MXk4NThLNWEyNjB0dDNySkcwUEdHQ1hCSUxncUlZcXBNTThTREZHZXhoZDY3cUJsTF9oNkhFT1lhM3FvQ3A1eFNmRDNWZ3NSeXVj?oc=5) — 12:47 · Google News (AI)
  > Big Tech warned over AI 'delusional' outputs by US attorneys general  Reuters
- [The AI that scored 95% — until consultants learned it was AI](https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai) — 07:00 · VentureBeat AI
  > Presented by SAPWhen SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.The fifth team was told the very same answers had come from AI.They rejected almost everything.Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.Overcoming AI skepticismResistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”The consultant time-shift: from tech execution to business insightHistorically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.That mismatch is exactly where Joule steps in.“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”Bringing new consultants up to speedAI is also transforming how new hires learn.“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.Looking ahead to the future of AI copilots“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.”
Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.
- [OpenAI report reveals a 6x productivity gap between AI power users and everyone else](https://venturebeat.com/ai/openai-report-reveals-a-6x-productivity-gap-between-ai-power-users-and) — 06:30 · VentureBeat AI
  > The tools are available to everyone. The subscription is company-wide. The training sessions have been held. And yet, in offices from Wall Street to Silicon Valley, a stark divide is opening between workers who have woven artificial intelligence into the fabric of their daily work and colleagues who have barely touched it.The gap is not small. According to a new report from OpenAI analyzing usage patterns across its more than one million business customers, workers at the 95th percentile of AI adoption are sending six times as many messages to ChatGPT as the median employee at the same companies. For specific tasks, the divide is even more dramatic: frontier workers send 17 times as many coding-related messages as their typical peers, and among data analysts, the heaviest users engage the data analysis tool 16 times more frequently than the median.This is not a story about access. It is a story about a new form of workplace stratification emerging in real time — one that may be reshaping who gets ahead, who falls behind, and what it means to be a skilled worker in the age of artificial intelligence.Everyone has the same tools, but not everyone is using themPerhaps the most striking finding in the OpenAI report is how little access explains. ChatGPT Enterprise is now deployed across more than 7 million workplace seats globally, a nine-fold increase from a year ago. The tools are the same for everyone. The capabilities are identical. And yet usage varies by orders of magnitude.Among monthly active users — people who have logged in at least once in the past 30 days — 19 percent have never tried the data analysis feature. Fourteen percent have never used reasoning capabilities. Twelve percent have never used search. These are not obscure features buried in submenus; they are core functionality that OpenAI highlights as transformative for knowledge work.The pattern inverts among daily users. Only 3 percent of people who use ChatGPT every day have never tried data analysis; just 1 percent have skipped reasoning or search. The implication is clear: the divide is not between those who have access and those who don't, but between those who have made AI a daily habit and those for whom it remains an occasional novelty.Employees who experiment more are saving dramatically more timeThe OpenAI report suggests that AI productivity gains are not evenly distributed across all users but concentrated among those who use the technology most intensively. Workers who engage across approximately seven distinct task types — data analysis, coding, image generation, translation, writing, and others — report saving five times as much time as those who use only four. Employees who save more than 10 hours per week consume eight times more AI credits than those who report no time savings at all.This creates a compounding dynamic. Workers who experiment broadly discover more uses. More uses lead to greater productivity gains. Greater productivity gains presumably lead to better performance reviews, more interesting assignments, and faster advancement—which in turn provides more opportunity and incentive to deepen AI usage further.Seventy-five percent of surveyed workers report being able to complete tasks they previously could not perform, including programming support, spreadsheet automation, and technical troubleshooting. For workers who have embraced these capabilities, the boundaries of their roles are expanding. For those who have not, the boundaries may be contracting by comparison.The corporate AI paradox: $40 billion spent, 95 percent seeing no returnThe individual usage gap documented by OpenAI mirrors a broader pattern identified by a separate study from MIT's Project NANDA. Despite $30 billion to $40 billion invested in generative AI initiatives, only 5 percent of organizations are seeing transformative returns. The researchers call this the "GenAI Divide" — a gap separating the few organizations that succeed in transforming processes with adaptive AI systems from the majority that remain stuck in pilots.The MIT report found limited disruption across industries: only two of nine major sectors—technology and media—show material business transformation from generative AI use. Large firms lead in pilot volume but lag in successful deployment.The pattern is consistent across both studies. Organizations and individuals are buying the technology. They are launching pilots. They are attending training sessions. But somewhere between adoption and transformation, most are getting stuck.While official AI projects stall, a shadow economy is thrivingThe MIT study reveals a striking disconnect: while only 40 percent of companies have purchased official LLM subscriptions, employees in over 90 percent of companies regularly use personal AI tools for work. Nearly every respondent reported using LLMs in some form as part of their regular workflow."This 'shadow AI' often delivers better ROI than formal initiatives and reveals what actually works for bridging the divide," MIT's Project NANDA found.The shadow economy offers a clue to what's happening at the individual level within organizations. Employees who take initiative — who sign up for personal subscriptions, who experiment on their own time, who figure out how to integrate AI into their workflows without waiting for IT approval — are pulling ahead of colleagues who wait for official guidance that may never come.These shadow systems, largely unsanctioned, often deliver better performance and faster adoption than corporate tools. Worker sentiment reveals a preference for flexible, responsive tools — precisely the kind of experimentation that separates OpenAI's frontier workers from the median.The biggest gaps show up in technical work that used to require specialistsThe largest relative gaps between frontier and median workers appear in coding, writing, and analysis — precisely the task categories where AI capabilities have advanced most rapidly. Frontier workers are not just doing the same work faster; they appear to be doing different work entirely, expanding into technical domains that were previously inaccessible to them.Among ChatGPT Enterprise users outside of engineering, IT, and research, coding-related messages have grown 36 percent over the past six months. Someone in marketing or HR who learns to write scripts and automate workflows is becoming a categorically different employee than a peer who has not — even if they hold the same title and started with the same skills.The academic research on AI and productivity offers a complicated picture. Several studies cited in the OpenAI report find that AI has an "equalizing effect," disproportionately helping lower-performing workers close the gap with their higher-performing peers. But the equalizing effect may apply only within the population of workers who actually use AI regularly. A meaningful share of workers are not in that group at all. They remain light users or non-users, even as their more adventurous colleagues pull away.Companies are divided too, and the gap is widening by the monthThe divide is not only between individual workers. It exists between entire organizations.Frontier firms — those at the 95th percentile of adoption intensity — generate approximately twice as many AI messages per employee as the median enterprise. For messages routed through custom GPTs, purpose-built tools that automate specific workflows, the gap widens to seven-fold.These numbers suggest fundamentally different operating models. At median companies, AI may be a productivity tool that individual workers use at their discretion. At frontier firms, AI appears to be embedded in core infrastructure: standardized workflows, persistent custom tools, systematic integration with internal data systems.The OpenAI report notes that roughly one in four enterprises still has not enabled connectors that give AI access to company data—a basic step that dramatically increases the technology's utility. The MIT study found that companies that purchased AI tools from specialized vendors succeeded 67 percent of the time, while internal builds had only a one-in-three success rate. For many organizations, the AI era has technically arrived but has not yet begun in practice.The technology is no longer the problem — organizations areFor executives, the data presents an uncomfortable challenge. The technology is no longer the constraint. OpenAI notes that it releases a new feature or capability roughly every three days; the models are advancing faster than most organizations can absorb. The bottleneck has shifted from what AI can do to whether organizations are structured to take advantage of it."The dividing line isn't intelligence," the MIT authors write. The problems with enterprise AI have to do with memory, adaptability, and learning capability. Problems stem less from regulations or model performance, and more from tools that fail to learn or adapt.Leading firms, according to the OpenAI report, consistently invest in executive sponsorship, data readiness, workflow standardization, and deliberate change management. They build cultures where custom AI tools are created, shared, and refined across teams. They track performance and run evaluations. They make AI adoption a strategic priority rather than an individual choice.The rest are leaving it to chance — hoping that workers will discover the tools on their own, experiment on their own time, and somehow propagate best practices without infrastructure or incentive. The six-fold gap suggests this approach is not working.The window to catch up is closing faster than most companies realizeWith enterprise contracts locking in over the next 18 months, there's a shrinking window for vendors and adopters to cross the divide.The GenAI Divide identified by the MIT report is not going to last forever. But the organizations that figure out a way across it soonest will be the ones that define the next era of business.Both reports carry caveats. The OpenAI data comes from a company with an obvious interest in promoting AI adoption. The productivity figures are self-reported by customers already paying for the product. The MIT study, while independent, relies on interviews and surveys rather than direct measurement. The long-term effects of this technology on employment, wages, and workplace dynamics remain uncertain.But the core finding — that access alone does not produce adoption, and that adoption varies enormously even within organizations that have made identical tools available to all — is consistent with how previous technologies have diffused through the economy. Spreadsheets, email, and the internet all created similar divides before eventually becoming universal. The question is how long the current gap persists, who benefits during the transition, and what happens to workers who find themselves on the wrong side of it.For now, the divide is stark. Ninety percent of users said they prefer humans for "mission-critical work," while AI has "won the war for simple work." The workers who are pulling ahead are not doing so because they have access their colleagues lack. They are pulling ahead because they decided to use what everyone already has—and kept using it until they figured out what it could do.The 6x gap is not about technology. It is about behavior. And behavior, unlike software, cannot be deployed with a company-wide rollout.
- [Quilter's AI just designed an 843‑part Linux computer that booted on the first try. Hardware will never be the same.](https://venturebeat.com/ai/quilters-ai-just-designed-an-843-part-linux-computer-that-booted-on-the) — 06:00 · VentureBeat AI
  > A Los Angeles-based startup has demonstrated what it calls a breakthrough in hardware development: an artificial intelligence system that designed a fully functional Linux computer in one week — a process that would typically consume nearly three months of skilled engineering labor.Quilter, which has raised more than $40 million from investors including Benchmark, Index Ventures, and Coatue, used its physics-driven AI to automate the design of a two-board computer system that booted successfully on its first attempt, requiring no costly revisions. The project, internally dubbed "Project Speedrun," required just 38.5 hours of human labor compared to the 428 hours that professional PCB designers quoted for the same task.The announcement also marks the first public disclosure that Tony Fadell, the engineer who led development of the iPod and iPhone at Apple and later founded Nest, has invested in the company and serves as an advisor."We didn't teach Quilter to draw; we taught it to think in physics," said Sergiy Nesterenko, Quilter's chief executive and a former SpaceX engineer, in an exclusive interview with VentureBeat. "The result wasn't a simulation — it was a working computer."Circuit board design remains the forgotten bottleneck that delays nearly every hardware productThe announcement shines a light on an unglamorous but critical chokepoint in technology development: printed circuit board layout. While semiconductors and software have received enormous attention and investment, the green fiberglass boards that connect chips, memory, and components in virtually every electronic device remain stubbornly manual to design."Besides auto-routers, the technology really hadn't changed since the early '90s," Fadell told VentureBeat. "The best boards are still made by hand. You go to Apple, they've got the tools, and these guys are just pushing traces, checking everything, doing flood fills—and you're like, there's got to be a better way."The PCB design process typically unfolds in three stages. Engineers first create a schematic — a logical diagram showing how components connect. Then a specialist manually draws the physical layout in CAD software, placing components and routing thousands of copper traces across multiple layers. Finally, the design goes to a manufacturer for fabrication.That middle step — the layout — creates a persistent bottleneck. For a board of moderate complexity, the process typically consumes four to eight weeks. For sophisticated systems like computers or automotive electronics, timelines stretch to three months or longer."The timeline was always this elastic thing—they'd say, 'Yeah, that's two weeks minimum,'" Fadell recalled of his experience at Apple and Nest. "And we'd say, 'No, no. Work day and night. It's two weeks.' But it was always this fixed bottleneck."The consequences ripple through hardware organizations. Firmware teams sit idle waiting for physical boards to test their code. Validation engineers cannot begin debugging. Product launches slip. According to Quilter's research, only about 10 percent of first board revisions work correctly, forcing expensive and time-consuming respins.Project Speedrun put Quilter's AI to the test with an 843-component computer that booted on the first tryProject Speedrun was designed to push the technology to its limits while producing an easily understood result: a working computer that could boot Linux, browse the internet, and run applications.The system consists of two boards based on NXP's i.MX 8M Mini reference platform, a processor architecture used in automotive infotainment, industrial automation, and machine vision applications.The main system-on-module contains a quad-core ARM processor running at 1.8 gigahertz, 2 gigabytes of LPDDR4 memory, and 32 gigabytes of eMMC storage. A companion baseboard provides connectivity including Ethernet, USB, HDMI, and audio.Together, the boards incorporate 843 components and 5,141 electrical connections, or "pins," routed across eight-layer circuit board stackups manufactured by Sierra Circuits in California. The minimum trace geometry reached 2 mils (two-thousandths of an inch) on the system-on-module — fine enough to require advanced high-density interconnect manufacturing techniques.Quilter's AI completed the layout with approximately 98 percent routing coverage and zero design rule violations. Both boards passed power-on testing and successfully booted Debian Linux on the first attempt."We made an entire computer to demonstrate that this technology works," Nesterenko said. "We took something that's typically quoted at 400 to 450 hours, automated the vast majority of it, and reduced it to about 30 to 40 hours of cleanup time."The cleanup time is work that human engineers still perform: reviewing the AI's output, fixing any issues, and preparing final fabrication files. But even with that overhead, the total elapsed time from schematic to fabricated boards collapsed from the typical 11 weeks to a single week.Unlike ChatGPT, Quilter's AI learns by playing billions of games against the laws of physicsQuilter's technical approach differs fundamentally from the large language models that have dominated recent AI headlines. Where systems like GPT-5 or Claude learn to predict text based on massive training datasets of human writing, Quilter's AI learns by playing what amounts to an elaborate game against the laws of physics."Language models don't apply to us because this is not a language problem," Nesterenko explained. "If you ask it to actually create a blueprint, it has no training data for that. It has no context for that."The company also rejected the seemingly obvious approach of training on examples of human-designed boards. Nesterenko cited three reasons: humans make frequent errors (explaining why most boards require revisions), the best designs are locked inside large companies unwilling to share proprietary data, and training on human examples would cap the AI's performance at human levels.Instead, Quilter built what Nesterenko describes as a "game" where the AI agent makes sequential decisions — place this component here, route this trace there — and receives feedback based on whether the resulting design satisfies electromagnetic, thermal, and manufacturing constraints."What you're really changing is not the probability of getting a very specific outcome of the model, but the probability of choosing a certain action based on that experience," Nesterenko said.The approach mirrors DeepMind's progression with its Go-playing systems. The original AlphaGo learned from human games, but its successor AlphaZero learned purely through self-play and ultimately surpassed human capability. Quilter harbors similar ambitions."In the long term, to come up with better designs for circuit boards than humans have ever tried to do," Nesterenko said.Fadell drew a parallel to an earlier technological transition: "I remember this with assembly. You had assembly and compilers, and engineers would say, 'I can't trust the compiler. I'm going to do the loop unrolling myself.' Now very, very few people write any assembly."He expects PCB design to follow a similar arc: "I hope the same thing happens with PCB design. Sure, a few people will hold out, but these tools are going to get so good that everyone else will move on."Fadell and Nesterenko spent months solving a delicate problem: how to automate design without stripping engineers of controlAutomating a task that skilled professionals have performed manually for decades raises an obvious question: how do engineers maintain control over designs that will ultimately ship in products where reliability matters?Fadell said he spent significant time with Nesterenko working through this tension. The solution, he said, lies in allowing users to choose their level of involvement at each stage of the process."If you're a control freak, you can be a control freak. If you want to say 'just do it for me,' you can do that too—and everything in between," Fadell said. "You can walk through each phase of the design and get involved wherever you want, or let the AI handle it."The workflow breaks into three phases: setup, where engineers define constraints and requirements; execution, where the AI generates candidate layouts; and cleanup, where humans review and refine the output. Engineers can intervene at any point, adjusting constraints and regenerating designs until they're satisfied."This is something Tony and I talk about a lot," Nesterenko said. "How do we give users control while still automating most of the work?"Quilter's technology has clear boundaries: 10,000 pins and 10 gigahertz mark the current limitsThe technology has clear limitations. Quilter currently handles boards with up to roughly 10,000 pins — sufficient for a wide range of applications but well short of the most complex designs, which can exceed 100,000 connections.Physics complexity also creates boundaries. The system handles high-speed communications up to approximately 10 gigahertz, covering typical consumer electronics and many industrial applications. But advanced systems like sophisticated radar, which can operate at 100 gigahertz, exceed current capabilities."There are boards where Quilter won't make enough progress to make the cleanup time worthwhile," Nesterenko acknowledged. "We're just not that helpful yet with the most advanced, sophisticated designs."The company has focused initially on categories where speed matters more than extreme complexity: test fixtures, evaluation boards, design validation boards, and environmental test hardware. These boards often sit in long queues behind higher-priority production designs, delaying engineering programs.The company bets that engineers will pay the same price for a 10x speed improvementQuilter prices its service by pin count, matching the billing conventions that already exist when companies hire outside layout specialists. The pitch to customers is cost neutrality with a ten-fold improvement in speed."We're going to charge you roughly the same that you would pay for the pins that you would with a person," Nesterenko said. "But the reason you choose us is that we do this 10 times faster."For a company waiting three months for a board layout, receiving it in a week fundamentally changes what's possible. Engineering teams can run multiple design experiments in parallel. Firmware developers get hardware faster. Products reach the market sooner.The company offers free access for hobbyists, students, and small businesses with less than $50,000 in revenue — a strategy to build familiarity while targeting enterprise customers for commercial revenue.The iPod creator waited years to attach his name to Quilter — until he could prove the technology actually worksFadell said he chose this moment to publicly acknowledge his investment because the Project Speedrun demonstration provides concrete evidence that the technology works."It's not about being comfortable—I was always comfortable with the team," he said. "This was about waiting until we had something you could hang your hat on. Now I can say, 'I've used the tool. I've seen it.'"He contrasted his approach with typical investor announcements: "Every investor goes, I invested in this, it's gonna change the world. It's like, no, I know better. I've used the tool. I know people who use it. I asked my startups to use the tool."Fadell's involvement goes beyond capital. He described email exchanges running to "a dozen pages of details" covering product design, user experience, enterprise sales, and technical architecture."Of all the investors I work with, Tony by far goes deepest with me on the product side," Nesterenko said.If Quilter succeeds, it could unlock a new generation of hardware startups that were never economically viable beforeThe stakes extend far beyond one company's product roadmap. If Quilter's technology scales, it could fundamentally alter the economics of building physical products.Fadell argued that hardware development has historically moved slowly because each step in the process — schematic design, PCB layout, manufacturing, assembly — created friction. Other innovations have already smoothed schematic tools and manufacturing. Layout remained the stubborn holdout."Once you shrink that from weeks to hours, you can iterate so much faster because all the other friction in the chain has been reduced," Fadell said.He predicted the technology would eventually extend upstream into schematic design itself, with AI that understands both logical connections and physical constraints helping engineers avoid problems earlier in the process.At MIT, where Fadell now spends time, he encounters would-be founders who have abandoned hardware ambitions because the process seemed insurmountable."I talk to professors and startup founders, and they say, 'I'm never doing hardware. It's too hard,'" he said. "I hope we can make it easier for more people to jump in and try things."Industry veterans remain skeptical. Auto-routing tools — previous attempts at automation — became notorious for producing unusable results, spawning T-shirts proclaiming engineers would "never trust the auto-router."Nesterenko has seen the skepticism dissolve in real time. He described a recent meeting with executives from a major customer who came to discuss Quilter's capabilities. As the conversation unfolded, one executive picked up the Project Speedrun boards and began photographing them from every angle, turning them over in his hands."He was just fascinated by the fact that this is possible now," Nesterenko said.The question is no longer whether AI can design circuit boards. A working Linux computer, assembled from 843 components and booted on the first attempt, answers that definitively. The question now is what engineers will build when layout stops being the bottleneck — when hardware, as Fadell put it, finally "moves at the speed of thought."On that point, Nesterenko offered a prediction. "If you ask the average electrical engineer today whether automation or AI could at all help with the board of this complexity, they would say no," he said. For decades, they would have been right. As of last week, they're not.
- [How Hud's runtime sensor cut triage time from 3 hours to 10 minutes](https://venturebeat.com/ai/how-huds-runtime-sensor-cut-triage-time-from-3-hours-to-10-minutes) — 06:00 · VentureBeat AI
  > Engineering teams are generating more code with AI agents than ever before. But they're hitting a wall when that code reaches production.The problem isn't necessarily the AI-generated code itself. It's that traditional monitoring tools generally struggle to provide the granular, function-level data AI agents need to understand how code actually behaves in complex production environments. Without that context, agents can't detect issues or generate fixes that account for production reality.It's a challenge that startup Hud is looking to help solve with the launch of its runtime code sensor on Wednesday. The company's eponymous sensor runs alongside production code, automatically tracking how every function behaves, giving developers a heads-up on what's actually occurring in deployment."Every software team building at scale faces the same fundamental challenge: building high-quality products that work well in the real world," Roee Adler, CEO and founder of Hud, told VentureBeat in an exclusive interview.  "In the new era of AI-accelerated development, not knowing how code behaves in production becomes an even bigger part of that challenge."What software developers are struggling with The pain points that developers are facing are fairly consistent across engineering organizations. Moshik Eilon, group tech lead at Monday.com, oversees 130 engineer and describes a familiar frustration with traditional monitoring tools."When you get an alert, you usually end up checking an endpoint that has an error rate or high latency, and you want to drill down to see the downstream dependencies," Eilon told VentureBeat. "A lot of times it's the actual application, and then it's a black box. You just get 80% downstream latency on the application."The next step typically involves manual detective work across multiple tools. Check the logs. Correlate timestamps. Try to reconstruct what the application was doing. For novel issues deep in a large codebase, teams often lack the exact data they need.Daniel Marashlian, CTO and co-founder at Drata, saw his engineers spending hours on what he referred to as an "investigation tax." "They were mapping a generic alert to a specific code owner, then digging through logs to reconstruct the state of the application," Marashlian told VentureBeat. "We wanted to eliminate that so our team could focus entirely on the fix rather than the discovery."Drata's architecture compounds the challenge. The company integrates with numerous external services to deliver automated compliance, which creates sophisticated investigations when issues arise. Engineers trace behavior across a very large codebase spanning risk, compliance, integrations, and reporting modules.Marashlian identified three specific problems that drove Drata toward investing in runtime sensors. The first issue was the cost of context switching. "Our data was scattered, so our engineers had to act as human bridges between disconnected tools," he said.The second issue, he noted, is alert fatigue. "When you have a complex distributed system, general alert channels become a constant stream of background noise, what our team describes as a 'ding, ding, ding' effect that eventually gets ignored," Marashlian said.The third key driver was a need to integrate with the company's AI strategy."An AI agent can write code, but it cannot fix a production bug if it can't see the runtime variables or the root cause," Marashlian said.Why traditional APMs can't solve the problem easilyEnterprises have long relied on a class of tools and services known as Application Performance Monitoring (APM). With the current pace of agentic AI development and modern development workflows, both Monday.com and Drata simply were not able to get the required visibility from existing APM tools."If I would want to get this information from Datadog or from CoreLogix, I would just have to ingest tons of logs or tons of spans, and I would pay a lot of money," Eilon said. Eilon noted that Monday.com used very low sampling rates because of cost constraints. That meant they often missed the exact data needed to debug issues.Traditional application performance monitoring tools also require prediction, which is a problem because sometimes a developer just doesn't know what they don't know."Traditional observability requires you to anticipate what you'll need to debug," Marashlian said. "But when a novel issue surfaces, especially deep within a large, complex codebase, you're often missing the exact data you need."Drata evaluated several solutions in the AI site reliability engineering and automated incident response categories and didn't find what was needed.  "Most tools we evaluated were excellent at managing the incident process, routing tickets, summarizing Slack threads, or correlating graphs," he said. "But they often stopped short of the code itself. They could tell us 'Service A is down,' but they couldn't tell us why specifically."Another common capability in some tools including error monitors like Sentry is the ability to capture exceptions. The challenge, according to Adler, is that being made aware of exceptions is nice, but that doesn't connect them to business impact or provide the execution context AI agents need to propose fixes.How runtime sensors work differentlyRuntime sensors push intelligence to the edge where code executes. Hud's sensor runs as an SDK that integrates with a single line of code. It sees every function execution but only sends lightweight aggregate data unless something goes wrong.When errors or slowdowns occur, the sensor automatically gathers deep forensic data including HTTP parameters, database queries and responses, and full execution context. The system establishes performance baselines within a day and can alert on both dramatic slowdowns and outliers that percentile-based monitoring misses."Now we just get all of this information for all of the functions regardless of what level they are, even for underlying packages," Eilon said. "Sometimes you might have an issue that is very deep, and we still see it pretty fast."The platform delivers data through four channels:Web application for centralized monitoring and analysisIDE extensions for VS Code, JetBrains and Cursor that surface production metrics directly where code is writtenMCP server that feeds structured data to AI coding agentsAlerting system that identifies issues without manual configurationThe MCP server integration is critical for AI-assisted development. Monday.com engineers now query production behavior directly within Cursor. "I can just ask Cursor a question: Hey, why is this endpoint slow?" Eilon said. "When it uses the Hud MCP, I get all of the granular metrics, and this function is 30% slower since this deployment. Then I can also find the root cause."This changes the incident response workflow. Instead of starting in Datadog and drilling down through layers, engineers start by asking an AI agent to diagnose the issue. The agent has immediate access to function-level production data.From voodoo incidents to minutes-long fixesThe shift from theoretical capability to practical impact becomes clear in how engineering teams actually use runtime sensors. What used to take hours or days of detective work now resolves in minutes."I'm used to having these voodoo incidents where there is a CPU spike and you don't know where it came from," Eilon said. "A few years ago, I had such an incident and I had to build my own tool that takes the CPU profile and the memory dump. Now I just have all of the function data and I've seen engineers just solve it so fast."At Drata, the quantified impact is dramatic. The company built an internal /triage command that support engineers run within their AI assistants to instantly identify root causes. Manual triage work dropped from approximately 3 hours per day to under 10 minutes. Mean time to resolution improved by approximately 70%.The team also generates a daily "Heads Up" report of quick-win errors. Because the root cause is already captured, developers can fix these issues in minutes. Support engineers now perform forensic diagnosis that previously required a senior developer. Ticket throughput increased without expanding the L2 team.Where this technology fitsRuntime sensors occupy a distinct space from traditional APMs, which excel at service-level monitoring but struggle with granular, cost-effective function-level data. They differ from error monitors that capture exceptions without business context.The technical requirements for supporting AI coding agents differ from human-facing observability. Agents need structured, function-level data they can reason over. They can't parse and correlate raw logs the way humans do. Traditional observability also assumes you can predict what you'll need to debug and instrument accordingly. That approach breaks down with AI-generated code where engineers may not deeply understand every function."I think we're entering a new age of AI-generated code and this puzzle, this jigsaw puzzle of a new stack emerging," Adler said. "I just don't think that the cloud computing observability stack is going to fit neatly into how the future looks like."What this means for enterprisesFor organizations already using AI coding assistants like GitHub Copilot or Cursor, runtime intelligence provides a safety layer for production deployments. The technology enables what Monday.com calls "agentic investigation" rather than manual tool-hopping.The broader implication relates to trust.  "With AI-generated code, we are getting much more AI-generated code, and engineers start not knowing all of the code," Eilon said. Runtime sensors bridge that knowledge gap by providing production context directly in the IDE where code is written.For enterprises looking to scale AI code generation beyond pilots, runtime intelligence addresses a fundamental problem. AI agents generate code based on assumptions about system behavior. Production environments are complex and surprising. Function-level behavioral data captured automatically from production gives agents the context they need to generate reliable code at scale.Organizations should evaluate whether their existing observability stack can cost-effectively provide the granularity AI agents require. If achieving function-level visibility requires dramatically increasing ingestion costs or manual instrumentation, runtime sensors may offer a more sustainable architecture for AI-accelerated development workflows already emerging across the industry.
- [The Download: a controversial proposal to solve climate change, and our future grids](https://www.technologyreview.com/2025/12/10/1129186/the-download-a-controversial-proposal-to-solve-climate-change-and-our-future-grids/) — 05:14 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. How one controversial startup hopes to cool the planet Stardust Solutions believes that it can solve climate change—for a price.  The Israel-based geoengineering startup has said it expects nations will soon pay it…
- [Securing VMware workloads in regulated industries](https://www.technologyreview.com/2025/12/10/1128475/securing-vmware-workloads-in-regulated-industries/) — 02:11 · MIT Technology Review (AI)
  > At a regional hospital, a cardiac patient’s lab results sit behind layers of encryption, accessible to his surgeon but shielded from those without strictly need-to-know status. Across the street at a credit union, a small business owner anxiously awaits the all-clear for a wire transfer, unaware that fraud detection systems have flagged it for further…
- [How one controversial startup hopes to cool the planet](https://www.technologyreview.com/2025/12/10/1129079/how-one-controversial-startup-hopes-to-cool-the-planet/) — 02:00 · MIT Technology Review (AI)
  > Stardust Solutions believes that it can solve climate change—for a price. The Israel-based geoengineering startup has said it expects  nations will soon pay it more than a billion dollars a year to launch specially equipped aircraft into the stratosphere. Once they’ve reached the necessary altitude, those planes will disperse particles engineered to reflect away enough…
