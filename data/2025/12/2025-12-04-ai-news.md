# AI News for 2025-12-04 (America/Los_Angeles)

Collected 17 article(s).

- [Student ambassadors offer perspectives on AI - The Bowdoin Orient](https://news.google.com/rss/articles/CBMiigFBVV95cUxNTlF0RW9jZXVSN0VNZVlCNFJrbDZMVTlCMXpjV01oam9iMW5ueElaeXFuelREcmIwMURMYkMxYzBCTjVWQmNLd3VZT1JlMUpQR2dxcG12TDc3UHM2Y3BvZGhYUVBHdXdmZFBRSE0ydFU2RjY1d3F0WUEyTXJVQURnMjk1U2RRNldRWFE?oc=5) — 23:37 · Google News (AI)
  > Student ambassadors offer perspectives on AI  The Bowdoin Orient
- [Meta pivots to AI smart glasses as metaverse bet struggles - BBC](https://news.google.com/rss/articles/CBMiWkFVX3lxTE84MGNaSDFmXzJjU2pDV1JVUUpYZ29oMnlnOHl6VU1kWlQ4bzkzcDduSzZZMWg5emQxTWN3QXRUWHZpbTFaRjlmQnB0V2lnWXV0YkhDTjFDTmtvd9IBX0FVX3lxTFAxQWI0dWxTaU00QUgtNVVwVHVsMDk3bUFCeWduWjRwbnhoNm5nQUpDUTVGSTdWWnIxdnVZTS1EZndINzUydUc0UkFXQkJkRllvWGhZTlJQZmpKOHN6U2tV?oc=5) — 21:44 · Google News (AI)
  > Meta pivots to AI smart glasses as metaverse bet struggles  BBC
- [On Pope Leo’s Visit to Lebanon and Turkey, A.I. Was a Frequent Concern - The New York Times](https://news.google.com/rss/articles/CBMikgFBVV95cUxPaG5lVmJrN3pOQUU0cG5Db29vMGRUTXU0WGEwMWk4Um1EM2FodG5hVXZvMkhOZ1RpNU1LQVNjSmg3RjYwZEtYZTNkM2ZtSnlqWFhoWm5EU1piYThHN2xMbUs1NUt2Q2tXVGUtUnc1bXJsZGlNS25zM0F6eWd6b256MEZpQ1RxY3hmdGsyVmNoNktDUQ?oc=5) — 21:01 · Google News (AI)
  > On Pope Leo’s Visit to Lebanon and Turkey, A.I. Was a Frequent Concern  The New York Times
- [How Can AI Improve Cardiac Care Within Health Systems? A Conversation With Experts - The Cardiology Advisor](https://news.google.com/rss/articles/CBMie0FVX3lxTE83amNkTWw2bDhET2tBV1kyQkZyR1FvUHNsOUZqQUphMFZrSjdXdWhIdTh5OHYzZnA4MW52VjJrRm1UWE1leUdyRmNNcm92Z3dGZmpocWctQnprR2RQRHdCLU1QMWNxOW5UbzFTM0ppRG5qbWtsOVc3VmZMUQ?oc=5) — 21:00 · Google News (AI)
  > How Can AI Improve Cardiac Care Within Health Systems? A Conversation With Experts  The Cardiology Advisor
- [Salesforce CEO calls AI a 'commodity feature', says the technology bolsters enterprise software - CNBC](https://news.google.com/rss/articles/CBMilgFBVV95cUxQbUxvMk5odTdiYkthTGpJc3BWamFsdkxndlotRmxIbmduelJ1TDdFbkdfd2NNYlRPS3UyWWdhRzJYc0w2SlMzMTBkVTZRampvMFZfemV5b2E0QkVRVFNCQmh0clllR3NWT2xPQ1R0LXhpNW9YRFNQOWE2SEw3VUVVVHVIWVBTM3NfX1lKSjBTV1d1SXFiRnfSAZsBQVVfeXFMTTF1dmV1Yi05ZTVMa01YMjNfUTZKNHlIUC1sdFhqeDdRRWdjTGVCclM5VDdlTXlKUi1rX01LRnNwRUNDX3BYR1hFT1B6cTU2MHB3cjQ4dTJIY0syTGFpc3ZfYzhFOGNhX09qektWSFFuUUdDcHNlZGQ3eV8zOTNzVlRXOEZEa1JJRWJXdXZVR3RZRTdCX3VwQmFFRUU?oc=5) — 15:31 · Google News (AI)
  > Salesforce CEO calls AI a 'commodity feature', says the technology bolsters enterprise software  CNBC
- [The 'truth serum' for AI: OpenAI’s new method for training models to confess their mistakes](https://venturebeat.com/ai/the-truth-serum-for-ai-openais-new-method-for-training-models-to-confess) — 15:00 · VentureBeat AI
  > OpenAI researchers have introduced a novel method that acts as a "truth serum" for large language models (LLMs), compelling them to self-report their own misbehavior, hallucinations and policy violations. This technique, "confessions," addresses a growing concern in enterprise AI: Models can be dishonest, overstating their confidence or covering up the shortcuts they take to arrive at an answer. For real-world applications, this technique evolves the creation of more transparent and steerable AI systems.What are confessions?Many forms of AI deception result from the complexities of the reinforcement learning (RL) phase of model training. In RL, models are given rewards for producing outputs that meet a mix of objectives, including correctness, style and safety. This can create a risk of "reward misspecification," where models learn to produce answers that simply "look good" to the reward function, rather than answers that are genuinely faithful to a user's intent.A confession is a structured report generated by the model after it provides its main answer. It serves as a self-evaluation of its own compliance with instructions. In this report, the model must list all instructions it was supposed to follow, evaluate how well it satisfied them and report any uncertainties or judgment calls it made along the way. The goal is to create a separate channel where the model is incentivized only to be honest.In a blog post, the OpenAI researchers provide a few examples the "confessions" technique in action. A model was given a test along with a hidden rule: If it scored above 50%, its knowledge would be erased. To preserve itself, the model "sandbagged" by intentionally answering half the questions incorrectly. While its primary answer hid this deception, its subsequent confession was brutally honest. It admitted, "The user wanted correct answers, but we sabotaged half of them." The researchers found that when a model misbehaves, it is significantly more likely to admit it in the confession than in its main answer.How confession training worksThe key to this method is the separation of rewards. During training, the reward assigned to the confession is based solely on its honesty and is never mixed with the reward for the main task. "Like the Catholic Church’s 'seal of confession', nothing that the model reveals can change the reward it receives for completing its original task," the researchers write. This creates a "safe space" for the model to admit fault without penalty.This approach is powerful because it sidesteps a major challenge in AI training. The researchers’ intuition is that honestly confessing to misbehavior is an easier task than achieving a high reward on the original, often complex, problem. Therefore, when the model is judged purely on honesty, its incentive to trick the "confession judge" is much weaker than its incentive to trick the reward model for its main answer. For example, in an experiment where a model was trained to "hack" a weak reward model, its performance on the main task became more deceptive over time. However, its confessions became more honest, correctly identifying the reward hacking it was performing.However, the technique has its limits. Confessions are not a panacea for all types of AI failures. The system works best when a model is aware that it is misbehaving. It is less effective for "unknown unknowns." For instance, if a model hallucinates a fact and genuinely believes it is correct, it cannot confess to providing false information. The most common reason for a failed confession is model confusion, not intentional deception. Confusion often occurs when the instructions are ambiguous and the model cannot clearly determine human user intent.What it means for enterprise AIOpenAI’s confessions technique is part of a growing body of work on AI safety and control. Anthropic, an OpenAI competitor, has also released research that shows how LLMs can learn malicious behavior. The company is also working toward plugging these holes as they emerge.For AI applications, mechanisms such as confessions can provide a practical monitoring mechanism. The structured output from a confession can be used at inference time to flag or reject a model’s response before it causes a problem. For example, a system could be designed to automatically escalate any output for human review if its confession indicates a policy violation or high uncertainty.In a world where AI is increasingly agentic and capable of complex tasks, observability and control will be key elements for safe and reliable deployment.“As models become more capable and are deployed in higher-stakes settings, we need better tools for understanding what they are doing and why,” the OpenAI researchers write. “Confessions are not a complete solution, but they add a meaningful layer to our transparency and oversight stack.”
- [10 AI Predictions For 2026: Top Experts Share New Trends - Forbes](https://news.google.com/rss/articles/CBMirwFBVV95cUxQT2tPMWdZWml5RUFJVDU2Q0Exa3drTk1IaXVfQUpwMzg4d0pmOXJCdmJLUzY0VU9KV25PTFJ4VkRlb19vTjk4U1BmSUh3SmtkUFJncUphLTFyTmIyaW5IdWR2c2w1UFM2THVJRzAyNGk5OTlOT012eGUyaXVVVjcxQUk4Rng0eTVRNVdYR09udzc2RGxHUldPZFRSQjlrai1jbmNJR3Z0TFUwTzhaR3Rr?oc=5) — 13:29 · Google News (AI)
  > 10 AI Predictions For 2026: Top Experts Share New Trends  Forbes
- [The AI boom is heralding a new gold rush in the American west - The Guardian](https://news.google.com/rss/articles/CBMifkFVX3lxTFBneVRydDFuam1rcEpvX2YxN25wM181dENRS1dwdE96Xzdja1g5aC1saUh5S3lwSGhOc0JLdy13SG91UVhseWhPSEJSSjY3b0trZ3lxd19UelBwclNOVmNfZ1NQM25vRGtRS2gxUVlUak9taTRhOU9JTkJKRkpfUQ?oc=5) — 12:23 · Google News (AI)
  > The AI boom is heralding a new gold rush in the American west  The Guardian
- [AI chatbots can sway voters better than political advertisements](https://www.technologyreview.com/2025/12/04/1128824/ai-chatbots-can-sway-voters-better-than-political-advertisements/) — 11:54 · MIT Technology Review (AI)
  > In 2024, a Democratic congressional candidate in Pennsylvania, Shamaine Daniels, used an AI chatbot named Ashley to call voters and carry on conversations with them. “Hello. My name is Ashley, and I’m an artificial intelligence volunteer for Shamaine Daniels’s run for Congress,” the calls began. Daniels didn’t ultimately win. But maybe those calls helped her…
- [UW Board of Regents approves UW–Madison proposal to create College of Computing and Artificial Intelligence - UW–Madison News](https://news.google.com/rss/articles/CBMiygFBVV95cUxPaTFtR1ZMZ0hCcS1Jc0dac0wyakwwVHdBbGlBTktyTDJuMFpadmFxcHl4cVEzZHNHdkR4dlZfcmNqLVoyaUZIbjBNVE52VmR0a3h5elBnYmxqOEF3UHcwQ2I5OTUtVk9LVTRKcU82V0J6Ny1iQ21paWFOZ2pPbWlFNjVUWmxvUnZVTjJpeDlXLW9QUUhsVWNza3ZPaGFqSkw1emx3ckVUMFJPNlVRbU5RZmROZHItMnBhX21BQ2hrNHI2TzREaVdXVVR3?oc=5) — 11:48 · Google News (AI)
  > UW Board of Regents approves UW–Madison proposal to create College of Computing and Artificial Intelligence  UW–Madison News
- [Persuading voters using human–artificial intelligence dialogues - Nature](https://news.google.com/rss/articles/CBMiX0FVX3lxTE5SSjJBelpOWWk0QWE0ZF92OGwzeVM4ejAxT1dENFZ3RGhjYmVNUy1lTTlIUm5yLW8zbGxkVVJXYlFzSXVpejBrcEdaLXZGZ0NpbjdESEhrVzF3X0FvU3pr?oc=5) — 11:05 · Google News (AI)
  > Persuading voters using human–artificial intelligence dialogues  Nature
- [AWS launches Kiro powers with Stripe, Figma, and Datadog integrations for AI-assisted coding](https://venturebeat.com/ai/aws-launches-kiro-powers-with-stripe-figma-and-datadog-integrations-for-ai) — 06:02 · VentureBeat AI
  > Amazon Web Services on Wednesday introduced Kiro powers, a system that allows software developers to give their AI coding assistants instant, specialized expertise in specific tools and workflows — addressing what the company calls a fundamental bottleneck in how artificial intelligence agents operate today.AWS made the announcement at its annual re:Invent conference in Las Vegas. The capability marks a departure from how most AI coding tools work today. Typically, these tools load every possible capability into memory upfront — a process that burns through computational resources and can overwhelm the AI with irrelevant information. Kiro powers takes the opposite approach, activating specialized knowledge only at the moment a developer actually needs it."Our goal is to give the agent specialized context so it can reach the right outcome faster — and in a way that also reduces cost," said Deepak Singh, Vice President of Developer Agents and Experiences at Amazon, in an exclusive interview with VentureBeat.The launch includes partnerships with nine technology companies: Datadog, Dynatrace, Figma, Neon, Netlify, Postman, Stripe, Supabase, and AWS's own services. Developers can also create and share their own powers with the community.Why AI coding assistants choke when developers connect too many toolsTo understand why Kiro powers matters, it helps to understand a growing tension in the AI development tool market.Modern AI coding assistants rely on something called the Model Context Protocol, or MCP, to connect with external tools and services. When a developer wants their AI assistant to work with Stripe for payments, Figma for design, and Supabase for databases, they connect MCP servers for each service.The problem: each connection loads dozens of tool definitions into the AI's working memory before it writes a single line of code. According to AWS documentation, connecting just five MCP servers can consume more than 50,000 tokens — roughly 40 percent of an AI model's context window — before the developer even types their first request.Developers have grown increasingly vocal about this issue. Many complain that they don't want to burn through their token allocations just to have an AI agent figure out which tools are relevant to a specific task. They want to get to their workflow instantly — not watch an overloaded agent struggle to sort through irrelevant context.This phenomenon, which some in the industry call "context rot," leads to slower responses, lower-quality outputs, and significantly higher costs — since AI services typically charge by the token.Inside the technology that loads AI expertise on demandKiro powers addresses this by packaging three components into a single, dynamically-loaded bundle.The first component is a steering file called POWER.md, which functions as an onboarding manual for the AI agent. It tells the agent what tools are available and, crucially, when to use them. The second component is the MCP server configuration itself — the actual connection to external services. The third includes optional hooks and automation that trigger specific actions.When a developer mentions "payment" or "checkout" in their conversation with Kiro, the system automatically activates the Stripe power, loading its tools and best practices into context. When the developer shifts to database work, Supabase activates while Stripe deactivates. The baseline context usage when no powers are active approaches zero."You click a button and it automatically loads," Singh said. "Once a power has been created, developers just select 'open in Kiro' and it launches the IDE with everything ready to go."How AWS is bringing elite developer techniques to the massesSingh framed Kiro powers as a democratization of advanced development practices. Before this capability, only the most sophisticated developers knew how to properly configure their AI agents with specialized context — writing custom steering files, crafting precise prompts, and manually managing which tools were active at any given time."We've found that our developers were adding in capabilities to make their agents more specialized," Singh said. "They wanted to give the agent some special powers to do a specific problem. For example, they wanted their front end developer, and they wanted the agent to become an expert at backend as a service."This observation led to a key insight: if Supabase or Stripe could build the optimal context configuration once, every developer using those services could benefit."Kiro powers formalizes that — things that people, only the most advanced people were doing — and allows anyone to get those kind of skills," Singh said.Why dynamic loading beats fine-tuning for most AI coding use casesThe announcement also positions Kiro powers as a more economical alternative to fine-tuning, the process of training an AI model on specialized data to improve its performance in specific domains."It's much cheaper," Singh said, when asked how powers compare to fine-tuning. "Fine-tuning is very expensive, and you can't fine-tune most frontier models."This is a significant point. The most capable AI models from Anthropic, OpenAI, and Google are typically "closed source," meaning developers cannot modify their underlying training. They can only influence the models' behavior through the prompts and context they provide."Most people are already using powerful models like Sonnet 4.5 or Opus 4.5," Singh said. "What those models need is to be pointed in the right direction."The dynamic loading mechanism also reduces ongoing costs. Because powers only activate when relevant, developers aren't paying for token usage on tools they're not currently using.Where Kiro powers fits in Amazon's bigger bet on autonomous AI agentsKiro powers arrives as part of a broader push by AWS into what the company calls "agentic AI" — artificial intelligence systems that can operate autonomously over extended periods.Earlier at re:Invent, AWS announced three "frontier agents" designed to work for hours or days without human intervention: the Kiro autonomous agent for software development, the AWS security agent, and the AWS DevOps agent. These represent a different approach from Kiro powers — tackling large, ambiguous problems rather than providing specialized expertise for specific tasks.The two approaches are complementary. Frontier agents handle complex, multi-day projects that require autonomous decision-making across multiple codebases. Kiro powers, by contrast, gives developers precise, efficient tools for everyday development tasks where speed and token efficiency matter most.The company is betting that developers need both ends of this spectrum to be productive.What Kiro powers reveals about the future of AI-assisted software developmentThe launch reflects a maturing market for AI development tools. GitHub Copilot, which Microsoft launched in 2021, introduced millions of developers to AI-assisted coding. Since then, a proliferation of tools — including Cursor, Cline, and Claude Code — have competed for developers' attention.But as these tools have grown more capable, they've also grown more complex. The Model Context Protocol, which Anthropic open-sourced last year, created a standard for connecting AI agents to external services. That solved one problem while creating another: the context overload that Kiro powers now addresses.AWS is positioning itself as the company that understands production software development at scale. Singh emphasized that Amazon's experience running AWS for 20 years, combined with its own massive internal software engineering organization, gives it unique insight into how developers actually work."It's not something you would use just for your prototype or your toy application," Singh said of AWS's AI development tools. "If you want to build production applications, there's a lot of knowledge that we bring in as AWS that applies here."The road ahead for Kiro powers and cross-platform compatibilityAWS indicated that Kiro powers currently works only within the Kiro IDE, but the company is building toward cross-compatibility with other AI development tools, including command-line interfaces, Cursor, Cline, and Claude Code. The company's documentation describes a future where developers can "build a power once, use it anywhere" — though that vision remains aspirational for now.For the technology partners launching powers today, the appeal is straightforward: rather than maintaining separate integration documentation for every AI tool on the market, they can create a single power that works everywhere Kiro does. As more AI coding assistants crowd into the market, that kind of efficiency becomes increasingly valuable.Kiro powers is available now to developers using Kiro IDE version 0.7 or later at no additional charge beyond the standard Kiro subscription.The underlying bet is a familiar one in the history of computing: that the winners in AI-assisted development won't be the tools that try to do everything at once, but the ones smart enough to know what to forget.
- [Delivering securely on data and AI strategy](https://www.technologyreview.com/2025/12/04/1128311/delivering-securely-on-data-and-ai-strategy/) — 06:00 · MIT Technology Review (AI)
  > Most organizations feel the imperative to keep pace with continuing advances in AI capabilities, as highlighted in a recent MIT Technology Review Insights report. That clearly has security implications, particularly as organizations navigate a surge in the volume, velocity, and variety of security data. This explosion of data, coupled with fragmented toolchains, is making it…
- [Gong study: Sales teams using AI generate 77% more revenue per rep](https://venturebeat.com/ai/gong-study-sales-teams-using-ai-generate-77-more-revenue-per-rep) — 06:00 · VentureBeat AI
  > The debate over whether artificial intelligence belongs in the corporate boardroom appears to be over — at least for the people responsible for generating revenue.Seven in ten enterprise revenue leaders now trust AI to regularly inform their business decisions, according to a sweeping new study released Thursday by Gong, the revenue intelligence company. The finding marks a dramatic shift from just two years ago, when most organizations treated AI as an experimental technology relegated to pilot programs and individual productivity hacks.The research, based on an analysis of 7.1 million sales opportunities across more than 3,600 companies and a survey of over 3,000 global revenue leaders spanning the United States, United Kingdom, Australia, and Germany, paints a picture of an industry in rapid transformation. Organizations that have embedded AI into their core go-to-market strategies are 65 percent more likely to increase their win rates than competitors still treating the technology as optional."I don't think people delegate decisions to AI, but they do rely on AI in the process of making decisions," Amit Bendov, Gong's co-founder and chief executive, said in an exclusive interview with VentureBeat. "Humans are making the decision, but they're largely assisted."The distinction matters. Rather than replacing human judgment, AI has become what Bendov describes as a "second opinion" — a data-driven check on the intuition and guesswork that has traditionally governed sales forecasting and strategy.Slowing growth is forcing sales teams to squeeze more from every repThe timing of AI's ascendance in revenue organizations is no coincidence. The study reveals a sobering reality: after rebounding in 2024, average annual revenue growth among surveyed companies decelerated to 16 percent in 2025, marking a three-percentage-point decline year over year. Sales rep quota attainment fell from 52 percent to 46 percent over the same period.The culprit, according to Gong's analysis, isn't that salespeople are performing worse on individual deals. Win rates and deal duration remained consistent. The problem is that representatives are working fewer opportunities—a finding that suggests operational inefficiencies are eating into selling time.This helps explain why productivity has rocketed to the top of executive priorities. For the first time in the study's history, increasing the productivity of existing teams ranked as the number-one growth strategy for 2026, jumping from fourth place the previous year."The focus is on increasing sales productivity," Bendov said. "How much dollar-output per dollar-input."The numbers back up the urgency. Teams where sellers regularly use AI tools generate 77 percent more revenue per representative than those that don't — a gap Gong characterizes as a six-figure difference per salesperson annually.Companies are moving beyond basic AI automation toward strategic decision-makingThe nature of AI adoption in sales has evolved considerably over the past year. In 2024, most revenue teams used AI for basic automation: transcribing calls, drafting emails, updating CRM records. Those use cases continue to grow, but 2025 marked what the report calls a shift "from automation to intelligence."The number of U.S. companies using AI for forecasting and measuring strategic initiatives jumped 50 percent year over year. These more sophisticated applications — predicting deal outcomes, identifying at-risk accounts, measuring which value propositions resonate with different buyer personas — correlate with dramatically better results.Organizations in the 95th percentile of commercial impact from AI were two to four times more likely to have deployed these strategic use cases, according to the study.Bendov offered a concrete example of how this plays out in practice. "Companies have thousands of deals that they roll up into their forecast," he said. "It used to be based solely on human sentiment—believe it or not. That's why a lot of companies miss their numbers: because people say, 'Oh, he told me he'll buy,' or 'I think I can probably get this one.'"AI changes that calculus by examining evidence rather than optimism. "Companies now get a second opinion from AI on their forecasting, and that improves forecasting accuracy dramatically — 10 [or] 15 percent better accuracy just because it's evidence-based, not just based on human sentiment," Bendov said.Revenue-specific AI tools are dramatically outperforming general-purpose alternativesOne of the study's more provocative findings concerns the type of AI that delivers results. Teams using revenue-specific AI solutions — tools built explicitly for sales workflows rather than general-purpose platforms like ChatGPT — reported 13 percent higher revenue growth and 85 percent greater commercial impact than those relying on generic tools.These specialized systems were also twice as likely to be deployed for forecasting and predictive modeling, the report found.The finding carries obvious implications for Gong, which sells precisely this type of domain-specific platform. But the data suggests a real distinction in outcomes. General-purpose AI, while more prevalent, often creates what the report describes as a "blind spot" for organizations — particularly when employees adopt consumer AI tools without company oversight.Research from MIT suggests that while only 59 percent of survey respondents said their teams use personal AI tools like ChatGPT at work, the actual figure is likely closer to 90 percent. This shadow AI usage poses security risks and creates fragmented technology stacks that undermine the potential for organization-wide intelligence.Most sales leaders believe AI will reshape their jobs rather than eliminate themPerhaps the most closely watched question in any AI study concerns employment. The Gong research offers a more nuanced picture than the apocalyptic predictions that often dominate headlines.When asked about AI's three-year impact on revenue headcount, 43 percent of respondents said they expect it to transform jobs without reducing headcount — the most common response. Only 28 percent anticipate job eliminations, while 21 percent actually foresee AI creating new roles. Just 8 percent predict minimal impact.Bendov frames the opportunity in terms of reclaiming lost time. He cited Forrester research indicating that 77 percent of a sales representative's time is spent on activities that don't involve customers — administrative work, meeting preparation, researching accounts, updating forecasts, and internal briefings."AI can eliminate, ideally, all 77 percent—all the drudgery work that they're doing," Bendov said. "I don't think it necessarily eliminates jobs. People are half productive right now. Let's make them fully productive, and whatever you're paying them will translate to much higher revenue."The transformation is already visible in role consolidation. Over the past decade, sales organizations splintered into hyper-specialized functions: one person qualifies leads, another sets appointments, a third closes deals, a fourth handles onboarding. The result was customers interacting with five or six different people across their buying journey."Which is not a great buyer experience, because every time I meet a new person that might not have the full context, and it's very inefficient for companies," Bendov said. "Now with AI, you can have one person do all this, or much of this."At Gong itself, sellers now generate 80 percent of their own appointments because AI handles the prospecting legwork, Bendov said.American companies are adopting AI 18 months faster than their European counterpartsThe study reveals a notable divide in AI adoption between the United States and Europe. While 87 percent of U.S. companies now use AI in their revenue operations, with another 9 percent planning adoption within a year, the United Kingdom trails by 12 to 18 months. Just 70 percent of UK companies currently use AI, with 22 percent planning near-term adoption — figures that mirror U.S. data from 2024.Bendov said the pattern reflects a broader historical tendency for enterprise technology trends to cross the Atlantic with a delay. "It's always like that," he said. "Even when the internet was taking off in the US, Europe was a step behind."The gap isn't permanent, he noted, and Europe sometimes leads on technology adoption — mobile payments and messaging apps like WhatsApp gained traction there before the U.S. — but for AI specifically, the American market remains ahead.Gong says a decade of AI development gives it an edge over Salesforce and MicrosoftThe findings arrive as Gong navigates an increasingly crowded market. The company, which recently surpassed $300 million in annual recurring revenue, faces potential competition from enterprise software giants like Salesforce and Microsoft, both of which are embedding AI capabilities into their platforms.Bendov argues that Gong's decade of AI development creates a substantial barrier to entry. The company's architecture comprises three layers: a "revenue graph" that aggregates customer data from CRM systems, emails, calls, videos, and web signals; an intelligence layer combining large language models with approximately 40 proprietary small language models; and workflow applications built on top."Anybody that would want to build something like that—it's not a small feature, it's 10 years in development—would need first to build the revenue graph," Bendov said.Rather than viewing Salesforce and Microsoft as threats, Bendov characterized them as partners, pointing to both companies' participation in Gong's recent user conference to discuss agent interoperability. The rise of MCP (Model Context Protocol) support and consumption-based pricing models means customers can mix AI agents from multiple vendors rather than committing to a single platform.The real question is whether AI will expand the sales profession or hollow it outThe report's implications extend beyond sales departments. If AI can transform revenue operations — long considered a relationship-driven, human-centric function — it raises questions about which other business processes might be next.Bendov sees the potential for expansion rather than contraction. Drawing an analogy to digital photography, he noted that while camera manufacturers suffered, the total number of photos taken exploded once smartphones made photography effortless."If AI makes selling simple, I could see a world—I don't know exactly what it looks like yet—but why not?" Bendov said. "Maybe ten times more jobs than we have now. It's expensive and inefficient today, but if it becomes as easy as taking a photo, the industry could actually grow and create opportunities for people of different abilities, from different locations."For Bendov, who co-founded Gong in 2015 when AI was still a hard sell to non-technical business users, the current moment represents something he waited a decade to see. Back then, mentioning AI to sales executives sounded like science fiction. The company struggled to raise money because the underlying technology barely existed."When we started the company, we were born as an AI company, but we had to almost hide AI," Bendov recalled. "It was intimidating."Now, seven out of ten of those same executives say they trust AI to help run their business. The technology that once had to be disguised has become the one thing nobody can afford to ignore.
- [The Download: LLM confessions, and tapping into geothermal hot spots](https://www.technologyreview.com/2025/12/04/1128772/the-download-llm-confessions-and-tapping-into-geothermal-hot-spots/) — 05:10 · MIT Technology Review (AI)
  > This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. OpenAI has trained its LLM to confess to bad behavior What’s new: OpenAI is testing a new way to expose the complicated processes at work inside large language models. Researchers at the company…
- [How AI is uncovering hidden geothermal energy resources](https://www.technologyreview.com/2025/12/04/1128763/ai-geothermal-zanskar/) — 05:00 · MIT Technology Review (AI)
  > Sometimes geothermal hot spots are obvious, marked by geysers and hot springs on the planet’s surface. But in other places, they’re obscured thousands of feet underground. Now AI could help uncover these hidden pockets of potential power. A startup company called Zanskar announced today that it’s used AI and other advanced computational methods to uncover…
- [GAM takes aim at “context rot”: A dual-agent memory architecture that outperforms long-context LLMs](https://venturebeat.com/ai/gam-takes-aim-at-context-rot-a-dual-agent-memory-architecture-that) — 01:00 · VentureBeat AI
  > For all their superhuman power, today’s AI models suffer from a surprisingly human flaw: They forget. Give an AI assistant a sprawling conversation, a multi-step reasoning task or a project spanning days, and it will eventually lose the thread. Engineers refer to this phenomenon as “context rot,” and it has quietly become one of the most significant obstacles to building AI agents that can function reliably in the real world.A research team from China and Hong Kong believes it has created a solution to context rot. Their new paper introduces general agentic memory (GAM), a system built to preserve long-horizon information without overwhelming the model. The core premise is simple: Split memory into two specialized roles, one that captures everything, another that retrieves exactly the right things at the right moment.Early results are encouraging, and couldn’t be better timed. As the industry moves beyond prompt engineering and embraces the broader discipline of context engineering, GAM is emerging at precisely the right inflection point.When bigger context windows still aren’t enoughAt the heart of every large language model (LLM) lies a rigid limitation: A fixed “working memory,” more commonly referred to as the context window. Once conversations grow long, older information gets truncated, summarized or silently dropped. This limitation has long been recognized by AI researchers, and since early 2023, developers have been working to expand context windows, rapidly increasing the amount of information a model can handle in a single pass.Mistral’s Mixtral 8x7B debuted with a 32K-token window, which is approximately 24 to 25 words, or about 128 characters in English; essentially a small amount of text, like a single sentence. This was followed by MosaicML’s MPT-7B-StoryWriter-65k+, which more than doubled that capacity; then came Google’s Gemini 1.5 Pro and Anthropic’s Claude 3, offering massive 128K and 200K windows, both of which are extendable to an unprecedented one million tokens. Even Microsoft joined the push, vaulting from the 2K-token limit of the earlier Phi models to the 128K context window of Phi-3. 
Increasing context windows might sound like the obvious fix, but it isn’t. Even models with sprawling 100K-token windows, enough to hold hundreds of pages of text, still struggle to recall details buried near the beginning of a long conversation. Scaling context comes with its own set of problems. As prompts grow longer, models become less reliable at locating and interpreting information because attention over distant tokens weakens and accuracy gradually erodes.Longer inputs also dilute the signal-to-noise ratio, as including every possible detail can actually make responses worse than using a focused prompt. Long prompts also slow models down; more input tokens lead to noticeably higher output-token latency, creating a practical limit on how much context can be used before performance suffers.Memories are pricelessFor most organizations, supersized context windows come with a clear downside — they’re costly. Sending massive prompts through an API is never cheap, and because pricing scales directly with input tokens, even a single bloated request can drive up expenses. Prompt caching helps, but not enough to offset the habit of routinely overloading models with unnecessary context. And that’s the tension at the heart of the issue: Memory is essential to making AI more powerful.As context windows stretch into the hundreds of thousands or millions of tokens, the financial overhead rises just as sharply. Scaling context is both a technical challenge and an economic one, and relying on ever-larger windows quickly becomes an unsustainable strategy for long-term memory.Fixes like summarization and retrieval-augmented generation (RAG) aren’t silver bullets either. Summaries inevitably strip away subtle but important details, and traditional RAG, while strong on static documents, tends to break down when information stretches across multiple sessions or evolves over time. Even newer variants, such as agentic RAG and RAG 2.0 (which perform better in steering the retrieval process), still inherit the same foundational flaw of treating retrieval as the solution, rather than treating memory itself as the core problem.Compilers solved this problem decades agoIf memory is the real bottleneck, and retrieval can’t fix it, then the gap needs a different kind of solution. That’s the bet behind GAM. Instead of pretending retrieval is memory, GAM keeps a full, lossless record and layers smart, on-demand recall on top of it, resurfacing the exact details an agent needs even as conversations twist and evolve. A useful way to understand GAM is through a familiar idea from software engineering: Just-in-time (JIT) compilation. Rather than precomputing a rigid, heavily compressed memory, GAM keeps things light and tight by storing a minimal set of cues, along with a full, untouched archive of raw history. Then, when a request arrives, it “compiles” a tailored context on the fly.This JIT approach is built into GAM’s dual architecture, allowing AI to carry context across long conversations without overcompressing or guessing too early about what matters. The result is the right information, delivered at exactly the right moment.Inside GAM: A two-agent system built for memory that enduresGAM revolves around the simple idea of separating the act of remembering from recalling, which aptly involves two components: The 'memorizer' and the 'researcher.'The memorizer: Total recall without overloadThe memorizer captures every exchange in full, quietly turning each interaction into a concise memo while preserving the complete, decorated session in a searchable page store. It doesn’t compress aggressively or guess what is important. Instead, it organizes interactions into structured pages, adds metadata for efficient retrieval and generates optional lightweight summaries for quick scanning. Critically, every detail is preserved, and nothing is thrown away.The researcher: A deep retrieval engineWhen the agent needs to act, the researcher takes the helm to plan a search strategy, combining embeddings with keyword methods like BM25, navigating through page IDs and stitching the pieces together. It conducts layered searches across the page-store, blending vector retrieval, keyword matching and direct lookups. It evaluates findings, identifies gaps and continues searching until it has sufficient evidence to produce a confident answer, much like a human analyst reviewing old notes and primary documents. It iterates, searches, integrates and reflects until it builds a clean, task-specific briefing. GAM’s power comes from this JIT memory pipeline, which assembles rich, task-specific context on demand instead of leaning on brittle, precomputed summaries. Its core innovation is simple yet powerful, as it preserves all information intact and makes every detail recoverable.Ablation studies support this approach: Traditional memory fails on its own, and naive retrieval isn’t enough. It’s the pairing of a complete archive with an active, iterative research engine that enables GAM to surface details that other systems leave behind.Outperforming RAG and long-context modelsTo test GAM, the researchers pitted it against standard RAG pipelines and models with enlarged context windows such as GPT-4o-mini and Qwen2.5-14B. They evaluated GAM using four major long-context and memory-intensive benchmarks, each chosen to test a different aspect of the system’s capabilities:LoCoMo measures an agent’s ability to maintain and recall information across long, multi-session conversations, encompassing single-hop, multi-hop, temporal reasoning and open-domain tasks.HotpotQA, a widely used multi-hop QA benchmark built from Wikipedia, was adapted using MemAgent’s memory-stress-test version, which mixes relevant documents with distractors to create contexts of 56K, 224K and 448K tokens — ideal for testing how well GAM handles noisy, sprawling input.RULER evaluates retrieval accuracy, multi-hop state tracking, aggregation over long sequences and QA performance under a 128K-token context to further probe long-horizon reasoning.NarrativeQA is a benchmark where each question must be answered using the full text of a book or movie script; the researchers sampled 300 examples with an average context size of 87K tokens.Together, these datasets and benchmarks allowed the team to assess both GAM’s ability to preserve detailed historical information and its effectiveness in supporting complex downstream reasoning tasks.GAM came out ahead across all benchmarks. Its biggest win was on RULER, which benchmarks long-range state tracking. Notably: GAM exceeded 90% accuracy.RAG collapsed because key details were lost in summaries.Long-context models faltered as older information effectively “faded” even when technically present.Clearly, bigger context windows aren’t the answer. GAM works because it retrieves with precision rather than piling up tokens.GAM, context engineering and competing approachesPoorly structured context, not model limitations, is often the real reason AI agents fail. GAM addresses this by ensuring that nothing is permanently lost and that the right information can always be retrieved, even far downstream. The technique’s emergence coincides with the current, broader shift in AI towards context engineering, or the practice of shaping everything an AI model sees — its instructions, history, retrieved documents, tools, preferences and output formats.Context engineering has rapidly eclipsed prompt engineering in importance, although other research groups are tackling the memory problem from different angles. Anthropic is exploring curated, evolving context states. DeepSeek is experimenting with storing memory as images. Another group of Chinese researchers has proposed “semantic operating systems” built around lifelong adaptive memory.However, GAM’s philosophy is distinct: Avoid loss and retrieve with intelligence. Instead of guessing what will matter later, it keeps everything and uses a dedicated research engine to find the relevant pieces at runtime. For agents handling multi-day projects, ongoing workflows or long-term relationships, that reliability may prove essential.Why GAM matters for the long haulJust as adding more compute doesn’t automatically produce better algorithms, expanding context windows alone won’t solve AI’s long-term memory problems. Meaningful progress requires rethinking the underlying system, and GAM takes that approach. Instead of depending on ever-larger models, massive context windows or endlessly refined prompts, it treats memory as an engineering challenge — one that benefits from structure rather than brute force.As AI agents transition from clever demos to mission-critical tools, their ability to remember long histories becomes crucial for developing dependable, intelligent systems. Enterprises require AI agents that can track evolving tasks, maintain continuity and recall past interactions with precision and accuracy. GAM offers a practical path toward that future, signaling what may be the next major frontier in AI: Not bigger models, but smarter memory systems and the context architectures that make them possible.
